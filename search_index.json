[["index.html", "Hands-On Spatial Data Science with R Introduction Disclaimer Using This Book Who This Book Is For", " Hands-On Spatial Data Science with R Luc Anselin, Grant Morrison, Angela Li, Karina Acosta 2023-10-25 Introduction This book contains the R version of the GeoDa Workbook developed by Luc Anselin. It accompanies the Introduction to Spatial Data Science course taught at the University of Chicago. Each chapter was originally developed as a standalone lab tutorial for one week of the class. As a result, it is possible to work through a single chapter on its own, though we recommend going from the beginning to the end. Disclaimer This book is still actively under development and may not work for you when you access it. For versions of the lab notebooks that have been tested and are not undergoing changes, please see the Tutorials page on our Spatial Analysis with R website. Using This Book We have developed an R data package (geodaData) to use along with this book, so you can work through through the exercises immediately. To install it, run the following in your R console: install.packages(&quot;geodaData&quot;) Who This Book Is For We assume that workshop attendees have used RStudio and are familiar with the basics of R. If you need a refresher, this R for Social Scientists tutorial developed by Data Carpentry is a good place to start. Additionally, Luc Anselin’s introductory R lab notes can be found on the CSDS Tutorials page. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["spatial-data-handling.html", "Chapter 1 Spatial Data Handling Introduction Preliminaries Obtaining data from the Chicago Open Data portal Selecting Observations for a Given Time Period Creating a Point Layer Abandoned Vehicles by Community Area Community Area Population Data Mapping Community Area Abandoned Vehicles Per Capita", " Chapter 1 Spatial Data Handling Introduction This R notebook covers the functionality of the Spatial Data Handling section of the GeoDa workbook. We refer to that document for details on the methodology, references, etc. The goal of these notes is to approximate as closely as possible the operations carried out using GeoDa by means of a range of R packages. The notes are written with R beginners in mind, more seasoned R users can probably skip most of the comments on data structures and other R particulars. Also, as always in R, there are typically several ways to achieve a specific objective, so what is shown here is just one way that works, but there often are others (that may even be more elegant, work faster, or scale better). In this lab, we will use the City of Chicago open data portal to download data on abandoned vehicles. Our end goal is to create a choropleth map with abandoned vehicles per capita for Chicago community areas. Before we can create the maps, we will need to download the information, select observations, aggregate data, join different files and carry out variable transformations in order to obtain a so-called “spatially intensive” variable for mapping (i.e., not just a count of abandoned vehicles, but a per capita ratio). Objectives After completing the notebook, you should know how to carry out the following tasks: Download data from any Socrata-driven open data portal, such as the City of Chicago open data portal Filtering a data frame for specific entries Selecting and renaming columns Creating a simple features spatial object Checking and adding/adjusting projection information Dealing with missing data Spatial join Spatial aggregation Parsing a pdf file Merging data sets Creating new variables Basic choropleth mapping R Packages used RSocrata: to read data directly from a Socrata powered open data portal, such as the Chicago open data portal tidyverse (includes dplyr): to manipulate data frames, such as filtering data, selecting columns, and creating new variables lubridate: to select information out of the date format when filtering the data sf: to create and manipulate simple features spatial objects, to read in the boundary file, and perform point in polygon on the data set to fill in missing community area information pdftools: to read and parse a pdf for chicago community area population information tmap: to make nice looking choropleth maps R Commands used Below follows a list of the commands used in this notebook. For further details and a comprehensive list of options, please consult the R documentation. base R: setwd, install.packages, library, head, dim, class, as.Date, names, !is.na, is.numeric, as.integer, is.integer, length, strsplit, unlist, for, vector, substr, gsub, as.numeric, data.frame RSocrata: read.socrata tidyverse: filter, %&gt;% (pipe), select (with renaming), count, rename, mutate lubridate: year, month sf: st_as_sf, plot, st_crs, read_sf, st_transform, st_join, st_geometry, st_write pdftools: pdf_text tmap: tm_shape, tm_polygons Preliminaries Before starting, make sure to have the latest version of R and of packages that are compiled for the matching version of R (this document was created using R 3.5.1 of 2018-07-02). Also, optionally, set a working directory, even though we will not actually be saving any files.1 Loading packages First, we load all the required packages using the library command. If you don’t have some of these in your system, make sure to install them first as well as their dependencies.2 You will get an error message if something is missing. If needed, just install the missing piece and everything will work after that. library(tidyverse) library(lubridate) library(sf) library(tmap) library(pdftools) library(RSocrata) Obtaining data from the Chicago Open Data portal We will use the specialized RSocrata package to download the file with 311 calls about abandoned vehicles from the City of Chicago open data portal. A list of different types of 311 nuisance calls is given by selecting the button for Service Requests. The abandoned vehicles data are contained in the entry for 311 Service Requests - Abandoned Vehicles. Select the API button and copy the API Endpoint from the interface. This is the target file that we will download using the read.socrata function from the RSocrata package. Note, this is a large file, so it may take a while to download. First, we set the target file name to pass to the read.socrata function. Copy the API Endpoint and paste the file path, as shown below. socrata.file &lt;- &quot;https://data.cityofchicago.org/resource/suj7-cg3j.csv&quot; Next, download the file using read.socrata. This will turn the data into an R data frame. After the download is completed, we use the head command to get a sense of the contents of the data frame. vehicle.data &lt;- read.socrata(socrata.file) head(vehicle.data) ## creation_date status completion_date service_request_number ## 1 2011-01-01 Completed - Dup 2011-01-07 11-00002779 ## 2 2011-01-01 Completed - Dup 2011-01-20 11-00003001 ## 3 2011-01-01 Completed - Dup 2011-01-21 11-00003309 ## 4 2011-01-01 Completed - Dup 2011-01-21 11-00003316 ## 5 2011-01-01 Completed 2011-01-05 11-00001976 ## 6 2011-01-01 Completed 2011-01-05 11-00002291 ## type_of_service_request ## 1 Abandoned Vehicle Complaint ## 2 Abandoned Vehicle Complaint ## 3 Abandoned Vehicle Complaint ## 4 Abandoned Vehicle Complaint ## 5 Abandoned Vehicle Complaint ## 6 Abandoned Vehicle Complaint ## license_plate vehicle_make_model ## 1 REAR PLATE STARTS W/848 AND FRONT PLATE STARTS W/ K Isuzu ## 2 9381880 Toyota ## 3 MI S CS860 Jeep/Cherokee ## 4 MI SCS860 ## 5 H924236 Ford ## 6 810 LYB WISCONSIN PLATES Mercury ## vehicle_color current_activity most_recent_action ## 1 Red ## 2 Silver ## 3 Gold ## 4 Gold ## 5 White ## 6 Green ## how_many_days_has_the_vehicle_been_reported_as_parked_ street_address ## 1 24 5629 N KEDVALE AVE ## 2 NA 2053 N KILBOURN AVE ## 3 NA 736 W BUENA AVE ## 4 NA 736 W BUENA AVE ## 5 60 6059 S KOMENSKY AVE ## 6 NA 4651 S WASHTENAW AVE ## zip_code x_coordinate y_coordinate ward police_district community_area ssa ## 1 60646 1147717 1937054 39 17 13 NA ## 2 60639 1146056 1913269 31 25 20 NA ## 3 60613 1170576 1928214 46 23 3 NA ## 4 60613 1170576 1928214 46 23 3 NA ## 5 60629 1150408 1864110 13 8 65 3 ## 6 60632 1159150 1873712 12 9 58 NA ## latitude longitude location ## 1 41.98368 -87.73197 POINT (41.983680361597564 -87.7319663736746) ## 2 41.91859 -87.73868 POINT (41.91858774162382 -87.73868431751842) ## 3 41.95861 -87.64888 POINT (41.95860696269331 -87.64887590959788) ## 4 41.95861 -87.64888 POINT (41.95860696269331 -87.64887590959788) ## 5 41.78237 -87.72394 POINT (41.78237428405976 -87.72394038021173) ## 6 41.80864 -87.69163 POINT (41.80863500843091 -87.69162625248853) ## location_address location_city location_state location_zip ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; A quick glance at the table reveals quite a bit of missing information, something we will have to deal with. We also check the dimension of this data frame using the dim command: dim(vehicle.data) ## [1] 261486 26 The table has 203,657 observations on 26 variables (the number of observations shown may be slightly different as the table is constantly updated). In RStudio, the type of the variable in each column is listed under its name. For example, under creation_date, we see S3: POSIXct. You can also find out the same information by applying a class command to the variable vehicle.data$creation_date, as in class(vehicle.data$creation_date) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; The result again yields POSIXct, which is a common format used for dates. Note that RSocrata is able to tell the date format from a simple string. In contrast, if we had downloaded the file manually as a csv (comma separated value) file, this would not be the case (see the GeoDa example). In that instance, we would have to convert the creation_date to a date format explicitly using as.Date. Selecting Observations for a Given Time Period As in the GeoDa example, we are not using all the data, but will analyze the abandoned vehicle locations for a given time period, i.e., the month of September 2016. Extracting observations for the desired time period To extract the observations for the selected year (2016) and month (9), we will use the year and month functions from the lubridate package. We will embed these expressions in a filter command (from tidyverse) to select the rows/observations that match the specified criterion. We will also use the pipe command %&gt;% to move the original data frame through the different filter stages and assign the end result to vehicle.sept16. We again check the contents with a head command. vehicle.sept16 &lt;- vehicle.data %&gt;% filter(year(creation_date) == 2016) %&gt;% filter(month(creation_date) == 9) head(vehicle.sept16) ## creation_date status completion_date service_request_number ## 1 2016-09-01 Completed - Dup 2016-09-01 16-06192603 ## 2 2016-09-01 Completed - Dup 2016-09-01 16-06192662 ## 3 2016-09-01 Completed - Dup 2016-09-01 16-06193608 ## 4 2016-09-01 Completed - Dup 2016-09-01 16-06194284 ## 5 2016-09-01 Completed - Dup 2016-09-01 16-06194594 ## 6 2016-09-01 Completed - Dup 2016-09-01 16-06197569 ## type_of_service_request license_plate vehicle_make_model vehicle_color ## 1 Abandoned Vehicle Complaint UNKNOWN Chevrolet White ## 2 Abandoned Vehicle Complaint UNKNOWN Green ## 3 Abandoned Vehicle Complaint UKNOWN Gray ## 4 Abandoned Vehicle Complaint NO PLATES Ford Blue ## 5 Abandoned Vehicle Complaint ## 6 Abandoned Vehicle Complaint ## current_activity most_recent_action ## 1 FVI - Outcome Create Work Order ## 2 FVI - Outcome Create Work Order ## 3 FVI - Outcome Create Work Order ## 4 FVI - Outcome Create Work Order ## 5 FVI - Outcome Create Work Order ## 6 FVI - Outcome Create Work Order ## how_many_days_has_the_vehicle_been_reported_as_parked_ street_address ## 1 14 3710 W IOWA ST ## 2 40 5240 S MAYFIELD AVE ## 3 7 8000 S ALBANY AVE ## 4 30 8654 W CATHERINE AVE ## 5 NA 4315 N MONTICELLO AVE ## 6 NA 2241 N MULLIGAN AVE ## zip_code x_coordinate y_coordinate ward police_district community_area ssa ## 1 60651 1151452 1905748 27 11 23 NA ## 2 60638 1137921 1869254 14 8 56 NA ## 3 60652 1157102 1851405 18 8 70 NA ## 4 60656 1117638 1934535 41 16 76 NA ## 5 60618 1151283 1928434 35 17 16 NA ## 6 60639 1133710 1914324 36 25 19 NA ## latitude longitude location ## 1 41.89736 -87.71933 POINT (41.89736153676566 -87.71933325878982) ## 2 41.79688 -87.76990 POINT (41.796881421903066 -87.76989633815052) ## 3 41.74792 -87.70005 POINT (41.74792366108626 -87.70004701460941) ## 4 41.97694 -87.84349 POINT (41.97694235046974 -87.8434945723464) ## 5 41.95972 -87.71907 POINT (41.95972327912134 -87.71906810908936) ## 6 41.92154 -87.78402 POINT (41.92154133910697 -87.78401648793171) ## location_address location_city location_state location_zip ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; and the dimension: dim(vehicle.sept16) ## [1] 2637 26 The filtered table now only has 2,637 observations. Selecting the variables for the final table The current data frame contains 26 variables. Several of these are not really of interest to us, since we basically want the locations of the events. We will use the select command from tidyverse to pick out the columns that we want to keep. In addition, we will use the rename option in select to give new variable names. While this is not absolutely necessary at this stage (RSocrata has turned any weird variable names into proper R names), we may later want to save the data as a point shape file. The data associated with a shape file are store in a separate dBase file, and dBase only allows 10 characters for variable names. So, in order to save ourselves some work later on, we will rename the selected variables to strings that do not exceed 10 characters. First, we check the variable names using the names command. names(vehicle.sept16) ## [1] &quot;creation_date&quot; ## [2] &quot;status&quot; ## [3] &quot;completion_date&quot; ## [4] &quot;service_request_number&quot; ## [5] &quot;type_of_service_request&quot; ## [6] &quot;license_plate&quot; ## [7] &quot;vehicle_make_model&quot; ## [8] &quot;vehicle_color&quot; ## [9] &quot;current_activity&quot; ## [10] &quot;most_recent_action&quot; ## [11] &quot;how_many_days_has_the_vehicle_been_reported_as_parked_&quot; ## [12] &quot;street_address&quot; ## [13] &quot;zip_code&quot; ## [14] &quot;x_coordinate&quot; ## [15] &quot;y_coordinate&quot; ## [16] &quot;ward&quot; ## [17] &quot;police_district&quot; ## [18] &quot;community_area&quot; ## [19] &quot;ssa&quot; ## [20] &quot;latitude&quot; ## [21] &quot;longitude&quot; ## [22] &quot;location&quot; ## [23] &quot;location_address&quot; ## [24] &quot;location_city&quot; ## [25] &quot;location_state&quot; ## [26] &quot;location_zip&quot; To keep things simple, we will only keep community_area, latitude and longitude, and turn them into comm, lat and lon. The new data set is vehicles.final. Note that to rename a variable, the new name is listed first, on the left hand side of the equal sign, and the old name is on the right hand side. We check the result with the head command. vehicles.final &lt;- vehicle.sept16 %&gt;% select(comm = community_area, lat = latitude, lon = longitude) head(vehicles.final) ## comm lat lon ## 1 23 41.89736 -87.71933 ## 2 56 41.79688 -87.76990 ## 3 70 41.74792 -87.70005 ## 4 76 41.97694 -87.84349 ## 5 16 41.95972 -87.71907 ## 6 19 41.92154 -87.78402 Creating a Point Layer So far, we have only dealt with a regular data frame, without taking advantage of any spatial features. However, the data frame contains fields with coordinates and R can turn these into an explicit spatial points layer that can be saved in a range of GIS formats. To accomplish this, we will use the (new) simple features or sf package functionality, which improves upon the older sp. We will first use the lat and lon columns in the data frame to create a spatial points object. Note that lon is the x-coordinate and lat is the y-coordinate. Creating a point layer from coordinates in a table - principle In sf, a simple features object is constructed by combining a geometry with the actual data (in a data frame). However, this is simplified for point objects when the data frame contains the coordinates as variables. This is the case in our example, where we have latitude and longitude. We also have x and y, but since we are not sure what projection these coordinates correspond with, they are not useful at this stage. The advantage of lat-lon is that they are decimal degrees, and thus unprojected. However, we can provide the information on the datum, typically WGS84 (the standard used in most applications for decimal degrees) by passing the coordinate reference system argument (crs) set to the EPSG code 4326. After that, we can use the built-in projection transformation functionality in sf to turn the points into any projection we want.3 Missing coordinates In order to create a points layer, we need coordinates for every observation. However, as we can see from the head command above, there are (at least) two observations that do not have lat-lon information. Before we can proceed, we need to remove these from the data frame. We again use a filter command, but now combine it with the !is.na expression, i.e., is not missing (na). We take a little short cut by assuming that if one of lat or lon is missing, the other one will be missing as well (although to keep it completely general, we would need to check each variable separately). We assign the result to the vehicle.coord data frame. vehicle.coord &lt;- vehicles.final %&gt;% filter(!(is.na(lat))) dim(vehicle.coord) ## [1] 2635 3 As it turns out, the two rows we noticed above were the only two with missing coordinates (the number of rows went from 2,637 to 2,635). Creating a spatial points object The sf package turns a non-spatial object like a data frame into a simple features spatial object by means of the st_as_sf function. This function can take a large number of arguments, but for now we will only use a few: the name of the data frame, i.e., vehicle.coord coords: the variable names for x and y (given in parentheses) crs: the coordinate reference system, here using the EPSG code of 4326 agr: the so-called attibute-geometry-relationship which specifies how the attribute information (the data) relate to the geometry (the points); in our example, we will use “constant” In our example, we create vehicle.points and check its class. vehicle.points = st_as_sf(vehicle.coord, coords = c(&quot;lon&quot;, &quot;lat&quot;), crs = 4326, agr = &quot;constant&quot;) class(vehicle.points) ## [1] &quot;sf&quot; &quot;data.frame&quot; Even though it is not that informative at this stage, we can also make a quick plot. Later, we will see how we can refine these plots using the tmap package. plot(vehicle.points) We can also do a quick check of the projection information using the st_crs command. st_crs(vehicle.points) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] The proj4string is a slightly more informative description than the simple EPSG code and confirms the data are not projected (longlat) using the WGS84 datum for the decimal degree coordinates. Abandoned Vehicles by Community Area At this point, we will go about things in a slightly different way from how they are illustrated in the GeoDa workbook example. As it turns out, some of the points have missing community area information, which is a critical element to compute the number of abandoned cars at that scale. In GeoDa, we used a visual approach to obtain the missing information. Here, we will exploit some of the GIS functionality in sf to carry out a spatial join. This boils down to identifying which points belong to each community area (a so-called point in polygon query) and assigning the corresponding community area identifier to each point. We proceed in three steps. First, we create a simple features spatial polygon object with the boundaries of the community areas, which we download from the Chicago Open Data portal. Next, we carry out a spatial join between our points object and the polygon object to assign a community area code to each point. Finally, we compute the point count by community area. Community Area boundary file We resort to the City of Chicago open data portal for the boundary file of the community areas. From the opening screen, select the button for Facilities &amp; Geo Boundaries. This yields a list of different boundary files for a range of geographic areal units. The one for the community areas is Boundaries - Community Areas (current). This brings up an overview map of the geography of the community areas of Chicago. Of course, we could simply select one of the export buttons to download the files, but we want to do this programmatically. As it turns out, sf can read a geojson formatted file directly from the web, and we will exploit that functionality. First, we need the name for the file. We can check the Socrata API file name, but that contains a json file, and we want a specific geojson file. As it turns out, the latter is simply the same file name, but with the geojson file extension. We set our variable comm.file to this URL and then use sf_read to load the boundary information into chicago.comm. As before, we can do a quick check of the class using the class command. comm.file &lt;- &quot;https://data.cityofchicago.org/resource/igwz-8jzy.geojson&quot; chicago.comm &lt;- read_sf(comm.file) class(chicago.comm) ## [1] &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; In addition, we check the projection information using st_crs. st_crs(chicago.comm) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] Again, the layer is unprojected in decimal degrees. Also, a quick plot. Note that, by default, sf draws a choropleth map for each variable included in the data frame. Since we won’t be using sf for mapping, we ignore that aspect for now. plot(chicago.comm) We also use head to check on the types of the variables. head(chicago.comm) ## Simple feature collection with 6 features and 9 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.7069 ymin: 41.79448 xmax: -87.58001 ymax: 41.99076 ## Geodetic CRS: WGS 84 ## # A tibble: 6 × 10 ## community area shape_area perimeter area_num_1 area_numbe comarea_id comarea ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 DOUGLAS 0 46004621.… 0 35 35 0 0 ## 2 OAKLAND 0 16913961.… 0 36 36 0 0 ## 3 FULLER PA… 0 19916704.… 0 37 37 0 0 ## 4 GRAND BOU… 0 48492503.… 0 38 38 0 0 ## 5 KENWOOD 0 29071741.… 0 39 39 0 0 ## 6 LINCOLN S… 0 71352328.… 0 4 4 0 0 ## # ℹ 2 more variables: shape_len &lt;chr&gt;, geometry &lt;MULTIPOLYGON [°]&gt; Changing projections Before moving on to the spatial join operation, we will convert both the community area boundaries and the vehicle points to the same projection, using the st_transform command. We assign the UTM (Universal Tranverse Mercator) zone 16N, which the the proper one for Chicago, with an EPSG code of 32616. After the projection transformation, we check the result using st_crs. chicago.comm &lt;- st_transform(chicago.comm,32616) st_crs(chicago.comm) ## Coordinate Reference System: ## User input: EPSG:32616 ## wkt: ## PROJCRS[&quot;WGS 84 / UTM zone 16N&quot;, ## BASEGEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]], ## CONVERSION[&quot;UTM zone 16N&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,-87, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,500000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Engineering survey, topographic mapping.&quot;], ## AREA[&quot;Between 90°W and 84°W, northern hemisphere between equator and 84°N, onshore and offshore. Belize. Canada - Manitoba; Nunavut; Ontario. Costa Rica. Cuba. Ecuador - Galapagos. El Salvador. Guatemala. Honduras. Mexico. Nicaragua. United States (USA).&quot;], ## BBOX[0,-90,84,-84]], ## ID[&quot;EPSG&quot;,32616]] vehicle.points &lt;- st_transform(vehicle.points,32616) st_crs(vehicle.points) ## Coordinate Reference System: ## User input: EPSG:32616 ## wkt: ## PROJCRS[&quot;WGS 84 / UTM zone 16N&quot;, ## BASEGEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]], ## CONVERSION[&quot;UTM zone 16N&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,-87, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,500000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Engineering survey, topographic mapping.&quot;], ## AREA[&quot;Between 90°W and 84°W, northern hemisphere between equator and 84°N, onshore and offshore. Belize. Canada - Manitoba; Nunavut; Ontario. Costa Rica. Cuba. Ecuador - Galapagos. El Salvador. Guatemala. Honduras. Mexico. Nicaragua. United States (USA).&quot;], ## BBOX[0,-90,84,-84]], ## ID[&quot;EPSG&quot;,32616]] Spatial join In essence, the spatial join operation finds the polygon to which each point belongs. Several points belong to the same polygon, so this is a many-to-one join. Instead of joining all the features of the polygon layer, we specify just area_num_1, which is the community area indicator. The command is st_join to which we pass the point layer as the first sf object, and the polygon layer as the second sf object (with only one column designated). We assign the result to the new spatial object comm.pts. We check the contents of the new object using a head command. comm.pts &lt;- st_join(vehicle.points,chicago.comm[&quot;area_num_1&quot;]) head(comm.pts) ## Simple feature collection with 6 features and 2 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 430118.3 ymin: 4622026 xmax: 441795.4 ymax: 4647560 ## Projected CRS: WGS 84 / UTM zone 16N ## comm area_num_1 geometry ## 1 23 23 POINT (440330.7 4638631) ## 2 56 56 POINT (436036.4 4627511) ## 3 70 70 POINT (441795.4 4622026) ## 4 76 76 POINT (430118.3 4647560) ## 5 16 16 POINT (440410.8 4645554) ## 6 19 19 POINT (434989.7 4641362) As we can see, the community area in comm matches the entry in area_num_1. However, there is one more issue to deal with. Upon closer examination, we find that the area_num_1 variable is not numeric using the is.numeric check. is.numeric(comm.pts$area_num_1) ## [1] FALSE So, we proceed to turn this variable into a numeric format using as.integer and then do a quick check by means of is.integer. comm.pts$area_num_1 &lt;- as.integer(comm.pts$area_num_1) is.integer(comm.pts$area_num_1) ## [1] TRUE The same problem occurs in the chicago.comm data set, which can cause trouble later on when we will join it with other data. Therefore, we turn it into an integer as well. chicago.comm$area_num_1 &lt;- as.integer(chicago.comm$area_num_1) Counts by community area We now need to count the number of points in each polygon. We proceed in two steps. First, we illustrate how we can move back from the simple features spatial points object to a simple data frame by stripping the geometry column. This is accomplished by setting st_geometry to NULL. We check the class of the new object to make sure it is no longer a simple feature. st_geometry(comm.pts) &lt;- NULL class(comm.pts) ## [1] &quot;data.frame&quot; We next take advantage of the tidyverse count function to create a new data frame with the identifier of the community area and the number of points contained in each community area. veh.cnts &lt;- comm.pts %&gt;% count(area_num_1) head(veh.cnts) ## area_num_1 n ## 1 1 67 ## 2 2 89 ## 3 3 21 ## 4 4 32 ## 5 5 18 ## 6 6 19 The new data frame has two fields: the original identifier area_num_1 and the count as n. We can change the variable names for the count to something more meaningful by means of the tidyverse rename command and turn it from n to AGG.COUNT (to use the same variable as in the GeoDa workbook). Similarly, we also shorten area_num_1 to comm. Again, the new name is on the LHS of the equal sign and the old name on the RHS. veh.cnts &lt;- veh.cnts %&gt;% rename(comm = area_num_1, AGG.COUNT = n) head(veh.cnts) ## comm AGG.COUNT ## 1 1 67 ## 2 2 89 ## 3 3 21 ## 4 4 32 ## 5 5 18 ## 6 6 19 Mapping the vehicle counts At this point, we have a polygon layer with the community area boundaries and some identifiers (chicago.comm) and a data frame with the community identifier and the aggregate vehicle count (veh.cnts). In order to map the vehicle counts by community area, we need to join the two tables. We use the left_join command and use area_num_1 as the key for the first table (the community area boundaries), and comm as the key for the second table (the vehicle counts). Since we assured that both variables are now integers, the join will work (if one were a character and the other integer, there would be an error message). Note how in the command below, the two keys can have different variable names (but they must have the same values), which is made explicit in the by statement. chicago.comm &lt;- left_join(chicago.comm,veh.cnts, by = c(&quot;area_num_1&quot; = &quot;comm&quot;)) We can double check that the vehicle counts were added using the head command. head(chicago.comm) ## Simple feature collection with 6 features and 10 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 441440.4 ymin: 4627153 xmax: 451817.1 ymax: 4648971 ## Projected CRS: WGS 84 / UTM zone 16N ## # A tibble: 6 × 11 ## community area shape_area perimeter area_num_1 area_numbe comarea_id comarea ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 DOUGLAS 0 46004621.… 0 35 35 0 0 ## 2 OAKLAND 0 16913961.… 0 36 36 0 0 ## 3 FULLER PA… 0 19916704.… 0 37 37 0 0 ## 4 GRAND BOU… 0 48492503.… 0 38 38 0 0 ## 5 KENWOOD 0 29071741.… 0 39 39 0 0 ## 6 LINCOLN S… 0 71352328.… 0 4 4 0 0 ## # ℹ 3 more variables: shape_len &lt;chr&gt;, geometry &lt;MULTIPOLYGON [m]&gt;, ## # AGG.COUNT &lt;int&gt; Basic choropleth map As we saw earlier, we can construct rudimentary maps using the plot command in sf, but for further control, we will use the tmap package. This uses a logic similar to Wilkinson’s grammar of graphics, which is also the basis for the structure of the plot commands in the ggplot package. We leave a detailed treatment of tmap for a future lab and just use the basic defaults in this example. The commands are layered and always start by specifying a layer using the tm_shape command. In our example, this is chicago.comm. Next (after the plus sign) follow one of more drawing commands that cover a wide range of geographic shapes. Here, we will just use tm_polygons and specify AGG.COUNT as the variable to determine the classification. We leave everything to the default and obtain a map that illustrates the spatial distribution of the abandoned vehicle counts by community area. tm_shape(chicago.comm) + tm_polygons(&quot;AGG.COUNT&quot;) However, this map can be highly misleading since it pertains to a so-called spatially extensive variable, such as a count. Even if every area had the same risk of having abandoned vehicles, larger community areas would have higher counts. In other words, since the count is directly related to the size of the area, it does not provide a proper indication of the risk. Instead, we should map a spatially intensive variable, which is corrected for the size of the unit. For example, this can be achieved by expressing the variable as a density (counts per area), or as some other ratio, such as the counts per capita. In order to calculate this ratio, we first need to obtain the population for each community area. Community Area Population Data The Chicago Community Area 2010 population is contained in a pdf file, available from the City of Chicago web site. This link is to a pdf file that contains a table with the neighborhood ID, the neighborhood name, the populations for 2010 and 2000, the difference between the two years and the percentage difference. The full path to the pdf file is https://www.cityofchicago.org/content/dam/city/depts/zlup/Zoning_Main_Page/Publications/Census_2010_Community_Area_Profiles/Census_2010_and_2000_CA_Populations.pdf Extracting a pdf file A pdf file is difficult to handle as a source of data, since it doesn’t contain tags like an html file. We will use the pdftools package that allows us to turn the contents of a pdf file into a list of long character strings. The resulting data structure is somewhat complex and not necessarily easy to parse. However, in our case, the table has such a simple structure that we can extract the population values by doing some sleuthing on which columns contain those values. This will illustrate the power of the various parsing and text extraction functions available in R. We use the pdf_text function from pdftools to turn the pdf file into a list of character strings, one for each page. We specify the URL of the file as the input source. pdf.file &lt;- &quot;https://www.cityofchicago.org/content/dam/city/depts/zlup/Zoning_Main_Page/Publications/Census_2010_Community_Area_Profiles/Census_2010_and_2000_CA_Populations.pdf&quot; pop.dat &lt;- pdf_text(pdf.file) class(pop.dat) ## [1] &quot;character&quot; We check the length of the data object using the length command and find that indeed it has only two elements (one for each page). length(pop.dat) ## [1] 2 Parsing the pdf file The pop.dat object has two entries, one for each page. Each entry is a single string. So, when you check the length of each item, it may be surprising that its length is only 1. That is because the underlying structure is unknown, it is simply a collection of characters contained in the string. For example, the first element, pop.dat[[1]]: length(pop.dat[[1]]) ## [1] 1 We will parse this file by first turning each element into a separate list and then extracting the parts we are interested in. First, to illustrate in detail what is going on, we will go through each step one by one, but then, in order to reach some level of efficiency, we turn it into a loop over the two elements, for (i in 1:2). We start by initializing a vector (nnlist) with an empty character, and confirm that it is indeed initialized. nnlist &lt;- &quot;&quot; nnlist ## [1] &quot;&quot; Next, we create a list of strings, one for each line in the table, by using the strsplit operation. This splits the long string into a list of one string for each line, by using the return character \\n as the separator (the value for the split argument). The resulting list, ppage, contains a list of 44 elements, matching the contents of the first page of the pdf file. ppage &lt;- strsplit(pop.dat[[1]],split=&quot;\\n&quot;) ppage[[1]] ## [1] &quot; CITY OF CHICAGO&quot; ## [2] &quot; CENSUS 2010 AND 2000&quot; ## [3] &quot;&quot; ## [4] &quot; Population&quot; ## [5] &quot;Num Community Area 2010 2,000 Difference Percentage&quot; ## [6] &quot; 1 Rogers Park 54,991 63,484 -8,493 -13.4%&quot; ## [7] &quot; 2 West Ridge 71,942 73,199 -1,257 -1.7%&quot; ## [8] &quot; 3 Uptown 56,362 63,551 -7,189 -11.3%&quot; ## [9] &quot; 4 Lincoln Square 39,493 44,574 -5,081 -11.4%&quot; ## [10] &quot; 5 North Center 31,867 31,895 -28 -0.1%&quot; ## [11] &quot; 6 Lake View 94,368 94,817 -449 -0.5%&quot; ## [12] &quot; 7 Lincoln Park 64,116 64,320 -204 -0.3%&quot; ## [13] &quot; 8 Near North Side 80,484 72,811 7,673 10.5%&quot; ## [14] &quot; 9 Edison Park 11,187 11,259 -72 -0.6%&quot; ## [15] &quot; 10 Norwood Park 37,023 37,669 -646 -1.7%&quot; ## [16] &quot; 11 Jefferson Park 25,448 25,859 -411 -1.6%&quot; ## [17] &quot; 12 Forest Glen 18,508 18,165 343 1.9%&quot; ## [18] &quot; 13 North Park 17,931 18,514 -583 -3.1%&quot; ## [19] &quot; 14 Albany Park 51,542 57,655 -6,113 -10.6%&quot; ## [20] &quot; 15 Portage Park 64,124 65,340 -1,216 -1.9%&quot; ## [21] &quot; 16 Irving Park 53,359 58,643 -5,284 -9.0%&quot; ## [22] &quot; 17 Dunning 41,932 42,164 -232 -0.6%&quot; ## [23] &quot; 18 Montclare 13,426 12,646 780 6.2%&quot; ## [24] &quot; 19 Belmont Cragin 78,743 78,144 599 0.8%&quot; ## [25] &quot; 20 Hermosa 25,010 26,908 -1,898 -7.1%&quot; ## [26] &quot; 21 Avondale 39,262 43,083 -3,821 -8.9%&quot; ## [27] &quot; 22 Logan Square 73,595 82,715 -9,120 -11.0%&quot; ## [28] &quot; 23 Humboldt Park 56,323 65,836 -9,513 -14.4%&quot; ## [29] &quot; 24 West Town 81,432 87,435 -6,003 -6.9%&quot; ## [30] &quot; 25 Austin 98,514 117,527 -19,013 -16.2%&quot; ## [31] &quot; 26 West Garfield Park 18,001 23,019 -5,018 -21.8%&quot; ## [32] &quot; 27 East Garfield Park 20,567 20,881 -314 -1.5%&quot; ## [33] &quot; 28 Near West Side 54,881 46,419 8,462 18.2%&quot; ## [34] &quot; 29 North Lawndale 35,912 41,768 -5,856 -14.0%&quot; ## [35] &quot; 30 South Lawndale 79,288 91,071 -11,783 -12.9%&quot; ## [36] &quot; 31 Lower West Side 35,769 44,031 -8,262 -18.8%&quot; ## [37] &quot; 32 Loop 29,283 16,388 12,895 78.7%&quot; ## [38] &quot; 33 Near South Side 21,390 9,509 11,881 124.9%&quot; ## [39] &quot; 34 Armour Square 13,391 12,032 1,359 11.3%&quot; ## [40] &quot; 35 Douglas 18,238 26,470 -8,232 -31.1%&quot; ## [41] &quot; 36 Oakland 5,918 6,110 -192 -3.1%&quot; ## [42] &quot; 37 Fuller Park 2,876 3,420 -544 -15.9%&quot; ## [43] &quot; 38 Grand Boulevard 21,929 28,006 -6,077 -21.7%&quot; ## [44] &quot; 39 Kenwood 17,841 18,363 -522 -2.8%&quot; ## [45] &quot; 40 Washington Park 11,717 14,146 -2,429 -17.2%&quot; Each element is one long string, corresponding to a table row. We remove the first four lines (using the - operation on the list elements 1 through 4). These first rows appear on each page, so we are safe to repeat this procedure for the second page (string) as well. nni &lt;- ppage[[1]] nni &lt;- nni[-(1:4)] nni ## [1] &quot;Num Community Area 2010 2,000 Difference Percentage&quot; ## [2] &quot; 1 Rogers Park 54,991 63,484 -8,493 -13.4%&quot; ## [3] &quot; 2 West Ridge 71,942 73,199 -1,257 -1.7%&quot; ## [4] &quot; 3 Uptown 56,362 63,551 -7,189 -11.3%&quot; ## [5] &quot; 4 Lincoln Square 39,493 44,574 -5,081 -11.4%&quot; ## [6] &quot; 5 North Center 31,867 31,895 -28 -0.1%&quot; ## [7] &quot; 6 Lake View 94,368 94,817 -449 -0.5%&quot; ## [8] &quot; 7 Lincoln Park 64,116 64,320 -204 -0.3%&quot; ## [9] &quot; 8 Near North Side 80,484 72,811 7,673 10.5%&quot; ## [10] &quot; 9 Edison Park 11,187 11,259 -72 -0.6%&quot; ## [11] &quot; 10 Norwood Park 37,023 37,669 -646 -1.7%&quot; ## [12] &quot; 11 Jefferson Park 25,448 25,859 -411 -1.6%&quot; ## [13] &quot; 12 Forest Glen 18,508 18,165 343 1.9%&quot; ## [14] &quot; 13 North Park 17,931 18,514 -583 -3.1%&quot; ## [15] &quot; 14 Albany Park 51,542 57,655 -6,113 -10.6%&quot; ## [16] &quot; 15 Portage Park 64,124 65,340 -1,216 -1.9%&quot; ## [17] &quot; 16 Irving Park 53,359 58,643 -5,284 -9.0%&quot; ## [18] &quot; 17 Dunning 41,932 42,164 -232 -0.6%&quot; ## [19] &quot; 18 Montclare 13,426 12,646 780 6.2%&quot; ## [20] &quot; 19 Belmont Cragin 78,743 78,144 599 0.8%&quot; ## [21] &quot; 20 Hermosa 25,010 26,908 -1,898 -7.1%&quot; ## [22] &quot; 21 Avondale 39,262 43,083 -3,821 -8.9%&quot; ## [23] &quot; 22 Logan Square 73,595 82,715 -9,120 -11.0%&quot; ## [24] &quot; 23 Humboldt Park 56,323 65,836 -9,513 -14.4%&quot; ## [25] &quot; 24 West Town 81,432 87,435 -6,003 -6.9%&quot; ## [26] &quot; 25 Austin 98,514 117,527 -19,013 -16.2%&quot; ## [27] &quot; 26 West Garfield Park 18,001 23,019 -5,018 -21.8%&quot; ## [28] &quot; 27 East Garfield Park 20,567 20,881 -314 -1.5%&quot; ## [29] &quot; 28 Near West Side 54,881 46,419 8,462 18.2%&quot; ## [30] &quot; 29 North Lawndale 35,912 41,768 -5,856 -14.0%&quot; ## [31] &quot; 30 South Lawndale 79,288 91,071 -11,783 -12.9%&quot; ## [32] &quot; 31 Lower West Side 35,769 44,031 -8,262 -18.8%&quot; ## [33] &quot; 32 Loop 29,283 16,388 12,895 78.7%&quot; ## [34] &quot; 33 Near South Side 21,390 9,509 11,881 124.9%&quot; ## [35] &quot; 34 Armour Square 13,391 12,032 1,359 11.3%&quot; ## [36] &quot; 35 Douglas 18,238 26,470 -8,232 -31.1%&quot; ## [37] &quot; 36 Oakland 5,918 6,110 -192 -3.1%&quot; ## [38] &quot; 37 Fuller Park 2,876 3,420 -544 -15.9%&quot; ## [39] &quot; 38 Grand Boulevard 21,929 28,006 -6,077 -21.7%&quot; ## [40] &quot; 39 Kenwood 17,841 18,363 -522 -2.8%&quot; ## [41] &quot; 40 Washington Park 11,717 14,146 -2,429 -17.2%&quot; To streamline the resulting data structure for further operations, we turn it into a simple vector by means of unlist. This then allows us to concatenate the result to the current nnlist vector (initially, this contains just a single element with an empty character, after the first step it contains the empty character and the first page). nnu &lt;- unlist(nni) nnlist &lt;- c(nnlist,nnu) nnlist ## [1] &quot;&quot; ## [2] &quot;Num Community Area 2010 2,000 Difference Percentage&quot; ## [3] &quot; 1 Rogers Park 54,991 63,484 -8,493 -13.4%&quot; ## [4] &quot; 2 West Ridge 71,942 73,199 -1,257 -1.7%&quot; ## [5] &quot; 3 Uptown 56,362 63,551 -7,189 -11.3%&quot; ## [6] &quot; 4 Lincoln Square 39,493 44,574 -5,081 -11.4%&quot; ## [7] &quot; 5 North Center 31,867 31,895 -28 -0.1%&quot; ## [8] &quot; 6 Lake View 94,368 94,817 -449 -0.5%&quot; ## [9] &quot; 7 Lincoln Park 64,116 64,320 -204 -0.3%&quot; ## [10] &quot; 8 Near North Side 80,484 72,811 7,673 10.5%&quot; ## [11] &quot; 9 Edison Park 11,187 11,259 -72 -0.6%&quot; ## [12] &quot; 10 Norwood Park 37,023 37,669 -646 -1.7%&quot; ## [13] &quot; 11 Jefferson Park 25,448 25,859 -411 -1.6%&quot; ## [14] &quot; 12 Forest Glen 18,508 18,165 343 1.9%&quot; ## [15] &quot; 13 North Park 17,931 18,514 -583 -3.1%&quot; ## [16] &quot; 14 Albany Park 51,542 57,655 -6,113 -10.6%&quot; ## [17] &quot; 15 Portage Park 64,124 65,340 -1,216 -1.9%&quot; ## [18] &quot; 16 Irving Park 53,359 58,643 -5,284 -9.0%&quot; ## [19] &quot; 17 Dunning 41,932 42,164 -232 -0.6%&quot; ## [20] &quot; 18 Montclare 13,426 12,646 780 6.2%&quot; ## [21] &quot; 19 Belmont Cragin 78,743 78,144 599 0.8%&quot; ## [22] &quot; 20 Hermosa 25,010 26,908 -1,898 -7.1%&quot; ## [23] &quot; 21 Avondale 39,262 43,083 -3,821 -8.9%&quot; ## [24] &quot; 22 Logan Square 73,595 82,715 -9,120 -11.0%&quot; ## [25] &quot; 23 Humboldt Park 56,323 65,836 -9,513 -14.4%&quot; ## [26] &quot; 24 West Town 81,432 87,435 -6,003 -6.9%&quot; ## [27] &quot; 25 Austin 98,514 117,527 -19,013 -16.2%&quot; ## [28] &quot; 26 West Garfield Park 18,001 23,019 -5,018 -21.8%&quot; ## [29] &quot; 27 East Garfield Park 20,567 20,881 -314 -1.5%&quot; ## [30] &quot; 28 Near West Side 54,881 46,419 8,462 18.2%&quot; ## [31] &quot; 29 North Lawndale 35,912 41,768 -5,856 -14.0%&quot; ## [32] &quot; 30 South Lawndale 79,288 91,071 -11,783 -12.9%&quot; ## [33] &quot; 31 Lower West Side 35,769 44,031 -8,262 -18.8%&quot; ## [34] &quot; 32 Loop 29,283 16,388 12,895 78.7%&quot; ## [35] &quot; 33 Near South Side 21,390 9,509 11,881 124.9%&quot; ## [36] &quot; 34 Armour Square 13,391 12,032 1,359 11.3%&quot; ## [37] &quot; 35 Douglas 18,238 26,470 -8,232 -31.1%&quot; ## [38] &quot; 36 Oakland 5,918 6,110 -192 -3.1%&quot; ## [39] &quot; 37 Fuller Park 2,876 3,420 -544 -15.9%&quot; ## [40] &quot; 38 Grand Boulevard 21,929 28,006 -6,077 -21.7%&quot; ## [41] &quot; 39 Kenwood 17,841 18,363 -522 -2.8%&quot; ## [42] &quot; 40 Washington Park 11,717 14,146 -2,429 -17.2%&quot; We now repeat this operation for pop.dat[[2]]. More efficiently, we implement it as a loop, replacing i in turn by 1 and 2. This yields: nnlist &lt;- &quot;&quot; for (i in 1:2) { ppage &lt;- strsplit(pop.dat[[i]],split=&quot;\\n&quot;) nni &lt;- ppage[[1]] nni &lt;- nni[-(1:4)] nnu &lt;- unlist(nni) nnlist &lt;- c(nnlist,nnu) } At the end of the loop, we check the contents of the vector nnlist. nnlist ## [1] &quot;&quot; ## [2] &quot;Num Community Area 2010 2,000 Difference Percentage&quot; ## [3] &quot; 1 Rogers Park 54,991 63,484 -8,493 -13.4%&quot; ## [4] &quot; 2 West Ridge 71,942 73,199 -1,257 -1.7%&quot; ## [5] &quot; 3 Uptown 56,362 63,551 -7,189 -11.3%&quot; ## [6] &quot; 4 Lincoln Square 39,493 44,574 -5,081 -11.4%&quot; ## [7] &quot; 5 North Center 31,867 31,895 -28 -0.1%&quot; ## [8] &quot; 6 Lake View 94,368 94,817 -449 -0.5%&quot; ## [9] &quot; 7 Lincoln Park 64,116 64,320 -204 -0.3%&quot; ## [10] &quot; 8 Near North Side 80,484 72,811 7,673 10.5%&quot; ## [11] &quot; 9 Edison Park 11,187 11,259 -72 -0.6%&quot; ## [12] &quot; 10 Norwood Park 37,023 37,669 -646 -1.7%&quot; ## [13] &quot; 11 Jefferson Park 25,448 25,859 -411 -1.6%&quot; ## [14] &quot; 12 Forest Glen 18,508 18,165 343 1.9%&quot; ## [15] &quot; 13 North Park 17,931 18,514 -583 -3.1%&quot; ## [16] &quot; 14 Albany Park 51,542 57,655 -6,113 -10.6%&quot; ## [17] &quot; 15 Portage Park 64,124 65,340 -1,216 -1.9%&quot; ## [18] &quot; 16 Irving Park 53,359 58,643 -5,284 -9.0%&quot; ## [19] &quot; 17 Dunning 41,932 42,164 -232 -0.6%&quot; ## [20] &quot; 18 Montclare 13,426 12,646 780 6.2%&quot; ## [21] &quot; 19 Belmont Cragin 78,743 78,144 599 0.8%&quot; ## [22] &quot; 20 Hermosa 25,010 26,908 -1,898 -7.1%&quot; ## [23] &quot; 21 Avondale 39,262 43,083 -3,821 -8.9%&quot; ## [24] &quot; 22 Logan Square 73,595 82,715 -9,120 -11.0%&quot; ## [25] &quot; 23 Humboldt Park 56,323 65,836 -9,513 -14.4%&quot; ## [26] &quot; 24 West Town 81,432 87,435 -6,003 -6.9%&quot; ## [27] &quot; 25 Austin 98,514 117,527 -19,013 -16.2%&quot; ## [28] &quot; 26 West Garfield Park 18,001 23,019 -5,018 -21.8%&quot; ## [29] &quot; 27 East Garfield Park 20,567 20,881 -314 -1.5%&quot; ## [30] &quot; 28 Near West Side 54,881 46,419 8,462 18.2%&quot; ## [31] &quot; 29 North Lawndale 35,912 41,768 -5,856 -14.0%&quot; ## [32] &quot; 30 South Lawndale 79,288 91,071 -11,783 -12.9%&quot; ## [33] &quot; 31 Lower West Side 35,769 44,031 -8,262 -18.8%&quot; ## [34] &quot; 32 Loop 29,283 16,388 12,895 78.7%&quot; ## [35] &quot; 33 Near South Side 21,390 9,509 11,881 124.9%&quot; ## [36] &quot; 34 Armour Square 13,391 12,032 1,359 11.3%&quot; ## [37] &quot; 35 Douglas 18,238 26,470 -8,232 -31.1%&quot; ## [38] &quot; 36 Oakland 5,918 6,110 -192 -3.1%&quot; ## [39] &quot; 37 Fuller Park 2,876 3,420 -544 -15.9%&quot; ## [40] &quot; 38 Grand Boulevard 21,929 28,006 -6,077 -21.7%&quot; ## [41] &quot; 39 Kenwood 17,841 18,363 -522 -2.8%&quot; ## [42] &quot; 40 Washington Park 11,717 14,146 -2,429 -17.2%&quot; ## [43] &quot;Num Community Area 2010 2,000 Difference Percentage&quot; ## [44] &quot; 41 Hyde Park 25,681 29,920 -4,239 -14.2%&quot; ## [45] &quot; 42 Woodlawn 25,983 27,086 -1,103 -4.1%&quot; ## [46] &quot; 43 South Shore 49,767 61,556 -11,789 -19.2%&quot; ## [47] &quot; 44 Chatham 31,028 37,275 -6,247 -16.8%&quot; ## [48] &quot; 45 Avalon Park 10,185 11,147 -962 -8.6%&quot; ## [49] &quot; 46 South Chicago 31,198 38,596 -7,398 -19.2%&quot; ## [50] &quot; 47 Burnside 2,916 3,294 -378 -11.5%&quot; ## [51] &quot; 48 Calumet Heights 13,812 15,974 -2,162 -13.5%&quot; ## [52] &quot; 49 Roseland 44,619 52,723 -8,104 -15.4%&quot; ## [53] &quot; 50 Pullman 7,325 8,921 -1,596 -17.9%&quot; ## [54] &quot; 51 South Deering 15,109 16,990 -1,881 -11.1%&quot; ## [55] &quot; 52 East Side 23,042 23,653 -611 -2.6%&quot; ## [56] &quot; 53 West Pullman 29,651 36,649 -6,998 -19.1%&quot; ## [57] &quot; 54 Riverdale 6,482 9,809 -3,327 -33.9%&quot; ## [58] &quot; 55 Hegewisch 9,426 9,781 -355 -3.6%&quot; ## [59] &quot; 56 Garfield Ridge 34,513 36,101 -1,588 -4.4%&quot; ## [60] &quot; 57 Archer Heights 13,393 12,644 749 5.9%&quot; ## [61] &quot; 58 Brighton Park 45,368 44,912 456 1.0%&quot; ## [62] &quot; 59 McKinley Park 15,612 15,962 -350 -2.2%&quot; ## [63] &quot; 60 Bridgeport 31,977 33,694 -1,717 -5.1%&quot; ## [64] &quot; 61 New City 44,377 51,721 -7,344 -14.2%&quot; ## [65] &quot; 62 West Elsdon 18,109 15,921 2,188 13.7%&quot; ## [66] &quot; 63 Gage Park 39,894 39,193 701 1.8%&quot; ## [67] &quot; 64 Clearing 23,139 22,331 808 3.6%&quot; ## [68] &quot; 65 West Lawn 33,355 29,235 4,120 14.1%&quot; ## [69] &quot; 66 Chicago Lawn 55,628 61,412 -5,784 -9.4%&quot; ## [70] &quot; 67 West Englewood 35,505 45,282 -9,777 -21.6%&quot; ## [71] &quot; 68 Englewood 30,654 40,222 -9,568 -23.8%&quot; ## [72] &quot; 69 Greater Grand Crossing 32,602 38,619 -6,017 -15.6%&quot; ## [73] &quot; 70 Ashburn 41,081 39,584 1,497 3.8%&quot; ## [74] &quot; 71 Auburn Gresham 48,743 55,928 -7,185 -12.8%&quot; ## [75] &quot; 72 Beverly 20,034 21,992 -1,958 -8.9%&quot; ## [76] &quot; 73 Washington Heights 26,493 29,843 -3,350 -11.2%&quot; ## [77] &quot; 74 Mount Greenwood 19,093 18,820 273 1.5%&quot; ## [78] &quot; 75 Morgan Park 22,544 25,226 -2,682 -10.6%&quot; ## [79] &quot; 76 O&#39;Hare 12,756 11,956 800 6.7%&quot; ## [80] &quot; 77 Edgewater 56,521 62,198 -5,677 -9.1%&quot; ## [81] &quot; Total 2,695,598 2,896,016 -200,418 -6.9%&quot; This is now a vector of 79 elements, each of which is a string. To clean things up, strip the first (empty) element, and the last element, which is nothing but the totals. We thus extract the elements from 2 to length - 1. nnlist &lt;- nnlist[2:(length(nnlist)-1)] 1.0.1 Extracting the population values We first initialize a vector of zeros to hold the population values. It is the preferred approach to initialize a vector first if one knows its size, rather than having it grow by appending rows or columns. We use the vector command and specify the mode=\"numeric\" and give the length as the length of the list. nnpop &lt;- vector(mode=&quot;numeric&quot;,length=length(nnlist)) We again will use a loop to process each element of the list (each line of the table) one by one. We use the substr command to extract the characters between position 27 and 39 (these values were determined after taking a careful look at the structure of the table). However, there is still a problem, since the population values contain commas. We now do two things in one line of code. First, we use gsub to substitute the comma character by an empty ““. We turn the result into a numeric value by means of as.numeric. We then assign this number to position i of the vector. The resulting vector nnpop contains the population for each of the community areas. for (i in (1:length(nnlist))) { popchar &lt;- substr(nnlist[i],start=27,stop=39) popval &lt;- as.numeric(gsub(&quot;,&quot;,&quot;&quot;,popchar)) nnpop[i] &lt;- popval } nnpop ## [1] 2010 54991 71942 56362 39493 31867 94368 64116 80484 11187 37023 25448 ## [13] 18508 17931 51542 64124 53359 41932 13426 78743 25010 39262 73595 56323 ## [25] 81432 98514 18001 20567 54881 35912 79288 35769 29283 21390 13391 18238 ## [37] 5918 2876 21929 17841 11717 2010 25 25 49 31 10 31 ## [49] 2 13 44 7 15 23 29 6 9 34 13 45 ## [61] 15 31 44 18 39 23 33 55 35 30 NA 41 ## [73] 48 20 26 19 22 12 56 Creating a data frame with population values As a final step in the process of collecting the community area population information, we combine the vector with the population counts and a vector with community ID information into a data frame. Since the community area indicators are simple sequence numbers, we create such a vector to serve as the ID, again using the length of the vector to determine the extent. nnid &lt;- (1:length(nnlist)) nnid ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 ## [51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 ## [76] 76 77 78 79 We turn the vectors nnid and nnpop into a data frame using the data.frame command. Since the variable names assigned automatically are not that informative, we next force them to NID and POP2010 using the names command. Also, as we did before, we make sure the ID variable is an integer (for merging in GeoDa) by means of as.integer. neighpop &lt;- data.frame(as.integer(nnid),nnpop) names(neighpop) &lt;- c(&quot;NID&quot;,&quot;POP2010&quot;) head(neighpop) ## NID POP2010 ## 1 1 2010 ## 2 2 54991 ## 3 3 71942 ## 4 4 56362 ## 5 5 39493 ## 6 6 31867 Mapping Community Area Abandoned Vehicles Per Capita Computing abandoned vehicles per capita Before proceeding further, we left_join the community population data to the community area layer, in the same way as we did for the vehicle counts. chicago.comm &lt;- left_join(chicago.comm,neighpop, by = c(&quot;area_num_1&quot; = &quot;NID&quot;)) head(chicago.comm) ## Simple feature collection with 6 features and 11 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 441440.4 ymin: 4627153 xmax: 451817.1 ymax: 4648971 ## Projected CRS: WGS 84 / UTM zone 16N ## # A tibble: 6 × 12 ## community area shape_area perimeter area_num_1 area_numbe comarea_id comarea ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 DOUGLAS 0 46004621.… 0 35 35 0 0 ## 2 OAKLAND 0 16913961.… 0 36 36 0 0 ## 3 FULLER PA… 0 19916704.… 0 37 37 0 0 ## 4 GRAND BOU… 0 48492503.… 0 38 38 0 0 ## 5 KENWOOD 0 29071741.… 0 39 39 0 0 ## 6 LINCOLN S… 0 71352328.… 0 4 4 0 0 ## # ℹ 4 more variables: shape_len &lt;chr&gt;, geometry &lt;MULTIPOLYGON [m]&gt;, ## # AGG.COUNT &lt;int&gt;, POP2010 &lt;dbl&gt; We will now create a new variable using the tidyverse mutate command as the ratio of vehicle counts per 1000 population. chicago.comm &lt;- chicago.comm %&gt;% mutate(vehpcap = (AGG.COUNT / POP2010) * 1000) head(chicago.comm) ## Simple feature collection with 6 features and 12 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 441440.4 ymin: 4627153 xmax: 451817.1 ymax: 4648971 ## Projected CRS: WGS 84 / UTM zone 16N ## # A tibble: 6 × 13 ## community area shape_area perimeter area_num_1 area_numbe comarea_id comarea ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 DOUGLAS 0 46004621.… 0 35 35 0 0 ## 2 OAKLAND 0 16913961.… 0 36 36 0 0 ## 3 FULLER PA… 0 19916704.… 0 37 37 0 0 ## 4 GRAND BOU… 0 48492503.… 0 38 38 0 0 ## 5 KENWOOD 0 29071741.… 0 39 39 0 0 ## 6 LINCOLN S… 0 71352328.… 0 4 4 0 0 ## # ℹ 5 more variables: shape_len &lt;chr&gt;, geometry &lt;MULTIPOLYGON [m]&gt;, ## # AGG.COUNT &lt;int&gt;, POP2010 &lt;dbl&gt;, vehpcap &lt;dbl&gt; Final choropleth map For our final choropleth, we use the same procedure as for the vehicle counts, but take vehpcap as the variable instead. tm_shape(chicago.comm) + tm_polygons(&quot;vehpcap&quot;) When compared to the total counts, we see quite a different spatial distribution. In particular, the locations of the highest ratios are quite different from those of the highest counts. As a rule, one should never create a choropleth map of a spatially extensive variable, unless the size of the areal units is somehow controlled for (e.g., equal area grid cells, or equal population zones). 1.0.1.1 Optional - save the community area file as a shape file Finally, we can write the community area layer to the working directory. Note that, so far, all operations have been carried out in memory, and when you close the program, everything will be lost (unless you save your workspace). We can write the community area to a shape file (actually, four files contained in a directory) by means of the sf command st_write. This command has many options, but we just use the minimal ones. The chicago.comm object will be written to a set of files in the directory chicago_vehicles using the ESRI Shapefile format. Note that if the directory already exists, it should be deleted or renamed first, since st_write only creates a new directory. Otherwise, there will be an error message. st_write(chicago.comm,&quot;chicago_vehicles&quot;,driver=&quot;ESRI Shapefile&quot;) ## Writing layer `chicago_vehicles&#39; to data source `chicago_vehicles&#39; using driver `ESRI Shapefile&#39; ## Writing 77 features with 12 fields and geometry type Multi Polygon. Use setwd(directorypath) to specify the working directory.↩︎ Use install.packages(packagename).↩︎ A good resource on coordinate reference systems is the spatialreference.org site, which contains thousands of references in a variety of commonly used formats.↩︎ "],["exploratory-data-analysis-1.html", "Chapter 2 Exploratory Data Analysis 1 Introduction Preliminaries Analyzing the Distribution of a Single Variable Bivariate Analysis: The Scatter Plot Spatial heterogeneity", " Chapter 2 Exploratory Data Analysis 1 Introduction This notebook covers the functionality of the Exploratory Data Analysis 1 section of the GeoDa workbook. We refer to that document for details on the methodology, references, etc. The goal of these notes is to approximate as closely as possible the operations carried out using GeoDa by means of a range of R packages. The notes are written with R beginners in mind, more seasoned R users can probably skip most of the comments on data structures and other R particulars. Also, as always in R, there are typically several ways to achieve a specific objective, so what is shown here is just one way that works, but there often are others (that may even be more elegant, work faster, or scale better). For this notebook, we will use socioeconomic data for 55 New York City sub-boroughs from the GeoDa website. Our goal in this lab is show how to implement exploratory data analysis methods that deal with one (univariate) and two (bivariate) variables. Objectives After completing the notebook, you should know how to carry out the following tasks: Creating basic univariate plots, i.e., histogram and box plot Creating a scatter plot Implementing different smoothing methods in a scatter plot (linear, loess, and lowess) Showing linear fits for different subsets of the data (spatial heterogeneity) Testing the constancy of a regression slope (Chow test) R Packages used tidyverse: for general data wrangling (includes readr and dplyr) ggplot2: to make statistical plots; we use this rather than base R for increased functionality and more aesthetically pleasing plots (also part of tidyverse) ggthemes: additional themes for use with ggplot Hmisc: contains a LOWESS smoother for ggplot gap: to run the chow test R Commands used Below follows a list of the commands used in this notebook. For further details and a comprehensive list of options, please consult the R documentation. Base R: setwd, install.packages, library, head, names, summary, range, var, sd,pdf,dev.off,saveRDS,readRDS, function, lm, str, dim tidyverse: read_csv, rename, mutate, if_else, filter ggplot2: ggplot, geom_histogram, bins, theme_classic, theme_minimal, xlab, ylab, ggtitle, theme, layer_data, ggsave, geom_boxplot, stat_boxplot, geom_point, coord_fixed, geom_smooth, stat_smooth, labs, scale_color_manual ggthemes: theme_tufte Hmisc: stat_plsmo gap: chow.test Preliminaries Before starting, make sure to have the latest version of R and of packages that are compiled for the matching version of R (this document was created using R 3.5.1 of 2018-07-02). Also, make sure to set a working directory.4 We will use a relative path to the working directory to read the data set. Load packages First, we load all the required packages using the library command. If you don’t have some of these in your system, make sure to install them first as well as their dependencies.5 You will get an error message if something is missing. If needed, just install the missing piece and everything will work after that. Note that ggplot2 does not need to be loaded separately since it is included in the tidyverse package collection. library(tidyverse) library(ggthemes) library(Hmisc) library(gap) library(geodaData) Obtaining the data The data to implement the operations in this workbook are contained in NYC Data on the GeoDa support web site. After the file is downloaded, it must be unzipped (e.g., double click on the file). The nyc folder should be moved to the current working directory for the path names we use below to work correctly. Creating an initial data frame We use the tidyverse function read_csv to read the data into a data frame nyc.data. We could also have used the base R read.csv, but read_csv is a bit more robust and creates a tibble, a data frame with some additional information. As usual, we check the contents of the data frame with a head command. nyc.data &lt;- nyc head(nyc.data) ## # A tibble: 6 × 34 ## bor_subb NAME CODE SUBBOROUGH FORHIS06 FORHIS07 FORHIS08 FORHIS09 FORWH06 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 501 North S… 501 North Sho… 37.1 34.0 27.4 29.3 13.3 ## 2 502 Mid-Isl… 502 Mid-Island 28.0 18.1 24.0 31.2 20.1 ## 3 503 South S… 503 South Sho… 10.7 12.1 9.69 14.7 10.3 ## 4 401 Astoria 401 Astoria 52.1 54.0 54.7 47.8 38.4 ## 5 402 Sunnysi… 402 Sunnyside… 62.7 69.4 67.1 58.3 37.1 ## 6 403 Jackson… 403 Jackson H… 68.5 68.5 66.5 69.2 34.4 ## # ℹ 25 more variables: FORWH07 &lt;dbl&gt;, FORWH08 &lt;dbl&gt;, FORWH09 &lt;dbl&gt;, ## # HHSIZ1990 &lt;dbl&gt;, HHSIZ00 &lt;dbl&gt;, HHSIZ02 &lt;dbl&gt;, HHSIZ05 &lt;dbl&gt;, ## # HHSIZ08 &lt;dbl&gt;, KIDS2000 &lt;dbl&gt;, KIDS2005 &lt;dbl&gt;, KIDS2006 &lt;dbl&gt;, ## # KIDS2007 &lt;dbl&gt;, KIDS2008 &lt;dbl&gt;, KIDS2009 &lt;dbl&gt;, RENT2002 &lt;dbl&gt;, ## # RENT2005 &lt;dbl&gt;, RENT2008 &lt;dbl&gt;, RENTPCT02 &lt;dbl&gt;, RENTPCT05 &lt;dbl&gt;, ## # RENTPCT08 &lt;dbl&gt;, PUBAST90 &lt;dbl&gt;, PUBAST00 &lt;dbl&gt;, YRHOM02 &lt;dbl&gt;, ## # YRHOM05 &lt;dbl&gt;, YRHOM08 &lt;dbl&gt; Making the variable names compatible Note, that in contrast to GeoDa (where the dbf file is read), reading the csv file into a data frame results in almost all the variable names being in caps. We confirm this with a names command: names(nyc.data) ## [1] &quot;bor_subb&quot; &quot;NAME&quot; &quot;CODE&quot; &quot;SUBBOROUGH&quot; &quot;FORHIS06&quot; ## [6] &quot;FORHIS07&quot; &quot;FORHIS08&quot; &quot;FORHIS09&quot; &quot;FORWH06&quot; &quot;FORWH07&quot; ## [11] &quot;FORWH08&quot; &quot;FORWH09&quot; &quot;HHSIZ1990&quot; &quot;HHSIZ00&quot; &quot;HHSIZ02&quot; ## [16] &quot;HHSIZ05&quot; &quot;HHSIZ08&quot; &quot;KIDS2000&quot; &quot;KIDS2005&quot; &quot;KIDS2006&quot; ## [21] &quot;KIDS2007&quot; &quot;KIDS2008&quot; &quot;KIDS2009&quot; &quot;RENT2002&quot; &quot;RENT2005&quot; ## [26] &quot;RENT2008&quot; &quot;RENTPCT02&quot; &quot;RENTPCT05&quot; &quot;RENTPCT08&quot; &quot;PUBAST90&quot; ## [31] &quot;PUBAST00&quot; &quot;YRHOM02&quot; &quot;YRHOM05&quot; &quot;YRHOM08&quot; We now use the tidyverse rename function to turn the all-caps variables into lower case for the examples we will use. As in the GeoDa workbook, we only use three variables, kids2009, kids2000, and pubast00. nyc.data &lt;- nyc.data %&gt;% rename(&quot;kids2009&quot; = &quot;KIDS2009&quot;, &quot;kids2000&quot; = &quot;KIDS2000&quot;, &quot;pubast00&quot; = &quot;PUBAST00&quot;) names(nyc.data) ## [1] &quot;bor_subb&quot; &quot;NAME&quot; &quot;CODE&quot; &quot;SUBBOROUGH&quot; &quot;FORHIS06&quot; ## [6] &quot;FORHIS07&quot; &quot;FORHIS08&quot; &quot;FORHIS09&quot; &quot;FORWH06&quot; &quot;FORWH07&quot; ## [11] &quot;FORWH08&quot; &quot;FORWH09&quot; &quot;HHSIZ1990&quot; &quot;HHSIZ00&quot; &quot;HHSIZ02&quot; ## [16] &quot;HHSIZ05&quot; &quot;HHSIZ08&quot; &quot;kids2000&quot; &quot;KIDS2005&quot; &quot;KIDS2006&quot; ## [21] &quot;KIDS2007&quot; &quot;KIDS2008&quot; &quot;kids2009&quot; &quot;RENT2002&quot; &quot;RENT2005&quot; ## [26] &quot;RENT2008&quot; &quot;RENTPCT02&quot; &quot;RENTPCT05&quot; &quot;RENTPCT08&quot; &quot;PUBAST90&quot; ## [31] &quot;pubast00&quot; &quot;YRHOM02&quot; &quot;YRHOM05&quot; &quot;YRHOM08&quot; Analyzing the Distribution of a Single Variable We follow the discussion in the GeoDa workbook and start with the common univariate descriptive graphs, the histogram and box plot. Before covering the specifics, we provide a brief overview of the principles behind the ggplot operations. Note that linking and brushing between a plot and a map is not (yet) readily implemented in R, so that our discussion will focus primarily on static graphs. A quick introduction to ggplot We will be using the commands in the ggplot2 package for the descriptive statistics plots. There are many options to create nice looking graphs in R, including the functionality in base R, but we chose ggplot2 for its clean logic and its similarity to the tmap package that we already encountered (in fact, tmap uses the same layered logic as ggplot).6 An in-depth introduction to ggplot is beyond our scope, but a quick overview can be found in the Data Visualization chapter of Wickham and Grolemund’s R for Data Science book, and full details are covered in Wickham’s ggplot2: elegant graphics for data analysis (2nd Edition) (Springer Verlag, 2016). The logic behind ggplot is an implementation of Wilkinson’s grammar for graphics, using the concept of layers. These are the components that make up a plot, such as a data set, aesthetic mappings (variables for different aspects of the graph, such as the x and y-axes, colors, shapes, etc.), statistical transformations, a geometric object and position adjustments. Several layers can be drawn on top of each other, providing the ability to create incredibly complex graphs. For now, the main parts to concentrate on are the data set and the aesthetics, or aes. The latter are typically (at least) the variables to be plotted. These are usually declared in the main ggplot command, e.g., ggplot(dataset,aes(x=var1,y=var2)) and apply to all the following layers. However, they can also be specified for each layer individually. Next follow one or more geometric objects, geom_* and various adjustments, added to the first command by means of a plus sign, just as we saw how a tmap choropleth map was constructed. The terminology may seem a little unfamiliar at first, but as long as you remember that aes are the variables and the geom_* are the plot types, you will be on your way. Histogram We start with the simple histogram command. As in the GeoDa workbook, we will use the kids2009 variable. The geom for a histogram is geom_histogram. In contrast to most plots in ggplot, only one variable needs to be passed. The general setup for ggplot is to think of the graph as a two-dimensional representation, with the x variable for the x axis and the y variable for the y-axis. In a histogram, the vertical axis is by default taken to be the count of the observations in each bin.7 The three pieces we need to create the plot are the data set (data), nyc.data, the aesthetic (aes), kids2009, and the geom, geom_histogram. The command is as follows, with all the other settings left to their default: ggplot(data=nyc.data,aes(kids2009)) + geom_histogram() The resulting histogram is not very informative, and the first thing we will do is heed the warning to pick a better bin width. Selecting the number of histogram bins The standard way in ggplot is to adjust the number of bins indirectly, by means of the binwidth option, i.e., the range of values that make up a bin, in the units of the variable under consideration. To keep the parallel with the GeoDa workbook, we instead use the option bins, which sets the number of bins directly. The resulting histogram now matches the one in GeoDa (except for the lack of color, which is immaterial). ggplot(data=nyc.data,aes(kids2009)) + geom_histogram(bins=7) As in the GeoDa workbook, we can now change the number of bins to 5, which yields the following histogram. ggplot(data=nyc.data,aes(kids2009)) + geom_histogram(bins=5) Spiffing up the graph The graph as shown is just rudimentary. There are many options in ggplot to change the appearance of the graph, too many to cover here. But to illustrate some basic features, below, we add a label for the x and y axes using xlab and ylab, and a title for the graph with ggtitle. An unfortunate aspect of the latter is that it left aligns the text, whereas we would typically want it to be centered over the graph. We can adjust this using the very powerful theme option. But first the basics. Every graph has a theme, which sets the main parameters for its appearance. The default theme with the grey grids, separated by white lines is theme_grey( ). If we want to change this, we can specify one of the other themes. For example, a classic graph a la base R plot, without background shading or grid lines is theme_classic( ). In order to obtain this specialized look, we set the associated theme command. Our histogram in this theme looks as follows, with a label on the x and y axis, and a title (and back to 7 bins). ggplot(data=nyc.data,aes(kids2009)) + geom_histogram(bins=7) + xlab(&quot;Percent kids in 2009&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Example Histogram&quot;) + theme_classic() There are seven built-in themes as well as several contributed ones. Another built-in example is theme_minimal( ), shown next. ggplot(data=nyc.data,aes(kids2009)) + geom_histogram(bins=7) + xlab(&quot;Percent kids in 2009&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Example Histogram&quot;) + theme_minimal() In addition, the package ggthemes contains several additional themes that look extremely professional. For example, theme_tufte( ). ggplot(data=nyc.data,aes(kids2009)) + geom_histogram(bins=7) + xlab(&quot;Percent kids in 2009&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Example Histogram&quot;) + theme_tufte() Besides selecting a different default theme, we can also override the basic settings associated with the current theme. For example, we adjust the plot.title (of course, you need to know what everything is called). Specifically, we set the element_text property’s horizontal justification (hjust) to 0.5. This centers the title. The number of other refinements is near infinite. Again, using the default theme_grey( ): ggplot(data=nyc.data,aes(kids2009)) + geom_histogram(bins=7) + xlab(&quot;Percent kids in 2009&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Example Histogram&quot;) + theme(plot.title = element_text(hjust = 0.5)) Histogram summary statistics The histogram in GeoDa has an option to display the contents of each bin as well as some descriptive statistics. As anything in R, the plot created by ggplot is nothing but an object. When we enter the commands as above, starting with ggplot, the result is drawn directly to the screen. But we can also assign the plot object to a variable. This variable will then contain all the information needed to draw the graph, which includes the count of observations in each bin, the min and max values for each bin, etc. For example, we can assign our histogram plot to the plot.data object, and then extract the information using the layer_data function. The result is a data frame. plot.data &lt;- ggplot(data=nyc.data,aes(kids2009)) + geom_histogram(bins=7) + xlab(&quot;Percent kids in 2009&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Example Histogram&quot;) + theme(plot.title = element_text(hjust = 0.5)) layer_data(plot.data) ## y count x xmin xmax density ncount ndensity ## 1 1 1 0.0000 -4.0109 4.0109 0.002266551 0.05555556 0.05555556 ## 2 2 2 8.0218 4.0109 12.0327 0.004533102 0.11111111 0.11111111 ## 3 4 4 16.0436 12.0327 20.0545 0.009066204 0.22222222 0.22222222 ## 4 8 8 24.0654 20.0545 28.0763 0.018132407 0.44444444 0.44444444 ## 5 18 18 32.0872 28.0763 36.0981 0.040797917 1.00000000 1.00000000 ## 6 17 17 40.1090 36.0981 44.1199 0.038531366 0.94444444 0.94444444 ## 7 5 5 48.1308 44.1199 52.1417 0.011332755 0.27777778 0.27777778 ## flipped_aes PANEL group ymin ymax colour fill linewidth linetype alpha ## 1 FALSE 1 -1 0 1 NA grey35 0.5 1 NA ## 2 FALSE 1 -1 0 2 NA grey35 0.5 1 NA ## 3 FALSE 1 -1 0 4 NA grey35 0.5 1 NA ## 4 FALSE 1 -1 0 8 NA grey35 0.5 1 NA ## 5 FALSE 1 -1 0 18 NA grey35 0.5 1 NA ## 6 FALSE 1 -1 0 17 NA grey35 0.5 1 NA ## 7 FALSE 1 -1 0 5 NA grey35 0.5 1 NA The convention used to create the histogram in ggplot is slightly different from that in GeoDa, hence small differences in the bounds of the bins. The summary statistics give the number of observations in each bin (count), the mid-point of the bin (x), and the lower and upper bound for the bin (xmin and xmax). Unlike GeoDa, where the histogram starts at the minimum value and ends at the maximum value, the histogram in ggplot starts with the minimum value at the mid-point of the lowest bin, and the maximum value at the mid-point of the upper bin. Assigning (part of) a graph to an object Any subset of ggplot commands can be assigned to an object, which can save on some typing if the same data set and variables are used for several plots. For example, we assign the main ggplot command with the geom_histogram to the object baseplt. As such, this does not draw anything. Next, we add the different options to the baseplt object, and the graph appears. baseplt &lt;- ggplot(data=nyc.data,aes(kids2009)) + geom_histogram(bins=7) baseplt + xlab(&quot;Percent kids in 2009&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Example Histogram&quot;) + theme(plot.title = element_text(hjust = 0.5)) Other descriptive statistics The usual descriptive statistics can be displayed by means of the base R summary command. In principle, we could assign these to an object and then add them to the plot using the geom_text geom, but that is beyond the current scope. We can easily obtain the descriptive statistics provided by GeoDa that are not contained in the R summary command, by means of range, var, and sd, for the range, variance and standard deviation. summary(nyc.data$kids2009) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00 26.69 33.53 32.07 39.68 48.13 range(nyc.data$kids2009) ## [1] 0.0000 48.1308 var(nyc.data$kids2009) ## [1] 107.535 sd(nyc.data$kids2009) ## [1] 10.36991 Writing the graph to a file In our discussion so far, the graphs are drawn to the screen and then disappear. To save a ggplot graph to a file for publication, there are two ways to proceed. One is the classic R approach, in which first a device is opened, e.g., by means of a pdf command, then the plot commands are entered, and finally the device is turned off by means of dev.off(). Note that it is always a good idea to specify the dimension of the graph (in inches). If not, the results can be unexpected. pdf(&quot;hist.pdf&quot;,height=3,width=3) ggplot(data=nyc.data,aes(kids2009)) + geom_histogram(bins=7) + xlab(&quot;Percent kids in 2009&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Example Histogram&quot;) + theme(plot.title = element_text(hjust = 0.5)) dev.off() ## quartz_off_screen ## 2 In addiiton to the standard R approach, ggplot also has the ggsave command, which does the same thing. It requires the name for the output file, but derives the proper format from the file extension. For example, an output file with a png file extension will create a png file, and similarly for pdf, etc. The second argument specifies the plot. It is optional, and when not specified, the last plot is saved. Again, it is a good idea to specify the width and height (in inches). In addition, for raster files, the dots per inch (dpi) can be set as well. The default is 300, which is fine for most use cases, but for high resolution graphs, one can set the dpi to 600, as in the example below. hist.plot &lt;- ggplot(data=nyc.data,aes(kids2009)) + geom_histogram(bins=7) + xlab(&quot;Percent kids in 2009&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Example Histogram&quot;) + theme(plot.title = element_text(hjust = 0.5)) ggsave(&quot;hist.png&quot;,plot=hist.plot,width=3,height=3,dpi=600) Saving the graph object An alternative approach to keep a plot object is to assign the plot commands to a variable and then save this to disk, using the standard R command saveRDS. This can later be brought back into an R session using readRDS. To save the plot, we need to specify a file name with an .rds file extension. saveRDS(hist.plot,&quot;hist.rds&quot;) At some later point (or in a different R session), we can then read the object and plot it. Note that we do not need to assign it to the same variable name as before. For example, here we call the graph object newplot. newplot &lt;- readRDS(&quot;hist.rds&quot;) newplot Box plot The box plot, also referred to as Tukey’s box and whisker plot, is an alternative way to visualize the distribution of a single variable, with a focus on descriptive statistics such as quartiles and the median.8 We continue our example using the kids2009 variable. We first consider the default option, then move on to various optional settings. Default settings The minimal arguments to create a boxplot are the data set and the x and y variables passed to aes. As mentioned above, the logic behind the graphs in ggplot is two-dimensional, so both x and y need to be specified. The x variable is used to create separate box plots for different subsets of the data. In our simple example, we don’t need this feature, so we set the x variable to empty, i.e., ” “. The y variable is the actual variable of interest, kids2009. The resulting graph is shown below. ggplot(data=nyc.data,aes(x=&quot;&quot;,y=kids2009)) + geom_boxplot() The box encloses the first and third quartile and shows the median as a thick horizontal bar. The vertical lines show the range of data, not the extent of the fences, as is the case in GeoDa. The single dot at value 0 is the lower outlier. Box plot statistics Unlike what is the case in GeoDa, there is no straightforward way to show the descriptive statistics on the graph. However, we can access the statistics, extract them, and then use text labeling techniques to add them to the plot. We will not pursue that here, but we will illustrate the type of statistics associated with the box plot. As we did for the histogram, we will assign the ggplot object to a variable and then use the layer_data function to extract the information. box.plt &lt;- ggplot(data=nyc.data,aes(x=&quot;&quot;,y=kids2009)) + geom_boxplot() box.dta &lt;- layer_data(box.plt) box.dta ## ymin lower middle upper ymax outliers notchupper notchlower x ## 1 8.6623 26.69425 33.5284 39.6773 48.1308 0 36.2944 30.7624 1 ## flipped_aes PANEL group ymin_final ymax_final xmin xmax xid newx new_width ## 1 FALSE 1 1 0 48.1308 0.625 1.375 1 1 0.75 ## weight colour fill alpha shape linetype linewidth ## 1 1 grey20 white NA 19 solid 0.5 The result is a data frame that contains all the information needed to create the graph. The descriptive statistics require a little clarification. The values for lower and upper are, respectively, the values for the first and third quartile, and middle is the median. ymin (8.6623) and ymax (48.1308) are not the smallest and largest values overall, but the smallest and largest values that fall inside the fences.9 They are the begin and end points of the vertical lines in the plot. outliers contains a list with the outlier values. In our example, there is just one, the value 0 (compare to Figure 18 in the GeoDa workbook). The fences, also sometimes called whiskers, are not contained among the statistics, but they can be easily computed. We illustrate a simple function to accomplish this (note that this function does not implement any error checking and is purely illustrative of the concepts involved). As anything else in R, a function is an object that is assigned to a name. It takes arguments and returns the result. For example, the function box.desc given below takes the layer_data object as the argument box.lyr, extracts the quartiles to compute the interquartile range and to calculate the fences. We also pass the multiplier as mult, with a default value of 1.5 (the default value is set by the equal sign). box.desc &lt;- function(box.lyr,mult=1.5) { # function to computer lower and upper fence in a box plot # box.lyr: a box plot layer_data object # mult: the multiplier for the fence calculation, default = 1.5 iqr &lt;- box.lyr$upper - box.lyr$lower # inter-quartile range upfence &lt;- box.lyr$upper + mult * iqr # upper fence lofence &lt;- box.lyr$lower - mult * iqr # lower fence return(c(lofence,upfence)) } We can now pass the box.dta results to this function to obtain the lower and upper fences. box.desc(box.dta) ## [1] 7.219675 59.151875 These results match the values in the GeoDa workbook. Changing the fence As we did in the GeoDa workbook, we can change the multiplier value to compute the fences. The default is 1.5, but we can set this to 3.0 by means of the coef option. We again assign the plot to an object to both illustrate the graph and the associated statistics. box.plt3 &lt;- ggplot(data=nyc.data,aes(x=&quot;&quot;,y=kids2009)) + geom_boxplot(coef=3) box.plt3 As in the GeoDa example, there are no longer any outliers. In the graph, the lowest point on the vertical line now corresponds with the value of 0. We extract the statistics as before. box.dta3 &lt;- layer_data(box.plt3) box.dta3 ## ymin lower middle upper ymax outliers notchupper notchlower x ## 1 0 26.69425 33.5284 39.6773 48.1308 36.2944 30.7624 1 ## flipped_aes PANEL group ymin_final ymax_final xmin xmax xid newx new_width ## 1 FALSE 1 1 0 48.1308 0.625 1.375 1 1 0.75 ## weight colour fill alpha shape linetype linewidth ## 1 1 grey20 white NA 19 solid 0.5 The statistics are the same, except that the value for ymin is now 0. We double check the results for the fences using our function box.desc, but now pass box.dta3 and set mult=3.0. box.desc(box.dta3,mult=3.0) ## [1] -12.25490 78.62645 Since the lower fence is negative, the value of 0 is no longer an outlier. Fancier options As is, the default box plot is pretty rudimentary. We will illustrate the power of ggplot by adding a number of features to the plot in order to mimic the visual representation given in GeoDa. First, we will remove the label for the x-axis by setting it to ““, and add a title using ggtitle. As we did for the histogram, we will center the title over the graph. To save on some typing, we will assign the ggplot command with its arguments to the variable base.plt and then build the graph by adding layers. First, just the labels. base.plt &lt;- ggplot(data=nyc.data,aes(x=&quot;&quot;,y=kids2009)) base.plt + geom_boxplot() + xlab(&quot;&quot;) + ggtitle(&quot;Example Box Plot&quot;) + theme(plot.title = element_text(hjust=0.5)) Next, we want to give the box plot a color, as in GeoDa. This is accomplished with the color (for the outlines) and fill (for the inside of the box) options to geom_boxplot. In addition, we can give the outlier point a different color by means of outlier.color. For example, setting the color to black, with the fill to purple (if we set both to the same color, we can no longer distinguish the median), and the outlier.color to red, we obtain: base.plt + geom_boxplot(color=&quot;black&quot;,fill=&quot;purple&quot;,outlier.color=&quot;red&quot;) + xlab(&quot;&quot;) + ggtitle(&quot;Example Box Plot&quot;) + theme(plot.title = element_text(hjust=0.5)) So far, the box plots do not show the fences, the way they do in GeoDa. This can be remedied, but not quite in the same way as in GeoDa. In ggplot, the fences are drawn at the location of the extreme values, the ymin and ymax we saw before, and not at the location of the fence cut-off values, as in GeoDa. The fences are obtained from the stat_boxplot function, by passing the geom as errorbar. The result is as shown below. base.plt + geom_boxplot(color=&quot;black&quot;,fill=&quot;purple&quot;,outlier.color=&quot;red&quot;) + stat_boxplot(geom=&quot;errorbar&quot;) + xlab(&quot;&quot;) + ggtitle(&quot;Example Box Plot&quot;) + theme(plot.title = element_text(hjust=0.5)) One final refinement. In GeoDa, the box plot also shows the locations of the actual observations as points on the central axis. We obtain the same effect by adding geom_point with color blue. We draw the points first, and the box plot on top of it, using the layers logic. However, we want to make sure that the central box doesn’t mask the points, which it does when the transparency is kept as the default. To accomplish this, we set the alpha level for both points and box plot at 0.5. The result comes as close to the GeoDa visualization as we can get without going overboard. base.plt + geom_point(color=&quot;blue&quot;,alpha=0.5) + geom_boxplot(color=&quot;black&quot;,fill=&quot;purple&quot;,outlier.color=&quot;red&quot;,alpha=0.5) + stat_boxplot(geom=&quot;errorbar&quot;) + xlab(&quot;&quot;) + ggtitle(&quot;Example Box Plot&quot;) + theme(plot.title = element_text(hjust=0.5)) As before, we can write this plot to a file using ggsave, or save the object for future use with saveRDS. Bivariate Analysis: The Scatter Plot The scatter plot shows the relationship between two variables as points with cartesian (x, y) coordinates matching the value for each variable, one on the x-axis, the other on the y-axis. In ggplot the scatter plot is constructed by means of a geom_point. The aesthetics are mapped to the variables for the x and y axis. We mimic the example in the GeoDa workbook and use kids2000 for x, and pubast00 for y. The bare bones scatter plot is obtained as follows: ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + geom_point() The graph looks slightly different from the one in GeoDa because of a different aspect ratio (the ratio between the scale used for the y-axis to that for the x-axis). In ggplot, the aspect ratio is set as an argument to the coord_fixed command. We can obtain a scatter plot that more closely mimics the shape of the one in GeoDa by setting the aspect ratio to 55/25, i.e., the ratio of the range on the x-axis over the range of the y-axis in the example in the GeoDa workbook (Figure 24). ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + geom_point() + coord_fixed(ratio=55.0/25.0) As we saw before, we can customize the graph further with a title and labels, but we will not pursue that here. The main interest is in bringing out the overall pattern of bivariate association by means of a scatter plot smoother, to which we turn next. Smoothing the scatter plot Scatter plot smoothers are implemented through the geom_smooth command in ggplot. Options include a linear smoother, as method = lm, and a nonlinear loess smoother, as method = loess. Note that the loess smoother is not quite the same as the LOWESS smoother implemented in GeoDa. The latter is not included in ggplot, but can be implemented by means of the stat_plsmo function from the Hmisc package. We consider each smoother in turn. Linear smoother The linear smoother is added to the plot by including the geom_smooth command after the geom_point call. The method is set as lm. In order to better distinguish the fitted line from the points, we set its color to blue, and add a centered title to specify the smoothing algorithm using ggtitle (also, in what follows, we ignore the aspect ratio issue). ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + geom_point() + geom_smooth(method=lm, color=&quot;blue&quot;) + ggtitle(&quot;Linear Smoother&quot;) + theme(plot.title = element_text(hjust=0.5)) By default, the linear smoother includes a 95% confidence interval band (the grey band around the blue line). To turn this off, we need to set the option se to FALSE, as below. ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + geom_point() + geom_smooth(method=lm, color=&quot;blue&quot;,se=FALSE) + ggtitle(&quot;Linear Smoother&quot;) + theme(plot.title = element_text(hjust=0.5)) Extracting the linear regression results In GeoDa, a linear fit to the scatter plot also yields the results of the regression, such as the coefficients, their standard errors, p-values and an R2 measure of fit. This is not the case in ggplot (by design). However, we can readily obtain these results from the lm function in base R. In its bare minimum, this function takes as arguments a formula and a data set (actually, the latter is not absolutely necessary, depending on how the variables are referred to). In our example, we specify the regression formula as pubast00 ~ kids2000 and the data set as nyc.data.10 Rather than just printing the results of the regression, we assign it to an object, reg1 in our example below. We then apply the summary command to this object, which we assign to yet another object (reg1.sum). When we list the latter, we see a summary of the regression results. reg1 &lt;- lm(pubast00 ~ kids2000, data=nyc.data) reg1.sum &lt;- summary(reg1) reg1.sum ## ## Call: ## lm(formula = pubast00 ~ kids2000, data = nyc.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.5284 -3.7925 -0.2762 3.6696 9.2054 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.61829 2.13013 -2.638 0.0109 * ## kids2000 0.39000 0.05645 6.909 6.32e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.682 on 53 degrees of freedom ## Multiple R-squared: 0.4739, Adjusted R-squared: 0.4639 ## F-statistic: 47.73 on 1 and 53 DF, p-value: 6.322e-09 The reason for taking what may seem like a circuitous route is that the summary object is simply a list of elements that correspond to aspects of the regression result. Of course, one needs to know what these are called, but, in doubt, a str command will reveal the full structure. str(reg1.sum) ## List of 11 ## $ call : language lm(formula = pubast00 ~ kids2000, data = nyc.data) ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language pubast00 ~ kids2000 ## .. ..- attr(*, &quot;variables&quot;)= language list(pubast00, kids2000) ## .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. ..$ : chr [1:2] &quot;pubast00&quot; &quot;kids2000&quot; ## .. .. .. ..$ : chr &quot;kids2000&quot; ## .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;kids2000&quot; ## .. ..- attr(*, &quot;order&quot;)= int 1 ## .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. ..- attr(*, &quot;response&quot;)= int 1 ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. ..- attr(*, &quot;predvars&quot;)= language list(pubast00, kids2000) ## .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;pubast00&quot; &quot;kids2000&quot; ## $ residuals : Named num [1:55] -3.703 -6.222 -8.528 -0.276 -3.061 ... ## ..- attr(*, &quot;names&quot;)= chr [1:55] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ coefficients : num [1:2, 1:4] -5.6183 0.39 2.1301 0.0564 -2.6375 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;kids2000&quot; ## .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Std. Error&quot; &quot;t value&quot; &quot;Pr(&gt;|t|)&quot; ## $ aliased : Named logi [1:2] FALSE FALSE ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;kids2000&quot; ## $ sigma : num 4.68 ## $ df : int [1:3] 2 53 2 ## $ r.squared : num 0.474 ## $ adj.r.squared: num 0.464 ## $ fstatistic : Named num [1:3] 47.7 1 53 ## ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;value&quot; &quot;numdf&quot; &quot;dendf&quot; ## $ cov.unscaled : num [1:2, 1:2] 0.206953 -0.005238 -0.005238 0.000145 ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;kids2000&quot; ## .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;kids2000&quot; ## - attr(*, &quot;class&quot;)= chr &quot;summary.lm&quot; For example, we see that the coefficients are in the list element coefficients. They can be extracted by means of the standard $ notation. reg1.sum$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.6182932 2.13013007 -2.637535 1.093595e-02 ## kids2000 0.3899956 0.05644857 6.908866 6.322337e-09 Similarly, we can extract the R2 and adjusted R2. c(reg1.sum$r.squared,reg1.sum$adj.r.squared) ## [1] 0.4738536 0.4639263 We can now use the text and labeling functionality of ggplot to place these results on the graph. We don’t pursue this any further. Loess smoother The default nonlinear smoother in ggplot uses the loess algorithm as a locally weighted regression model. This is similar in spirit to the LOWESS method used in GeoDa, but not the same.11 The implementation is along the same lines as the linear smoother, using geom_smooth, with the only difference that the method is now loess, as shown below. ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + geom_point() + geom_smooth(method=loess, color=&quot;blue&quot;) + ggtitle(&quot;Loess Smoother&quot;) + theme(plot.title = element_text(hjust=0.5)) As in any local regression method, an important parameter is how much of the data is used in the local fit, the so-called span. This is typically set to 2/3 of the data by default. A narrower span will yield a smoother that emphasizes local changes. In order to set the span parameter, we use the stat_smooth command. It is in all respects equivalent to geom_smooth, but allows for somewhat more flexibility. For example, we see the difference between the default and a smoother with a span = 0.4. We also turn off the confidence interval by setting se = FALSE. ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + stat_smooth(method=&quot;loess&quot;,span=0.4,color=&quot;blue&quot;,se=FALSE) + geom_point() + ggtitle(&quot;Loess Smoother - Span=0.4&quot;) + theme(plot.title = element_text(hjust=0.5)) And even more with a span = 0.2. ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + stat_smooth(method=&quot;loess&quot;,span=0.2,color=&quot;blue&quot;,se=FALSE) + geom_point() + ggtitle(&quot;Loess Smoother - Span=0.2&quot;) + theme(plot.title = element_text(hjust=0.5)) LOWESS smoother The LOWESS smoother is not implemented in ggplot, but can be found in the Hmisc package. The approach taken illustrates an alternative way to compute a smoother, similar to the stat_smooth function in ggplot. The latter is equivalent to geom_smooth, but allows for non-standard geoms to visualize the results. We won’t pursue that here, but want to point out that this is the spirit in which the function stat_plsmo is implemented. The default is to use a span of 2/3 of the observations (the stat_plsmo command does not have a confidence interval option, so that we do not need to set se). ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + stat_plsmo(color=&quot;blue&quot;) + geom_point() + ggtitle(&quot;LOWESS Smoother&quot;) + theme(plot.title = element_text(hjust=0.5)) When compared to the loess results, we can distinguish minor differences. These become more pronounced when setting the span=0.4. ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + stat_plsmo(span=0.4,color=&quot;blue&quot;) + geom_point() + ggtitle(&quot;LOWESS Smoother - Span=0.4&quot;) + theme(plot.title = element_text(hjust=0.5)) And even more when setting the span=0.2. ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + stat_plsmo(span=0.2,color=&quot;blue&quot;) + geom_point() + ggtitle(&quot;LOWESS Smoother - Span=0.2&quot;) + theme(plot.title = element_text(hjust=0.5)) Putting it all together In GeoDa, it is easy to show both the linear and LOWESS smoother on the same plot. In order to accomplish the same in ggplot, we will use some of the really powerful customization options to create a graph that contains all three smoothing methods. We distinguish between them by setting the color argument in aes to the name of the method, in essence a constant (not a variable). In other words, the color argument to aes is not set to a variable, where it would take a different color depending on the value of that variable, but to a constant, where the color is fixed. This will yield a different color for each of the methods. In order to highlight the curves themselves, we turn off se for lm and loess. The plot will include a legend by default, showing the color with the name of the method. The default title of the legend will be color, i.e., the argument used to reflect the categories. We override this by means of the labs command (for legend labeling), and set color=\"Method\". The result is as shown below. ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + stat_plsmo(aes(color=&quot;lowess&quot;)) + geom_point() + geom_smooth(aes(color=&quot;lm&quot;),method=lm,se=FALSE) + geom_smooth(aes(color=&quot;loess&quot;),method=loess,se=FALSE) + ggtitle(&quot;Comparison of Smoothing Methods&quot;) + theme(plot.title = element_text(hjust=0.5)) + labs(color=&quot;Method&quot;) Spatial heterogeneity In GeoDa, it is relatively straightforward to assess the structural stability of the regression coefficients across selected and unselected observations. The selection can be carried out interactively, from the scatter plot or from any other open view, through linking and brushing. The corresponding regression lines and coefficient information are updated dynamically. In R, this is not (yet) quite possible. To illustrate the visualization and assessment of spatial heterogeneity, we will assume we have a way to express the selection as a logical statement. As it turns out, the sub-boroughs in Manhattan have a CODE from 301 to 310, and the sub-boroughs in the Bronx have a CODE from 101 to 110. While this is not exactly the example given in the GeoDa workbook, it is easy enough to replicate. We will proceed by using the mutate command to create a new variable manbronx that matches this selection. For all practical purposes, this gives the same result as if we had selected these observations in a map. Then we will use this classification to create a scatterplot with a separate regression line for the selected and unselected observations, as well as for all the observations, mimicking the behavior of GeoDa (but in a static fashion). Structural breaks in the scatter plot The first step in our process is to create the new variable manbronx using mutate. We also use the if_else command from dplyr to create values for the new variable of Select when the condition is true, and Rest when the condition is false. The condition checks whether the values for CODE are between 300 and 311 (using the symbol &amp; for the logical and), or (using the symbol |) between 100 and 111, the codes for Manhattan and the Bronx. As a check, we list the values for our new variable (since there are only 55 observations, this is not too onerous). nyc.data &lt;- nyc.data %&gt;% mutate(manbronx = if_else((CODE &gt; 300 &amp; CODE &lt; 311) | (CODE &gt; 100 &amp; CODE &lt; 111),&quot;Select&quot;,&quot;Rest&quot;)) nyc.data$manbronx ## [1] &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; ## [9] &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; ## [17] &quot;Rest&quot; &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; ## [25] &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; ## [33] &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; &quot;Select&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; ## [41] &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; ## [49] &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; &quot;Rest&quot; Next, we create the plot. There are several new elements that we introduce, highlighting the flexibility of ggplot. First, we set the color of the points to correspond with the two categories in the manbronx variable by specifying aes(color=manbronx) for geom_point (i.e., the aesthetic color is mapped to the values of the variable manbronx). Then, we create two separate linear regression lines, one for each category, again by setting aes(color=manbronx) in the geom_smooth command. In order not to overwhelm the graph, we turn the confidence band off (se=FALSE). To construct a regression line for the full sample, we again use geom_smooth, but now we set the color explicitly to black. Since this is outside the aes setting, the regression line is for the full sample. We next set the colors for selected and unselected observations to red and blue, to match the color code in GeoDa (the default setting will have Rest colored red, and Select blue, which is the opposite of the behavior in GeoDa). To accomplish this, we use scale_color_manual to set the values to blue and red, in this order (unselected comes first, since it matches FALSE in the logical statement). Finally, we use labs as before to specify the title for the legend to Selection, and add a centered title. Except for the aspect ratio, the result looks like what one would obtain in GeoDa. ggplot(nyc.data,aes(x=kids2000,y=pubast00)) + geom_point(aes(color=manbronx)) + geom_smooth(aes(color=manbronx),method=lm,se=FALSE) + geom_smooth(method=lm,se=FALSE,color=&quot;black&quot;) + scale_color_manual(values=c(&quot;blue&quot;,&quot;red&quot;)) + ggtitle(&quot;Spatial Heterogeneity&quot;) + theme(plot.title = element_text(hjust=0.5)) + labs(color=&quot;Selection&quot;) Chow test In GeoDa, a Chow test on the equality of the regression coefficients between the selected and unselected observations is calculated on the fly and shown at the bottom of the scatter plot. This is not supported by ggplot, but we can run separate regressions for each subset using lm. We can also run the Chow test itself, using the chow.test command from the gap package. First, we create two subsets from the nyc.data by means of filter, based on the value of the manbronx variable. We call the two resulting subsets nyc.select and nyc.rest. We double check their size (there should be 20 selected observations and 35 unselected ones) using the dim command. nyc.select &lt;- nyc.data %&gt;% filter(manbronx == &quot;Select&quot;) nyc.rest &lt;- nyc.data %&gt;% filter(manbronx == &quot;Rest&quot;) dim(nyc.select) ## [1] 20 35 dim(nyc.rest) ## [1] 35 35 Next, we carry out two separate regressions, one for each subset, and list the results. reg.select &lt;- lm(pubast00 ~ kids2000,data=nyc.select) reg.rest &lt;- lm(pubast00 ~ kids2000,data=nyc.rest) summary(reg.select) ## ## Call: ## lm(formula = pubast00 ~ kids2000, data = nyc.select) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.0486 -1.4829 0.3248 2.0625 4.7156 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.74763 1.84198 -2.577 0.019 * ## kids2000 0.47225 0.05071 9.313 2.64e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.406 on 18 degrees of freedom ## Multiple R-squared: 0.8281, Adjusted R-squared: 0.8186 ## F-statistic: 86.73 on 1 and 18 DF, p-value: 2.639e-08 summary(reg.rest) ## ## Call: ## lm(formula = pubast00 ~ kids2000, data = nyc.rest) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.4415 -2.8231 -0.3905 1.9686 8.2359 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.01398 3.33123 -2.106 0.042939 * ## kids2000 0.37260 0.08648 4.308 0.000139 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.957 on 33 degrees of freedom ## Multiple R-squared: 0.36, Adjusted R-squared: 0.3406 ## F-statistic: 18.56 on 1 and 33 DF, p-value: 0.0001391 These values match what we would have obtained in GeoDa with the same observations selected. Finally, we implement the Chow test as chow.test. We pass the y and x variables for each subset, in turn. Again, the results are the same as what one would have obtained in GeoDa and suggest a significant difference between the slopes of the two regression lines. chow &lt;- chow.test(nyc.select$pubast00,nyc.select$kids2000, nyc.rest$pubast00,nyc.rest$kids2000) chow ## F value d.f.1 d.f.2 P value ## 1.534013e+01 2.000000e+00 5.100000e+01 6.082099e-06 Use setwd(directorypath) to specify the working directory.↩︎ Use install.packages(packagename).↩︎ Note that, strictly speaking, the package is ggplot2, i.e., the second iteration of the ggplot package, but the commands use ggplot. From now on, we will use ggplot to refer to both.↩︎ In order to obtain the frequency on the vertical axis, the y variable needs to be set to ..density.., as in aes(y = ..density..).↩︎ For a fuller technical description, see the GeoDa workbook.↩︎ This can be checked in GeoDa by selecting the corresponding points and checking their value in the Table.↩︎ A formula in R is the generic way to specify a functional relationship. The dependent variable is on the left hand side of the ~ sign, with an expression for the explanatory variables on the right hand side. The latter are typically separated by +, but in our example, we only have a bivariate relationship. Also, a constant term is included by default.↩︎ See the GeoDa workbook for further discussion↩︎ "],["exploratory-data-analysis-2.html", "Chapter 3 Exploratory Data Analysis 2 Introduction Preliminaries Scatter Plot Matrix Three Variables: Bubble Chart and 3D Scatter Plot True Multivariate EDA: Parallel Coordinate Plot and Conditional Plots", " Chapter 3 Exploratory Data Analysis 2 Introduction This notebook cover the functionality of the Exploratory Data Analysis 2 section of the GeoDa workbook. We refer to that document for details on the methodology, references, etc. The goal of these notes is to approximate as closely as possible the operations carried out using GeoDa by means of a range of R packages. The notes are written with R beginners in mind, more seasoned R users can probably skip most of the comments on data structures and other R particulars. Also, as always in R, there are typically several ways to achieve a specific objective, so what is shown here is just one way that works, but there often are others (that may even be more elegant, work faster, or scale better). For this notebook, we continue to use the socioeconomic data about 55 sub-boroughs in NYC from the GeoDa website. Our goal in this lab is show how to implement exploratory data analysis methods with three or more variables. Objectives After completing the notebook, you should know how to carry out the following tasks: Creating a scatterplot matrix Adding different types of smoothers to a scatter plot matrix Creating a bubble plot Creating a 3d scatter plot Creating a parallel coordinate plot Constructing conditional plots Making graphs interactive R Packages used tidyverse: for general data wrangling (includes readr and dplyr) ggplot2: to draw statistical plots, including conditional plots. We use this rather than base R for increased functionality and more aesthetically pleasing plots (included in tidyverse) GGally: a ggplot add-on package to create a scatterplot matrix and parallel coordinate plot scatterplot3d: to create a static 3d scattter plot plotly: to construct interactive 3d scatter and parallel coordinate plots R Commands used Below follows a list of the commands used in this notebook. For further details and a comprehensive list of options, please consult the R documentation. Base R: setwd, install.packages, library, head, names tidyverse: read_csv, rename, select GGally: ggscatmat, ggpairs, ggparcoord ggplot2: ggplot, geom_point, xlab, ylab, ggtitle, theme, cut_number, facet_grid, geom_smooth, geom_histogram scatterplot3d: scatterplot3d plotly: plot_ly, add_markers, layout Preliminaries Before starting, make sure to have the latest version of R and of packages that are compiled for the matching version of R (this document was created using R 3.5.1 of 2018-07-02). Also, make sure to set a working directory, such that the data set is in the right path.12 Load packages First, we load all the required packages using the library command. If you don’t have some of these in your system, make sure to install them first as well as their dependencies.13 You will get an error message if something is missing. If needed, just install the missing piece and everything will work after that. Note that ggplot2 does not need to be loaded separately since it is included in the tidyverse package collection. library(tidyverse) library(GGally) library(scatterplot3d) library(plotly) library(geodaData) Obtaining the data The data to implement the operations in this workbook are contained in NYC Data on the GeoDa support web site. After the file is downloaded, it must be unzipped (e.g., double click on the file). The nyc folder should be moved to the current working directory for the path names we use below to work correctly. Creating an initial data frame We use the tidyverse function read_csv to read the data into a data frame nyc.data. We could also have used the base R read.csv, but read_csv is a bit more robust and creates a tibble, a data frame with some additional information. As usual, we check the contents of the data frame with a head command. nyc.data &lt;- nyc head(nyc.data) ## # A tibble: 6 × 34 ## bor_subb NAME CODE SUBBOROUGH FORHIS06 FORHIS07 FORHIS08 FORHIS09 FORWH06 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 501 North S… 501 North Sho… 37.1 34.0 27.4 29.3 13.3 ## 2 502 Mid-Isl… 502 Mid-Island 28.0 18.1 24.0 31.2 20.1 ## 3 503 South S… 503 South Sho… 10.7 12.1 9.69 14.7 10.3 ## 4 401 Astoria 401 Astoria 52.1 54.0 54.7 47.8 38.4 ## 5 402 Sunnysi… 402 Sunnyside… 62.7 69.4 67.1 58.3 37.1 ## 6 403 Jackson… 403 Jackson H… 68.5 68.5 66.5 69.2 34.4 ## # ℹ 25 more variables: FORWH07 &lt;dbl&gt;, FORWH08 &lt;dbl&gt;, FORWH09 &lt;dbl&gt;, ## # HHSIZ1990 &lt;dbl&gt;, HHSIZ00 &lt;dbl&gt;, HHSIZ02 &lt;dbl&gt;, HHSIZ05 &lt;dbl&gt;, ## # HHSIZ08 &lt;dbl&gt;, KIDS2000 &lt;dbl&gt;, KIDS2005 &lt;dbl&gt;, KIDS2006 &lt;dbl&gt;, ## # KIDS2007 &lt;dbl&gt;, KIDS2008 &lt;dbl&gt;, KIDS2009 &lt;dbl&gt;, RENT2002 &lt;dbl&gt;, ## # RENT2005 &lt;dbl&gt;, RENT2008 &lt;dbl&gt;, RENTPCT02 &lt;dbl&gt;, RENTPCT05 &lt;dbl&gt;, ## # RENTPCT08 &lt;dbl&gt;, PUBAST90 &lt;dbl&gt;, PUBAST00 &lt;dbl&gt;, YRHOM02 &lt;dbl&gt;, ## # YRHOM05 &lt;dbl&gt;, YRHOM08 &lt;dbl&gt; Making the variable names compatible As in the previous exercise, we need to make the variable names compatible with their lower case counterparts in the GeoDa Workbook. Again, we will use the tidyverse rename function to turn the all-caps variables into lower case for the examples we will use. As in the GeoDa workbook, we will use the average people per household in 2000 (hhsiz00), the percentage households with children under 18 in 2000 (kids2000), the average number of years lived in the current residence in 2002 (yrhom02), the percentage households receiving public assistance in 2000 (pubast00), and the median rent in 2002 (rent2002). nyc.data &lt;- nyc.data %&gt;% rename(&quot;hhsiz00&quot; = &quot;HHSIZ00&quot;,&quot;kids2000&quot; = &quot;KIDS2000&quot;, &quot;yrhom02&quot;=&quot;YRHOM02&quot;,&quot;pubast00&quot; = &quot;PUBAST00&quot;, &quot;rent2002&quot;=&quot;RENT2002&quot;) names(nyc.data) ## [1] &quot;bor_subb&quot; &quot;NAME&quot; &quot;CODE&quot; &quot;SUBBOROUGH&quot; &quot;FORHIS06&quot; ## [6] &quot;FORHIS07&quot; &quot;FORHIS08&quot; &quot;FORHIS09&quot; &quot;FORWH06&quot; &quot;FORWH07&quot; ## [11] &quot;FORWH08&quot; &quot;FORWH09&quot; &quot;HHSIZ1990&quot; &quot;hhsiz00&quot; &quot;HHSIZ02&quot; ## [16] &quot;HHSIZ05&quot; &quot;HHSIZ08&quot; &quot;kids2000&quot; &quot;KIDS2005&quot; &quot;KIDS2006&quot; ## [21] &quot;KIDS2007&quot; &quot;KIDS2008&quot; &quot;KIDS2009&quot; &quot;rent2002&quot; &quot;RENT2005&quot; ## [26] &quot;RENT2008&quot; &quot;RENTPCT02&quot; &quot;RENTPCT05&quot; &quot;RENTPCT08&quot; &quot;PUBAST90&quot; ## [31] &quot;pubast00&quot; &quot;yrhom02&quot; &quot;YRHOM05&quot; &quot;YRHOM08&quot; Scatter Plot Matrix A scatter plot matrix visualizes the bivariate relationships among several pairs of variables. The individual scatter plots are stacked such that each variable is in turn on the x-axis and on the y-axis. Basic scatter plot matrix A scatter plot matrix is not included in the functionality of ggplot2, but it can be created in a number of ways using the GGally package, which extends ggplot2 with many additional features. Extensive documentation of GGally functionality is available on its Github page. A quick and dirty scatter plot matrix is created by means of the ggscatmat command (detailed documentation is available on the GGally Github page). The ggscatmat function provides pairwise scatter plots in a lower diagonal of the graph, a density graph in the diagonal, and the pairwise correlations in the upper diagonal. This contrasts with GeoDa, where all pairwise scatter plots are given, and the diagonal is populated with a histogram for the individual variable (GeoDa currently does not support density plots). The command is very simple: it takes the data set, the list of variables passed to columns and a few options (color, choice of correlation coefficient). However, it does not seem to contain a way to show a linear or nonlinear smoother. In order to accomplish this, we will need the more powerful ggpairs function (see below). We follow the example in the GeoDa Workbook and use the four variables hhsiz00, kids2000, yrhom02, and pubast00. ggscatmat(nyc.data, columns= c(&quot;hhsiz00&quot;,&quot;kids2000&quot;, &quot;yrhom02&quot;, &quot;pubast00&quot;)) Scatter plot matrix with smoothing An alternative approach that provides much finer control of the graph can be based on the ggpairs function of GGally (see the GGobi Github page for extensive documentation). As in ggscatmat, this function takes the data set as an argument, followed by the variables specified in the columns argument. The lower and upper triangle part of the matrix, and the diagonal are specified by means of the arguments lower, upper, and diag. The values for these arguments must be passed as a list. Default scatter plot matrix First, we illustrate the default setting. We do not need to include the specifics, but they amount to: lower=list(continuous=\"points\"), for a scatter plot in the lower triangle diag=list(continuous=\"densityDiag\"), for a density plot on the diagonal upper=list(coninuous='cor'), for a correlation coefficient in the upper diagonal ggpairs(nyc.data, columns=c(&quot;hhsiz00&quot;,&quot;kids2000&quot;, &quot;yrhom02&quot;, &quot;pubast00&quot;)) Pairwise scatter plots In order to obtain a scatter plot in both lower and upper triangles, we set upper = list(continuous=\"points\") in the arguments to ggpairs. In addition, to have histograms in the diagonal, set set diag=list(continuous=\"barDiag\"). ggpairs(nyc.data, columns=c(&quot;hhsiz00&quot;,&quot;kids2000&quot;, &quot;yrhom02&quot;, &quot;pubast00&quot;), upper=list(continuous=&quot;points&quot;),diag=list(continuous=&quot;barDiag&quot;)) Scatter plot matrix with linear smoother At this point, we can add a linear smoother by specifying list(continuous=\"smooth\") instead of continuous=\"points\" for both upper and lower parameters (back with the default density plot on the diagonal). ggpairs(nyc.data, columns=c(&quot;hhsiz00&quot;,&quot;kids2000&quot;, &quot;yrhom02&quot;, &quot;pubast00&quot;), upper=list(continuous=&quot;smooth&quot;),lower=list(continuous=&quot;smooth&quot;)) Scatter plot matrix with loess smoother The ggpairs function also supports a nonlinear loess smoother, but not the LOWESS smoother implemented in GeoDa. In order to include the latter, it would be necessary to create a custom function to pass as an argument to the upper and lower settings. This is beyond our current scope (again, see the GGobi Github page for techical details). Similarly, if one wanted finer control over the parameters of the smoothing method (like setting a span), this must be implemented by means of a custom function. The loess smoother is passed in the same way as the linear smoother, as an argument to continuous = \"smooth_loess\". ggpairs(nyc.data, columns=c(&quot;hhsiz00&quot;,&quot;kids2000&quot;, &quot;yrhom02&quot;, &quot;pubast00&quot;), upper=list(continuous=&quot;smooth_loess&quot;),lower=list(continuous=&quot;smooth_loess&quot;)) The ggpairs function has many other customization features to deal with axis labels, titles, etc., which we do not further pursue here. As mentioned above, just about anything can be included as a custom function using the ggplot API (for example, using the wrap functionality documented on the Github pages). Finally, as already alluded to earlier, linking and brushing are not included in the functionality of ggplot. The graphs can be made interactive by means of the plotly package, which we illustrate below for the 3D scatter plot. Three Variables: Bubble Chart and 3D Scatter Plot Bubble chart The bubble chart augments the scatter plot with a third dimension, the size of the point (or, bubble). Optionally, a fourth dimension can be added as the color of the point, but this quickly becomes difficult to discern. In GeoDa, the four dimensions are available by default, with the third and fourth set to the same variable. In ggplot, this is accomplished by setting a third and potentially fourth aesthetic for size and col to a variable. We first illustrate a bubble chart using the variable kids2000 for the x-axis, pubast00 for the y-axis, and rent2002 for the bubble size. These are passed as arguments to aes. This is followed by the geom_point geom. As before, we can add labels for the x and y axis, as well as a title. ggplot(data=nyc.data,aes(x=kids2000,y=pubast00,size=rent2002)) + geom_point() + xlab(&quot;Percent HH with kids&quot;) + ylab(&quot;Percent HH with public assistance&quot;) + ggtitle(&quot;Bubble Chart&quot;) + theme(plot.title = element_text(hjust = 0.5)) Now, we also introduce the col as rent2002. ggplot(data=nyc.data,aes(x=kids2000,y=pubast00,size=rent2002,col=rent2002)) + geom_point() + xlab(&quot;Percent HH with kids&quot;) + ylab(&quot;Percent HH with public assistance&quot;) + ggtitle(&quot;Bubble Chart&quot;) + theme(plot.title = element_text(hjust = 0.5)) Note that we used the default color bar for the color argument. There is a wide scope for customization of legends and color schemes in ggplot, which is beyond the current scope. In a nutshell, in order to fully mimic the graphs in GeoDa, one would need to use one of the ColorBrewer color schemes, which are available as an option in ggplot. 3D Scatter Plot Basic 3D scatter plot The three-dimensional scatter plot is a simple generalization of the two-dimensional case by creating a graph that projects a 3D cube onto the two-dimensional screen (or paper), as a perspective plot. This is not (currently) supported by ggplot, so we resort to the specialized package scatterplot3d (see Ligges and Mächler 2003). This is an older package that predates the layered logic of ggplot, and instead uses the approach taken in the base R plot commands. A rudimentary plot follows from the scatterplot3d function to which the variables for the three dimensions are passed as arguments to x, y, and z. In contrast to ggplot, there is no data argument, but the variables must be specified using the standard $ notation. We again use the variables kids2000, pubast00, and rent2002. In addition, we spiff up the graph a bit by adding a main title, as well as titles for xlab, ylab, and zlab. In addition, we set the symbol to a filled circle (the default is a hollow circle), using the base R pch = 20 argument, and color it red (color = \"red\"), as in the GeoDa Workbook example. The result is as given below. scatterplot3d(x = nyc.data$kids2000, y = nyc.data$pubast00, z = nyc.data$rent2002, main = &quot;NYC 3D Scatterplot&quot;, xlab = &quot;Percent HH with kids&quot;, ylab = &quot;Percent HH with public assistance&quot;, zlab = &quot;Median rent&quot;, pch = 20, color = &quot;red&quot;) Plotly in a nutshell The static 3D scatter plot is fine as a traditional graph, but is not that useful for data exploration. To that effect, we will illustrate some functionality contained in the plotly package. This package forms an R interface to the extensive open source Javascript graphing library of the same name, plotly.js, which leverages the extensive collection of D3 charts. The plotly package is just one of several interfaces to plotly. Another commonly used one is Plotly.py for Python. The functionality in plotly is huge, and much more than we can cover here. We refer to the many web resources for further details. An excellent overview is given in Carson Sievert’s plotly for R book, especially in Chapter 2, The Plotly Cookbook. There are two main ways to create interactive graphs using plotly. One is to pass the usual arguments to the plot_ly command, which has its own Grammar of Graphics syntax. This uses the concept of traces, which is similar to the layers in ggplot. The second way uses the ggplotly command, which takes one or more ggplot objects and makes them interactive. We will not cover the second approach, but it is a fairly straightforward way to make any of our earlier ggplot graphs interactive. The basic arguments to plot_ly are the same as for ggplot, i.e., the data set, and the axes (x, y, and for 3D, z). The variable names are passed in a slightly different way, and use the formula notation, with the variable name prefaced by the ~ symbol. So, for example, if the x-axis would map to the variable kids2000, that would be specified as x = ~kids2000, and similarly for the other axes. Just like ggplot, plotly has a layered approach to constructing a graph, but instead of using a plus sign to separate the layers, a pipe command, %&gt;% is used. Also, the various options for customization are passed to the respective arguments as a list. We will illustrate the basics of plot_ly by constructing a 3D scatter plot for the same three variables as above. Interacting with the 3D scatter plot There are several ways to interact with a plot in plotly, but here we will illustrate some basic functionality to zoom in, zoom out, and rotate the 3D cube, similar to what is available in GeoDa. We start with a bare bones graph. We pass the data set and the three variables to the plot_ly function. Since the default trace is a scatter plot, this is all we really need to specify. plot_ly(nyc.data, x = ~kids2000, y = ~pubast00, z = ~rent2002) There are warnings, but it works. Basically, since we did not specify a trace type, the default scatter3d is applied, which is exactly what we wanted. Also, since no mode is specified, the default is set to markers. The moment we move the pointer over the graph, a number of small icons appear in the right-hand top. These correspond to different types of interactions that can be carried out. Before we proceed with those, however, we move the pointer to one of the points. The hovering functionality (here left to the default of listing all data dimensions) will list the values for x, y and z in a small box, and draw the projections to each of the axes. The second left-most icon at the top of the graph invokes the Zoom functionality. Now, moving the pointer back and forth makes the cube smaller or larger. Other interesting options are the Orbital rotation and Turntable rotation, the two icons to the right of the home symbol. Both options move the cube around as the pointer changes position. The left-most icon allows a static version of the plot to be downloaded as a png file. Next, we illustrate a very simple way to add some further information to the graph. First, we use add_markers to turn the observation points red. Note how the argument marker is set equal to a list to pass the needed color parameter. The add_markers command follows the initial plot_ly setting after a %&gt;% pipe symbol. A final touch is to set titles for the axes, by means of the layout command and the scene option, again after a pipe symbol. The titles are set by means of a list command for each of the three xaxis, yaxis, and zaxis. At this point, when we execute the command, there are no more warnings. Also, we can interact with the graph in the same way as before. plot_ly(nyc.data, x = ~kids2000, y = ~pubast00, z = ~rent2002) %&gt;% add_markers(marker = list(color=&quot;red&quot;)) %&gt;% layout(scene = list(xaxis = list(title = &quot;Percent HH with kids&quot;), yaxis = list(title = &quot;Percent HH with public assistance&quot;), zaxis = list(title = &quot;Median rent&quot;))) True Multivariate EDA: Parallel Coordinate Plot and Conditional Plots True multivariate EDA deals with situations where more than three variables are considered. We follow the GeoDa Workbook and illustrate the Parallel Coordinate Plot, or PCP, and conditional plots. For the former, we again need to resort to GGally, but for the latter, we can exploit the facet_wrap and facet_grid functions of ggplot. In addition, we can turn these plots into interactive graphs by means of the plotly functionality. Parallel Coordinate Plot (PCP) PCP in GGally The PCP is implemented in the ggparcoord function of GGally. However, its implementation does not follow the regular columns specification we used above for the scatterplot matrix. Instead of passing a list of variable names, the actual column numbers of the variables in the data frame must be specified (but those may be in any order). An easy, though not very elegant way to deal with this is to create a subset of the data for those variables to be plotted, and then exploit the default of columns = 1:ncol(data). In other words, we don’t have to specify the columns argument at all. In the example below, we use the same four variables as in the GeoDa Workbook: kids2000, rent2002, pubast00, and yrhom02. We first select those from the nyc.data set to create a subset we call pcp.vars. Then, we pass this subset as the argument to data in ggparcoord. The result is a fairly rudimentary PCP, with the axes organized vertically (in GeoDa, they are horizontal). Many customizations are possible, for which detailed options can be found in the documentation pages. vars &lt;- c(&quot;kids2000&quot;,&quot;rent2002&quot;,&quot;pubast00&quot;,&quot;yrhom02&quot;) pcp.vars &lt;- select(nyc.data,vars) ggparcoord(data = pcp.vars) PCP in plotly In plotly, the PCP functionality in implemented as the type = \"parcoords\". This is passed as the second argument to the plot_ly function (the first argument, as usual, is the data set, nyc.data). The axes of the PCP are specified through the dimensions argument. As is the case in other plot_ly examples, they are passed as a list. In this instance, this is actually a list of lists, one for each axis. In each of these lists, we include a label for the axis, and a variable as the argument to the values parameter. As before, we need to use a formula format for the variables and precede their names with the ~ symbol. plot_ly(nyc.data,type = &quot;parcoords&quot;, dimensions = list( list(label = &quot;Kids&quot;, values = ~kids2000), list(label = &quot;Public Assistance&quot;, values = ~pubast00), list(label = &quot;Rent&quot;, values = ~rent2002), list(label = &quot;Stable&quot;, values = ~yrhom02) ) ) Once we move the pointer over the graph, a few icons appear on the top right (but fewer than for the 3D scatter plot). The interaction with the graph is not that intuitive, but once you know what to look for, it is quite powerful. The easiest way to proceed is to click on one of the axes: with the cross hair + symbol placed at any location along an axis, clicking will change the color and select the observations (lines) covered by the small vertical bar (the pointer will turn into an arrow that points up or down, depending on the direction of the selection). Clicking the cross hair in another position on the same axis turns the selection off. A second interactive feature allows one to change the order of the axes. For example, if we move the pointer to the top of the Stable axis, it changes from a cross hair to a double sided arrow &lt;-&gt;. Pressing down on the pointer now lets us move this axis to the left, e.g., to become the third axis. This can be done even while certain observations are selected. Several options for customization of the PCP graph can be found in the plotly documentation for parcoords. Conditional Plots Conditional plots are a major feature of the functionality of ggplot, where they are referred to as facetting, or small multiples. This is implemented in the facet_wrap and facet_grid functions. The main difference between the two approaches is that facet_grid is explicitly two-dimensional. In that aspect, it is the closest matches to the conditional plot design in GeoDa. There is one major difference between the approach taken in GeoDa and that in ggplot. In GeoDa, the conditioning variables are typically continuous, and different types of classifications can be applied to them to obtain the actual condition. For example, in the GeoDa Workbook illustration, the variables hhsiz00 and yrhom02 are used as conditioning variables for respectively, the x-axis and the y-axis. A classification such as quantiles (e.g., 3 or 2 in the GeoDa Workbook examples) yields the categories for the sub-plots. In ggplot, the conditioning is based on a categorical variable that needs to be available in the data set. The facetting formula does not evaluate functions, so the conditioning categories need to be computed beforehand. There are three so-called helper functions to make this easy: cut_interval, cut_width, and cut_number. The closest to the median (2 quantiles) conditioning illustrated in the GeoDa Workbook is the cut_number function. We pass the variable, e.g., hhziz00, and the number of categories, say n = 2. This creates the new variable as an R factor, giving the intervals that resulted from the cut. For example, we create a new variable cut.hhsiz using a quantile classification with two categories (as in the GeoDa Workbook, the variable will be split on the median value), by setting n=2. We need to use the $ notation to ensure that the new variable is added to the relevant data set. Since we only have 55 observations, we can easily list the full set of values to verify. Internally, they are stored as factors (hence, the summary of the Levels at the end of the listing). nyc.data$cut.hhsiz &lt;- cut_number(nyc.data$hhsiz00,n=2) nyc.data$cut.hhsiz ## [1] (2.72,3.2] [1.57,2.72] (2.72,3.2] [1.57,2.72] [1.57,2.72] (2.72,3.2] ## [7] (2.72,3.2] [1.57,2.72] [1.57,2.72] [1.57,2.72] [1.57,2.72] (2.72,3.2] ## [13] (2.72,3.2] [1.57,2.72] (2.72,3.2] (2.72,3.2] [1.57,2.72] [1.57,2.72] ## [19] [1.57,2.72] [1.57,2.72] [1.57,2.72] [1.57,2.72] [1.57,2.72] [1.57,2.72] ## [25] [1.57,2.72] [1.57,2.72] (2.72,3.2] (2.72,3.2] (2.72,3.2] (2.72,3.2] ## [31] (2.72,3.2] (2.72,3.2] [1.57,2.72] (2.72,3.2] [1.57,2.72] [1.57,2.72] ## [37] (2.72,3.2] (2.72,3.2] (2.72,3.2] (2.72,3.2] [1.57,2.72] [1.57,2.72] ## [43] [1.57,2.72] (2.72,3.2] (2.72,3.2] (2.72,3.2] (2.72,3.2] (2.72,3.2] ## [49] (2.72,3.2] [1.57,2.72] (2.72,3.2] (2.72,3.2] [1.57,2.72] [1.57,2.72] ## [55] [1.57,2.72] ## Levels: [1.57,2.72] (2.72,3.2] And, similarly for cut.yrhom: nyc.data$cut.yrhom &lt;- cut_number(nyc.data$yrhom02,n=2) nyc.data$cut.yrhom ## [1] [8.22,12.4] (12.4,16.1] (12.4,16.1] (12.4,16.1] (12.4,16.1] (12.4,16.1] ## [7] [8.22,12.4] (12.4,16.1] (12.4,16.1] (12.4,16.1] (12.4,16.1] [8.22,12.4] ## [13] (12.4,16.1] (12.4,16.1] (12.4,16.1] (12.4,16.1] (12.4,16.1] [8.22,12.4] ## [19] (12.4,16.1] [8.22,12.4] [8.22,12.4] [8.22,12.4] [8.22,12.4] (12.4,16.1] ## [25] (12.4,16.1] (12.4,16.1] (12.4,16.1] [8.22,12.4] [8.22,12.4] [8.22,12.4] ## [31] [8.22,12.4] [8.22,12.4] (12.4,16.1] [8.22,12.4] (12.4,16.1] [8.22,12.4] ## [37] (12.4,16.1] (12.4,16.1] [8.22,12.4] [8.22,12.4] [8.22,12.4] [8.22,12.4] ## [43] [8.22,12.4] [8.22,12.4] [8.22,12.4] [8.22,12.4] [8.22,12.4] [8.22,12.4] ## [49] [8.22,12.4] (12.4,16.1] (12.4,16.1] [8.22,12.4] (12.4,16.1] (12.4,16.1] ## [55] [8.22,12.4] ## Levels: [8.22,12.4] (12.4,16.1] If we compare the breakpoints to the ones in Figure 34 of the Workbook, we see that they are close, but not exactly the same, i.e., 2.72 vs. 2.703 in GeoDa, and 12.4 vs. 12.368. More precisely, upon closer examination, we find that for cut.hhsiz, the lower group has 28 observations vs. 27 in GeoDa. Since we have so few data points, this may lead to slight differences in the graphs. At this point, we can set up the conditioning in the facet_grid function, expressed as a formula, with the row conditioning variable first. Note that the row conditioning variable is the y-axis in GeoDa, and the column conditioning variable is the x-axis. For example, with our new categories cut.hhsiz and cut.yrhom, this would be facet_grid(cut.yrhom ~ cut.hhsiz). One final aspect is how the categories are ordered in the graph. The default (as.table=TRUE) is to have the highest category in the lower-right corner. In order to mimic the organization in GeoDa, we set as.table=FALSE. This results in the highest category being in the upper-right corner. We now illustrate this for a conditional scatter plot and a conditional histogram. Conditional scatter plot We replicate the example in the GeoDa Workbook and condition a scatter plot with kids2000 on the x-axis and pubast00 on the y-axis. We set these two variables as x and y in the aes argument of ggplot. Next, we specify the geom as geom_point, for the default scatter plot. Finally, we add the facet_grid command. ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + geom_point() + facet_grid(cut.yrhom ~ cut.hhsiz,as.table=FALSE) We can add a linear smoother by means of geom_smooth(method=\"lm\"): ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + geom_point() + geom_smooth(method=&quot;lm&quot;) + facet_grid(cut.yrhom ~ cut.hhsiz,as.table=FALSE) We can also add a loess smoother by means of geom_smooth(method=\"loess\"): ggplot(data=nyc.data,aes(x=kids2000,y=pubast00)) + geom_point() + geom_smooth(method=&quot;loess&quot;) + facet_grid(cut.yrhom ~ cut.hhsiz,as.table=FALSE) As is the case for all graphs in ggplot, many further customizations can be added, but we do not consider that further. Conditional histogram We conclude with a conditional histogram for the variable pubast00. The principle is the same as before. The only difference is that now only one variable needs to be specified in aes, and the geom_histogram is used. As we did earlier, we set the bins=7 (the default of 30 is not appropriate in this example). The resulting graph differs slightly from the example in the GeoDa Workbook due to different bin widths. With some customization, they can be made to look exactly the same, but we won’t pursue that here. ggplot(data=nyc.data,aes(pubast00)) + geom_histogram(bins=7) + facet_grid(cut.yrhom ~ cut.hhsiz,as.table=FALSE) Use setwd(directorypath) to specify the working directory.↩︎ Use install.packages(packagename).↩︎ "],["basic-mapping.html", "Chapter 4 Basic Mapping Introduction Preliminaries Basic Choropleth Mapping Common Map Classifications Extreme Value Maps Mapping Categorical Variables Conditional Map Cartogram", " Chapter 4 Basic Mapping Introduction This notebook covers the functionality of the Basic Mapping section of the GeoDa workbook. We refer to that document for details on the methodology, references, etc. The goal of these notes is to approximate as closely as possible the operations carried out using GeoDa by means of a range of R packages. The notes are written with R beginners in mind, more seasoned R users can probably skip most of the comments on data structures and other R particulars. Also, as always in R, there are typically several ways to achieve a specific objective, so what is shown here is just one way that works, but there often are others (that may even be more elegant, work faster, or scale better). For this notebook, we will continue to use the socioeconomic data for 55 New York City sub-boroughs from the GeoDa website. Objectives After completing the notebook, you should know how to carry out the following tasks (some of these were also covered in the spatial data handling notes): Reading and loading a shapefile Creating choropleth maps for different classifications Customizing choropleth maps Selecting appropriate color schemes Calculating and plotting polygon centroids Composing conditional maps Creating a cartogram R Packages used sf: to manipulate simple features tmap: to create and customize choropleth maps RColorBrewer: to select color schemes cartogram: to construct a cartogram R Commands used Below follows a list of the commands used in this notebook. For further details and a comprehensive list of options, please consult the R documentation. Base R: setwd, install.packages, library, summary, quantile, function, unname, vector, floor, ceiling, cut, as.factor, as.character, as.numeric, which, length, rev, unique sf: st_read, st_crs, plot, st_centroid, st_write, st_set_geometry(NULL) tmap: tm_shape, tm_polygons, tm_fill, tm_borders, tm_layout, tmap_mode, tm_basemap, tmap_save, tm_dots, tm_facets RColorBrewer: brewer.pal, display.brewer.pal cartogram: cartogram_dorling, cartogram_cont, cartogram_ncont Preliminaries Before starting, make sure to have the latest version of R and of packages that are compiled for the matching version of R (this document was created using R 3.5.1 of 2018-07-02). Also, make sure to set a working directory.14 We will use a relative path to the working directory to read the data set. Load packages First, we load all the required packages using the library command. If you don’t have some of these in your system, make sure to install them first as well as their dependencies.15 You will get an error message if something is missing. If needed, just install the missing piece and everything will work after that. library(sf) library(tmap) library(RColorBrewer) library(cartogram) library(geodaData) Obtaining the data The data set used to implement the operations in this workbook is the same as the one we used for exploratory data analysis. The variables are contained in NYC Data on the GeoDa support web site. After the file is downloaded, it must be unzipped (e.g., double click on the file). The nyc folder should be moved to the current working directory for the path names we use below to work correctly. We use the st_read command from the sf package to read in the shape file information. This provides a brief summary of the geometry of the layer, such as the path (your path will differ), the driver (ESRI Shapefile), the geometry type (MULTIPOLYGON), bounding box and projection information. The latter is contained in the proj4string information. nyc.bound &lt;- nyc_sf The projection in question is the ESRI projection 102718, NAD 1983 stateplane New York Long Island fips 3104 feet. It does not have an EPSG code (see the spatial data handling notes for further details on projections), but it has a valid proj4string. So, as long as we don’t change anything, we should be OK. We can double check the projection information with st_crs: st_crs(nyc.bound) ## Coordinate Reference System: ## User input: EPSG:2263 ## wkt: ## PROJCRS[&quot;NAD83 / New York Long Island (ftUS)&quot;, ## BASEGEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4269]], ## CONVERSION[&quot;SPCS83 New York Long Island zone (US Survey feet)&quot;, ## METHOD[&quot;Lambert Conic Conformal (2SP)&quot;, ## ID[&quot;EPSG&quot;,9802]], ## PARAMETER[&quot;Latitude of false origin&quot;,40.1666666666667, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8821]], ## PARAMETER[&quot;Longitude of false origin&quot;,-74, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8822]], ## PARAMETER[&quot;Latitude of 1st standard parallel&quot;,41.0333333333333, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8823]], ## PARAMETER[&quot;Latitude of 2nd standard parallel&quot;,40.6666666666667, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8824]], ## PARAMETER[&quot;Easting at false origin&quot;,984250, ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219], ## ID[&quot;EPSG&quot;,8826]], ## PARAMETER[&quot;Northing at false origin&quot;,0, ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219], ## ID[&quot;EPSG&quot;,8827]]], ## CS[Cartesian,2], ## AXIS[&quot;easting (X)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219]], ## AXIS[&quot;northing (Y)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219]], ## USAGE[ ## SCOPE[&quot;Engineering survey, topographic mapping.&quot;], ## AREA[&quot;United States (USA) - New York - counties of Bronx; Kings; Nassau; New York; Queens; Richmond; Suffolk.&quot;], ## BBOX[40.47,-74.26,41.3,-71.8]], ## ID[&quot;EPSG&quot;,2263]] As we saw in the spatial data handling notes, we can create a quick map using the sf plot command. This will result in a choropleth map for the first few variables in the data frame. plot(nyc.bound) Basic Choropleth Mapping We now turn to the choropleth mapping functionality included in the tmap package. We have already covered some functionality of tmap in the spatial data handling notes. Here, we will explore some further customizations. The tmap package is extremely powerful, and there are many features that we do not cover. Detailed information on tmap functionality can be found at tmap documentation. In addition, several extensive examples are listed in the review in the Journal of Statistical Software by Tennekes (2018). Another useful resource is the chapter on mapping in Lovelace, Nowosad, and Muenchow’s Geocomputation with R book. Default settings We have already seen (in the spatial data handling notes) that tmap uses the same layered logic as ggplot. The initial command is tm_shape, which specifies the geography to which the mapping is applied. This is followed by a number of tm_* options that select the type of map and several optional customizations. We follow the example in the GeoDa Workbook and start with a basic choropleth map of the rent2008 variable. In tmap there are two ways to create a choropleth map. The simplest one, which we have already seen, is to use the tm_polygons command with the variable name as the argument (under the hood, this is the value for the col parameter). In our example, the map is created with two commands, as shown below. The color coding corresponds to style = \"pretty\", which is the default when the classification breaks are not set explicitly, and the default values are used (see the discussion of map classifications below). tm_shape(nyc.bound) + tm_polygons(&quot;rent2008&quot;) The tm_polygons command is a wrapper around two other functions, tm_fill and tm_borders. tm_fill controls the contents of the polygons (color, classification, etc.), while tm_borders does the same for the polygon outlines. For example, using the same shape (but no variable), whe obtain the outlines of the sub-boroughs from the tm_borders command. tm_shape(nyc.bound) + tm_borders() Similarly, we obtain a choropleth map without the polygon outlines when we just use the tm_fill command. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;) When we combine the two commands, we obtain the same map as with tm_polygons (this illustrates how in R one can often obtain the same result in a number of different ways). tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;) + tm_borders() An extensive set of options is available to customize the appearance of the map. A full list is given in the documentation page for tm_fill. In what follows, we briefly consider the most common ones. Color palette The range of colors used to depict the spatial distribution of a variable is determined by the palette. The palette is an argument to the tm_fill function. Several built-in palettes are contained in tmap. For example, using palette = \"Reds\" would yield the following map for our example. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,palette=&quot;Reds&quot;) + tm_borders() Under the hood, “Reds” refers to one of the color schemes supported by the RColorBrewer package (see below). 4.0.0.1 Custom color palettes In addition to the built-in palettes, customized color ranges can be created by specifying a vector with the desired colors as anchors. This will create a spectrum of colors in the map that range between the colors specified in the vector. For instance, if we used c(“red”, “blue”), the color spectrum would move from red to purple, then to blue, with in between shades. In our example: tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,palette=c(&quot;red&quot;,&quot;blue&quot;)) + tm_borders() Not exactly a pretty picture. In order to mimic the diverging scale used in many of GeoDa’s extreme value maps, we insert “white” in between red and blue (but see the next section for a better approach). tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,palette=c(&quot;red&quot;,&quot;white&quot;,&quot;blue&quot;)) + tm_borders() Better, but still not quite there. ColorBrewer A preferred approach to select a color palette is to chose one of the schemes contained in the RColorBrewer package. These are based on the research of cartographer Cynthia Brewer (see the colorbrewer2 web site for details). ColorBrewer makes a distinction between sequential scales (for a scale that goes from low to high), diverging scales (to highlight how values differ from a central tendency), and qualitative scales (for categorical variables). For each scale, a series of single hue and multi-hue scales are suggested. In the RColorBrewer package, these are referred to by a name (e.g., the “Reds” palette we used above is an example). The full list is contained in the RColorBrewer documentation. There are two very useful commands in this package. One sets a color palette by specifying its name and the number of desired categories. The result is a character vector with the hex codes of the corresponding colors. For example, we select a sequential color scheme going from blue to green, as BuGn, by means of the command brewer.pal, with the number of categories (6) and the scheme as arguments. The resulting vector contains the HEX codes for the colors. pal &lt;- brewer.pal(6,&quot;BuGn&quot;) pal ## [1] &quot;#EDF8FB&quot; &quot;#CCECE6&quot; &quot;#99D8C9&quot; &quot;#66C2A4&quot; &quot;#2CA25F&quot; &quot;#006D2C&quot; Using this palette in our map yields the following result. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,palette=&quot;BuGn&quot;) + tm_borders() The command display.brewer.pal allows us to explore different color schemes before applying them to a map. For example: display.brewer.pal(6,&quot;BuGn&quot;) Legend There are many options to change the formatting of the legend entries through the legend.format argument. We refer to the tm_fill documentation for specific details. Often, the automatic title for the legend is not that attractive, since it is simply the variable name. This can be customized by setting the title argument to tm_fill. For example, keeping all the other settings to the default, we change the legend to Rent in 2008. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;) + tm_borders() Another important aspect of the legend is its positioning. This is handled through the tm_layout function. This function has a vast number of options, as detailed in the documentation. There are also specialized subsets of layout functions, focused on specific aspects of the map, such as tm_legend, tm_style and tm_format. We illustrate the positioning of the legend. The default is to position the legend inside the map. Often, this default solution is appropriate, but sometimes further control is needed. The legend.position argument to the tm_layout function takes a vector of two string variables that determine both the horizontal position (“left”, “right”, or “center”) and the vertical position (“top”, “bottom”, or “center”). For example, if we would want to move the legend to the lower-right position (clearly inferior to the default solution), we would use the following set of commands. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;) + tm_borders() + tm_layout(legend.position = c(&quot;right&quot;, &quot;bottom&quot;)) There is also the option to position the legend outside the frame of the map. This is accomplished by setting legend.outside to TRUE (the default is FALSE), and optionally also specify its position by means of legend.outside.position. The latter can take the values “top”, “bottom”, “right”, and “left”. For example, to position the legend outside and on the right, would be accomplished by the following commands. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;) + tm_borders() + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) We can also customize the size of the legend, its alignment, font, etc. We refer to the documentation for specifics. While there is no obvious way to show the number of observations in each category (as is the case in GeoDa), tmap has an option to add a histogram to the legend. This is accomplished by setting legend.hist=TRUE in the tm_fill command. Further customization is possible, but is not covered here. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;,legend.hist=TRUE) + tm_borders() + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) Title Another functionality of the tm_layout function is to set a title for the map, and specify its position, size, etc. For example, we can set the title, the title.size and the title.position as in the example below (for details, see the tm_layout documentation). We made the font size a bit smaller (0.8) in order not to overwhelm the map, and positioned it in the bottom right-hand corner. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;) + tm_borders() + tm_layout(title = &quot;Rent 2008 NYC Sub-Boroughs&quot;, title.size = 0.8, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) To have a title appear on top (or on the bottom) of the map, rather than inside (the default), we need to set the main.title argument of the tm_layout function, with the associated main.title.position, as illustrated below (with title.size set to 1.5 to have a larger font). tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;) + tm_borders() + tm_layout(main.title = &quot;Rent 2008 NYC Sub-Boroughs&quot;, title.size = 1.5,main.title.position=&quot;center&quot;) Border options So far, we have not specified any arguments to the tm_borders function. Common options are the color for the border lines (col), their thickness (lwd) and the type of line (lty). The line type is a base R functionality and can be set by specifying a string (such as “solid”, “dashed”, “dotted”, etc.), or the internal code (e.g., 1 for solid, 2 for dashed, 3 for dotted). To illustrate this, we set the borders to blue, with a line width of 2.0 and use a dotted line. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;) + tm_borders(col=&quot;blue&quot;,lwd=2,lty=3) Interactive base map The default mode in tmap is plot, i.e., the standard plotting environment in R to draw a map on the screen or to a device (e.g., a pdf file). There is also an interactive mode, which builds upon leaflet to add a basemap and interact with the map through zooming and identification of individual observations. The interactive mode is referred to as view. We switch between modes by means of the tmap_mode command. tmap_mode(&quot;view&quot;) There are a number of different ways in which a basemap can be added. The current preferred approach is through the tm_basemap command. This takes two important arguments. One is the name of the server. A number of basemaps are supported (and no doubt, that number will increase over time), but we will illustrate the OpenStreetMap option. A second argument is the degree of transparency, or alpha level. This can also be set for the map itself through tm_fill. In practice, one typically needs to experiment a bit to find the right balance between the information in the choropleth map and the background. In the example below, we set the alpha level for the main map to 0.7, and for the base layer to 0.5. When we move the pointer over the polygons, their ID value is shown. A click on a location also gives the value for the variable that is being mapped. Zooming and panning are supported as well. In addition, it is possible to stack several layers, but we won’t pursue that here. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;,alpha=0.7) + tm_borders() + tm_basemap(server=&quot;OpenStreetMap&quot;,alpha=0.5) Before we proceed, we turn the mode back to plot. tmap_mode(&quot;plot&quot;) Saving a map as a file So far, we have only plotted the map to the screen or an interactive widget. In order to save the map as an output, we first assign the result of a series of tmap commands to a variable (an object). We can then save this object by means of the tmap_save function.16 This function takes the map object as the argument tm and the output file name as the argument filename. The default output is a png file. Other formats are obtained by including the proper file extension. There are many other options in this command, such as specifying the height, width, and dpi. Details can be found in the list of options. For example, we assign our rudimentary default map to the variable mymap and then save that to the file mymap.png. mymap &lt;- tm_shape(nyc.bound) + tm_fill(&quot;kids2000&quot;) + tm_borders() tmap_save(tm = mymap, filename = &quot;mymap.png&quot;) ## Map saved to mymap.png ## Resolution: 2100 by 1500 pixels ## Size: 7 by 5 inches (300 dpi) Shape centers GeoDa has functionality invoked from a map window to map or save shape centers, i.e., mean centers and centroids. The st_centroid function is part of sf (there is no obvious counterpart to the mean center functionality). It creates a point simple features layer and contains all the variables of the original layer. nycentroid &lt;- st_centroid(nyc.bound) summary(nycentroid) ## bor_subb name code subborough ## Min. :101.0 Length:55 Min. :101.0 Length:55 ## 1st Qu.:204.5 Class :character 1st Qu.:204.5 Class :character ## Median :218.0 Mode :character Median :218.0 Mode :character ## Mean :274.4 Mean :274.4 ## 3rd Qu.:403.5 3rd Qu.:403.5 ## Max. :503.0 Max. :503.0 ## forhis06 forhis07 forhis08 forhis09 ## Min. :10.70 Min. :10.37 Min. : 9.689 Min. :14.66 ## 1st Qu.:29.61 1st Qu.:28.92 1st Qu.:28.182 1st Qu.:28.28 ## Median :39.72 Median :39.21 Median :39.567 Median :35.99 ## Mean :39.22 Mean :38.33 Mean :37.764 Mean :37.46 ## 3rd Qu.:44.61 3rd Qu.:45.30 3rd Qu.:45.496 3rd Qu.:45.14 ## Max. :69.52 Max. :69.40 Max. :69.341 Max. :69.16 ## forwh06 forwh07 forwh08 forwh09 ## Min. : 0.00 Min. : 0.00 Min. : 0.00 Min. : 0.00 ## 1st Qu.:14.26 1st Qu.:14.56 1st Qu.:14.30 1st Qu.:13.57 ## Median :21.18 Median :21.10 Median :21.39 Median :19.69 ## Mean :24.36 Mean :23.54 Mean :22.67 Mean :21.98 ## 3rd Qu.:31.53 3rd Qu.:29.78 3rd Qu.:31.38 3rd Qu.:27.98 ## Max. :60.36 Max. :59.64 Max. :59.11 Max. :56.89 ## hhsiz1990 hhsiz00 hhsiz02 hhsiz05 ## Min. :1.560 Min. :1.574 Min. :1.532 Min. :1.544 ## 1st Qu.:2.428 1st Qu.:2.445 1st Qu.:2.234 1st Qu.:2.271 ## Median :2.716 Median :2.718 Median :2.568 Median :2.567 ## Mean :2.635 Mean :2.639 Mean :2.505 Mean :2.471 ## 3rd Qu.:2.937 3rd Qu.:2.929 3rd Qu.:2.812 3rd Qu.:2.682 ## Max. :3.376 Max. :3.202 Max. :3.087 Max. :3.106 ## hhsiz08 kids2000 kids2005 kids2006 ## Min. :1.544 Min. : 8.382 Min. : 8.708 Min. : 8.683 ## 1st Qu.:2.203 1st Qu.:30.301 1st Qu.:27.871 1st Qu.:26.215 ## Median :2.488 Median :38.228 Median :35.763 Median :36.524 ## Mean :2.424 Mean :36.040 Mean :34.353 Mean :33.847 ## 3rd Qu.:2.665 3rd Qu.:42.773 3rd Qu.:42.152 3rd Qu.:41.174 ## Max. :3.222 Max. :55.367 Max. :50.872 Max. :51.911 ## kids2007 kids2008 kids2009 rent2002 ## Min. : 8.067 Min. : 8.004 Min. : 0.00 Min. : 0.0 ## 1st Qu.:27.366 1st Qu.:27.843 1st Qu.:26.69 1st Qu.: 750.0 ## Median :35.414 Median :33.883 Median :33.53 Median : 800.0 ## Mean :33.801 Mean :33.212 Mean :32.07 Mean : 944.4 ## 3rd Qu.:40.290 3rd Qu.:40.610 3rd Qu.:39.68 3rd Qu.:1000.0 ## Max. :51.502 Max. :49.716 Max. :48.13 Max. :2500.0 ## rent2005 rent2008 rentpct02 rentpct05 ## Min. : 0.0 Min. : 0 Min. : 0.00 Min. : 0.00 ## 1st Qu.: 897.5 1st Qu.:1000 1st Qu.:13.27 1st Qu.:13.04 ## Median : 990.0 Median :1100 Median :21.11 Median :22.59 ## Mean :1045.9 Mean :1257 Mean :22.01 Mean :22.08 ## 3rd Qu.:1100.0 3rd Qu.:1362 3rd Qu.:29.49 3rd Qu.:30.51 ## Max. :2500.0 Max. :2900 Max. :43.38 Max. :40.88 ## rentpct08 pubast90 pubast00 yrhom02 ## Min. : 0.00 Min. :23.89 Min. : 0.8981 Min. : 8.216 ## 1st Qu.:15.48 1st Qu.:62.38 1st Qu.: 3.3860 1st Qu.:11.264 ## Median :23.70 Median :75.44 Median : 6.8781 Median :12.391 ## Mean :23.41 Mean :71.53 Mean : 8.4372 Mean :12.251 ## 3rd Qu.:31.50 3rd Qu.:84.59 3rd Qu.:11.5369 3rd Qu.:13.160 ## Max. :47.38 Max. :96.65 Max. :23.4318 Max. :16.124 ## yrhom05 yrhom08 geometry ## Min. : 8.583 Min. : 8.278 POINT :55 ## 1st Qu.:11.268 1st Qu.:10.846 epsg:2263 : 0 ## Median :12.217 Median :12.274 +proj=lcc ...: 0 ## Mean :12.273 Mean :12.058 ## 3rd Qu.:13.288 3rd Qu.:13.001 ## Max. :16.627 Max. :15.425 We can now exploit the layered logic of tm_map to first draw the outlines of the sub-boroughs using tm_borders, followed by tm_dots for the point layer. The tm_dots command is a specialized version of tm_symbols, which has a very large set of options. We use two simple options: size=0.2 to make the points a little larger than the default, and col to set the color to red. tm_shape(nyc.bound) + tm_borders() + tm_shape(nycentroid) + tm_dots(size=0.2,col=&quot;red&quot;) A close examination of the map reveals that the centroid locations can be problematic for non-convex polygons. We can save the centroid point layer as a shape file by means of the st_write function we saw earlier (in the spatial data handling notes). st_write(nycentroid, &quot;nyc_centroids&quot;, driver = &quot;ESRI Shapefile&quot;) ## Writing layer `nyc_centroids&#39; to data source `nyc_centroids&#39; using driver `ESRI Shapefile&#39; ## Writing 55 features with 34 fields and geometry type Point. Common Map Classifications The statistical aspects of a choropleth map are expressed through the map classification that is used to convert observations for a continuous variable into a small number of bins, similar to what is the case for a histogram. Different classifications reveal different aspects of the spatial distribution of the variable. In tmap, the classification is selected by means of the style option in tm_fill. So far, we have used the default, which is set to pretty. The latter is a base R function to compute a sequence of roughly equally spaced values. Note that the number of intervals that results is not necessariy equal to n (the preferred number of classes), which can seem confusing at first.17 There are many classification types available in tmap, which each correspond to either a base R function, or to a custom expression contained in the classIntervals functionality. In this section, we review the quantile map, natural breaks map, and equal intervals map, and also illustrate how to set custom breaks with their own labels. For a detailed discussion of the methodological issues associated with these classifications, we refer to the GeoDa Workbook. We continue to use the same example for rent2008. Quantile map A quantile map is obtained by setting style=“quantile” in the tm_fill command. The number of categories is taken from the n argument, which has a default value of 5. So, for example, using this default will yield a quintile map with five categories (we specify the type of map in the title through tm_layout). tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;,style=&quot;quantile&quot;) + tm_borders() + tm_layout(title = &quot;Quintile Map&quot;, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) We follow the example in the GeoDa Workbook and illustrate a quartile map. This is created by specifying n=4, for four categories with style=“quantile”. The rest of the commands are the same as before. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;,n=4,style=&quot;quantile&quot;) + tm_borders() + tm_layout(title = &quot;Quartile Map&quot;, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) Natural breaks map A natural breaks map is obtained by specifying the style = “jenks” in tm_fill. All the other options are as before. Again, we illustrate this for four categories, with n=4. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;,n=4,style=&quot;jenks&quot;) + tm_borders() + tm_layout(title = &quot;Natural Breaks Map&quot;, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) Equal intervals map An equal intervals map for four categories is obtained by setting n=4, and style=“equal”. The resulting map replicates the result in GeoDa. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;,n=4,style=&quot;equal&quot;) + tm_borders() + tm_layout(title = &quot;Equal Intervals Map&quot;, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) Custom breaks For all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill function. To illustrate this, we mimic the example in the GeoDa Workbook, which uses the variable kids2000. The classification results in six categories. Unlike what is the case in the GeoDa Category Editor, in tmap the breaks include a minimum and maximum (in GeoDa, only the break points themselves need to be specified). As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order). It is always good practice to get some descriptive statistics on the variable before setting the break points. For kids2000, a summary command yields the following results. summary(nyc.bound$kids2000) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 8.382 30.301 38.228 36.040 42.773 55.367 Following the example in the GeoDa workbook, we set break points at 20, 30, 40, 45, and 50. In addition, we also need to include a minimum and maximum, which we set at 0 and 65. Our breaks vector is thus c(0,20,30,40,45,50,60). We also use the same color palette as in the GeoDa Workbook. This is a sequential scale, referred to as “YlOrBr” in ColorBrewer. The resulting map is identical to the one obtained with GeoDa. tm_shape(nyc.bound) + tm_fill(&quot;kids2000&quot;,title=&quot;Percentage Kids&quot;,breaks=c(0,20,30,40,45,50,60),palette=&quot;YlOrBr&quot;) + tm_borders() + tm_layout(title = &quot;Custom Breaks Map&quot;, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) Extreme Value Maps In addition to the common map classifications, GeoDa also supports three types of extreme value maps: a percentile map, box map, and standard deviation map. For details on the rationale and methodology behind these maps, we refer to the GeoDa Workbook. Of the three extreme value maps, only the standard deviation map is included as a style in tmap. To create the other two, we will need to use the custom breaks functionality. Ideally, we should create an additional style, but that’s beyond the scope of these notes. The three extreme value maps in GeoDa share the same color palette. This is a diverging scale with 6 categories, corresponding to the ColorBrewer “RdBu” scheme. We can illustrate the associated colors with a display.brewer.pal command. display.brewer.pal(6,&quot;RdBu&quot;) This seems OK at first sight, but the ranking is the opposite of the one used in GeoDa. Therefore, to reverse the order, we will need to preface the name with a “-” in the palette specification. Percentile map The percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. We can obtain the corresponding breakpoints by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included, as we already pointed out in the discussion of the generic custom break points. A percentile map is then created by passing the resulting break points to tm_fill. In addition, we can also pass labels for the new categories, using the labels option, for example, as labels = c(\"&lt; 1%\", \"1% - %10\", \"10% - 50%\", \"50% - 90%\",\"90% - 99%\", \"&gt; 99%\"). Extracting a variable from an sf data frame There is one catch, however. When variables are extracted from an sf dataframe, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile function gives an error. For example, this is illustrated when we extract the variable rent2008 and then apply the quantile function. percent &lt;- c(0,.01,.1,.5,.9,.99,1) var &lt;- nyc.bound[&quot;rent2008&quot;] quantile(var,percent) ## Error in xtfrm.data.frame(x): cannot xtfrm data frames We remove the geometry by means of the st_set_geometry(NULL) operation in sf, but it still doesn’t work. var &lt;- nyc.bound[&quot;rent2008&quot;] %&gt;% st_set_geometry(NULL) quantile(var,percent) ## Error in xtfrm.data.frame(x): cannot xtfrm data frames As it happens, the selection of the variable returns a data frame, not a vector, and the quantile function doesn’t know which column to use. We make that explicit by taking the first column (and all rows), as [,1]. Now, it works. var &lt;- nyc.bound[&quot;rent2008&quot;] %&gt;% st_set_geometry(NULL) quantile(var[,1],percent) ## 0% 1% 10% 50% 90% 99% 100% ## 0 0 950 1100 2140 2873 2900 Since we will have to carry out this kind of transformation any time we want to carry out some computation on a variable, we create a simple helper function to extract the variable vname from the simple features data frame df. We have added one extra feature, i.e., we remove the name of the vector by means of unname. get.var &lt;- function(vname,df) { # function to extract a variable as a vector out of an sf data frame # arguments: # vname: variable name (as character, in quotes) # df: name of sf data frame # returns: # v: vector with values (without a column name) v &lt;- df[vname] %&gt;% st_set_geometry(NULL) v &lt;- unname(v[,1]) return(v) } A percentile map function To create a percentile map for the variable rent2008 from scratch, we proceed in four steps. First, we set the cumulative percentages to compute the quantiles, in the vector percent. Second, we extract the variable as a vector using our function get.var. Next, we compute the breaks with the quantile function. Finally, we create the map using tm_fill with the breaks and labels option, and the palette set to “-RdBu” (to reverse the default order of the colors). percent &lt;- c(0,.01,.1,.5,.9,.99,1) var &lt;- get.var(&quot;rent2008&quot;,nyc.bound) bperc &lt;- quantile(var,percent) tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;,breaks=bperc,palette=&quot;-RdBu&quot;, labels=c(&quot;&lt; 1%&quot;, &quot;1% - %10&quot;, &quot;10% - 50%&quot;, &quot;50% - 90%&quot;,&quot;90% - 99%&quot;, &quot;&gt; 99%&quot;)) + tm_borders() + tm_layout(title = &quot;Percentile Map&quot;, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) We can summarize this in a simple function. Note that this is just a bare bones implementation. Additional arguments could be passed to customize various features of the map (such as the title, legend positioning, etc.), but we leave that for an exercise. Also, the function assumes (and does not check) that get.var is available. For general use, this should not be assumed. The minimal arguments we can pass are the variable name (vnam), the data frame (df), a title for the legend (legtitle, set to NA as default, the same default as in tm_fill), and an optional title for the map (mtitle, set to Percentile Map as the default). percentmap &lt;- function(vnam,df,legtitle=NA,mtitle=&quot;Percentile Map&quot;){ # percentile map # arguments: # vnam: variable name (as character, in quotes) # df: simple features polygon layer # legtitle: legend title # mtitle: map title # returns: # a tmap-element (plots a map) percent &lt;- c(0,.01,.1,.5,.9,.99,1) var &lt;- get.var(vnam,df) bperc &lt;- quantile(var,percent) tm_shape(df) + tm_fill(vnam,title=legtitle,breaks=bperc,palette=&quot;-RdBu&quot;, labels=c(&quot;&lt; 1%&quot;, &quot;1% - %10&quot;, &quot;10% - 50%&quot;, &quot;50% - 90%&quot;,&quot;90% - 99%&quot;, &quot;&gt; 99%&quot;)) + tm_borders() + tm_layout(title = mtitle, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) } We illustrate this for the rent2008 variable, without a legend title. percentmap(&quot;rent2008&quot;,nyc.bound) Box map To create a box map, we proceed largely along the same lines as for the percentile map, using the custom breaks specification. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present. In essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value). The same complication occurs at the upper end of the distribution. When there are upper outliers, then the last value for the breaks is the maximum value, and the next to last value is the upper fence. In contrast, when there are no upper outliers, then the last value is the upper fence, and the next to last is the maximum. As such, however, this is not satisfactory, because the default operation of the cuts is to have the interval.closure=“left”. This implies that the observation with the maximum value (the break point for the last category) will always be assigned to the outlier category when there are in fact no outliers. Similarly, the observation with the minimum value (the break point for the second category) will be assigned to the first group. We therefore create a small function to deal with these complications and compute the break points for a box map. Then we create a function for the box map along the same lines as for the percentile map. Computing the box map break points We follow the logic outlined above to create the 7 breakpoints (for the 6 categories) for the box map. We handle the problem with the interval closure by rounding the maximum value such that its observation will always fall in the proper category. We need to distinguish between a minimum and a maximum. For the former, we use the floor function, for the latter, the ceiling function. Our function boxbreaks returns the break points. It reuses the same calculations that we already exploited to compute the fences for the box plot. It takes a vector of observations and optionally a value for the multiplier as arguments (the default multiplier is 1.5). The function first gets the quartile values from the quantile function (the default is to compute quartiles, so we do not have to pass cumulative probabilities). Next, we calculate the upper and lower fence for the given multiplier value. The break points vector (bb) is initialized as a zero vector with 7 values (one more than the number of categories). The middle three values (3 to 5) will correspond to the first quartile, median, and third quartile. The first two values are allocated depending on whether there are lower outliers, and the same for the last two values. boxbreaks &lt;- function(v,mult=1.5) { # break points for box map # arguments: # v: vector with observations # mult: multiplier for IQR (default 1.5) # returns: # bb: vector with 7 break points # compute quartile and fences qv &lt;- unname(quantile(v)) iqr &lt;- qv[4] - qv[2] upfence &lt;- qv[4] + mult * iqr lofence &lt;- qv[2] - mult * iqr # initialize break points vector bb &lt;- vector(mode=&quot;numeric&quot;,length=7) # logic for lower and upper fences if (lofence &lt; qv[1]) { # no lower outliers bb[1] &lt;- lofence bb[2] &lt;- floor(qv[1]) } else { bb[2] &lt;- lofence bb[1] &lt;- qv[1] } if (upfence &gt; qv[5]) { # no upper outliers bb[7] &lt;- upfence bb[6] &lt;- ceiling(qv[5]) } else { bb[6] &lt;- upfence bb[7] &lt;- qv[5] } bb[3:5] &lt;- qv[2:4] return(bb) } We illustrate this for rent2008, taking advantage of our get.var function. var &lt;- get.var(&quot;rent2008&quot;,nyc.bound) boxbreaks(var) ## [1] 0.00 456.25 1000.00 1100.00 1362.50 1906.25 2900.00 In our example, there are both lower and upper outliers. As a result, the first and last values in the vector are, respectively, the minimum and maximum, and positions 2 and 6 are taken by the lower and upper fence. A box map function Our function boxmap is designed along the same lines as the percentile map. It assumes that both get.var and boxbreaks are available. Again, the minimal arguments we can pass are the variable name (vnam), the data frame (df), a title for the legend (legtitle, set to NA as default, the same default as in tm_fill), and an optional title for the map (mtitle, set to Percentile Map as the default). In addition, we also need to pass a value for the multiplier (mult, needed by boxbreaks). boxmap &lt;- function(vnam,df,legtitle=NA,mtitle=&quot;Box Map&quot;,mult=1.5){ # box map # arguments: # vnam: variable name (as character, in quotes) # df: simple features polygon layer # legtitle: legend title # mtitle: map title # mult: multiplier for IQR # returns: # a tmap-element (plots a map) var &lt;- get.var(vnam,df) bb &lt;- boxbreaks(var) tm_shape(df) + tm_fill(vnam,title=legtitle,breaks=bb,palette=&quot;-RdBu&quot;, labels = c(&quot;lower outlier&quot;, &quot;&lt; 25%&quot;, &quot;25% - 50%&quot;, &quot;50% - 75%&quot;,&quot;&gt; 75%&quot;, &quot;upper outlier&quot;)) + tm_borders() + tm_layout(title = mtitle, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) } Illustrated for the rent2008 variable: boxmap(&quot;rent2008&quot;,nyc.bound) 4.0.1 Standard deviation map A standard deviation map is included in tmap as style=“sd”. We follow the GeoDa Workbook and return to the rent2008 variable for our examples. Apart from setting the style, we also specify the palette=“-RdBu”. This yields the same map as GeoDa. tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;,title=&quot;Rent in 2008&quot;,style=&quot;sd&quot;,palette=&quot;-RdBu&quot;) + tm_borders() + tm_layout(title = &quot;Standard Deviation Map&quot;, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) Mapping Categorical Variables In the GeoDa Workbook, the mapping of categorical variables is illustrated by extracting the classifications from a map as integer values. There doesn’t seem to be an obvious way to do this for a tmap element. Instead, we will resort to the base R function cut to create integer values for a classification. We will then turn these into factors to be able to use the style=“cat” option in tm_fill. Note that it is important to have the variable as a factor. When integer values are used as such, the resulting map follows a standard sequential classification. Creating categories We use the same example variables as in the GeoDa Workbook and create box map categories for kids2000 and pubast00. We first extract the variable, using our get.var function. var &lt;- get.var(&quot;kids2000&quot;,nyc.bound) Next, we construct a vector with the boxmap breakpoints using our boxbreaks function. vbreaks &lt;- boxbreaks(var) vbreaks ## [1] 8.38150 11.59360 30.30115 38.22780 42.77285 56.00000 61.48040 We verify the treatment of lower and upper fences by comparing the result to a summary. summary(var) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 8.382 30.301 38.228 36.040 42.773 55.367 We observe that the first value in the breaks vector is the minimum, indicating lower outliers. However, the last value is larger than the maximum, indicating the lack of upper outliers. Consequently, our integer codes will not include the value 6. We use the boxmap breaks to create a classification by means of the cut function. We pass the vector of observations and the vector of breaks and then specify some important options. The default in cut is to create labels for the categories as strings that show the lower and upper bound. Instead, we want to have a numeric code, so we set the option labels=FALSE (the opposite of the default). Also, we want to make sure that the mininum value is included if it is the lowest value (the default is that it would not be) by setting include.lowest=TRUE. The result is a vector of integer codes. Since we only have 55 observations, we can list the result (note the absence of a code of 6). vcat &lt;- cut(var,breaks=vbreaks,labels=FALSE,include.lowest=TRUE) vcat ## [1] 4 3 4 2 2 4 4 3 2 3 3 5 4 3 5 4 4 1 2 1 1 2 2 3 3 3 4 5 5 5 5 5 3 5 2 3 4 3 ## [39] 5 5 2 2 3 5 5 5 5 4 4 2 4 4 3 3 2 Finally, we need to add the new variable as a factor to the simple features layer, using the standard $ notation. We call the new variable kidscat, as in the GeoDa Workbook. We could (and should) turn this whole operation into a function, but we leave that as an exercise for now. nyc.bound$kidscat &lt;- as.factor(vcat) Unique value map We are now in a position to create a unique value map for the new variable kidscat. All the options are as before, except that style=“cat” and we set palette=“Paired” to replicate the map in GeoDa. tm_shape(nyc.bound) + tm_fill(&quot;kidscat&quot;,style=&quot;cat&quot;,palette=&quot;Paired&quot;) + tm_borders() + tm_layout(title = &quot;Unique Value Map&quot;, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) Co-location map A special case of a map for categorical variables is a so-called co-location map, implemented in GeoDa. This map shows the values for those locations where two categorical variables take on the same value (it is up to the user to make sure the values make sense). Further details are given in the GeoDa Workbook. To replicate a co-location map in R takes a little bit of work. First we need to identify the locations that match. Then, in order to make a map that makes sense, we need to make sure we pick the right colors from the color palette. We follow the example in the GeoDa Workbook and create a co-location map for the box map categories for kids2000 and pubast00. We already created the former, as the new variable kidscat. We still need to create a categorical variable for the latter. We proceed in the same way as before to add the variable asstcat to the nyc.bound layer. var2 &lt;- get.var(&quot;pubast00&quot;,nyc.bound) vb2 &lt;- boxbreaks(var2) vcat2 &lt;- cut(var2,breaks=vb2,labels=FALSE,include.lowest=TRUE) nyc.bound$asstcat &lt;- as.factor(vcat2) A unique value map illustrates the spatial distribution of the categories. tm_shape(nyc.bound) + tm_fill(&quot;asstcat&quot;,style=&quot;cat&quot;,palette=&quot;Paired&quot;) + tm_borders() + tm_layout(title = &quot;Unique Value Map&quot;, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) Identifying the matching locations We find the positions of the matching locations by checking the equality between the variables kidscat and asstcat (note that we need to specify the data frame, using the $ notation to select the variable). Since there is no guarantee that the two variables have the same number of categories (in fact, kidscat has categories 1-5, whereas asstcat has 2-5), a direct comparison of the factors might fail. Therefore, we first turn the factors into numeric values, and then do the comparison. In order to make sure that the resulting integers are the correct categories, we must first convert the factors into strings, using as.character. For example, since our second variable starts at level 2, a simple as.numeric will turn that into 1 (since it is the first level). Finally, we use the which command to find the positions of the observations where the condition is TRUE. There are 25 such locations (out of 55). v1 &lt;- as.numeric(as.character(nyc.bound$kidscat)) v2 &lt;- as.numeric(as.character(nyc.bound$asstcat)) ch &lt;- v1 == v2 locmatch &lt;- which(ch == TRUE) locmatch ## [1] 5 8 9 17 22 23 28 29 30 31 32 33 34 35 37 39 40 44 45 48 49 51 52 53 54 length(locmatch) ## [1] 25 Since the two variables use the same coding scheme, and the matching locations have the same value, we can use either one to extract a categorical value for the unique value map. We start by initializing a vector of zeros equal in length to the number of observations. We will use a code of 0 for those locations without a match. We next set the vector locations with matching values (the indices contained in locmatch) to their corresponding values in kidscat. Finally, we add the new vector to the simple features layer as a factor. matchcat &lt;- vector(mode=&quot;numeric&quot;,length=length(nyc.bound$kidscat)) matchcat[locmatch] &lt;- nyc.bound$kidscat[locmatch] nyc.bound$colocate &lt;- as.factor(matchcat) Customizing the color palette We continue to follow the GeoDa Workbook example and use the color palette for a box map (since both categorical variables were derived from a box map). This is the “RdBu” color scheme in RColorBrewer. However, as we pointed out before, the color order needs to be reversed. We accomplish this with the rev function. In addition, we need to include a color for the mismatch category, which we have set to 0. We take a shade of white for those locations, more specifically, so-called white smoke, with a HEX code of “#F5F5F5”. We add this value in front of the other codes and print the result as a check. pal &lt;- rev(brewer.pal(6,&quot;RdBu&quot;)) pal &lt;- c(&quot;#F5F5F5&quot;,pal) pal ## [1] &quot;#F5F5F5&quot; &quot;#2166AC&quot; &quot;#67A9CF&quot; &quot;#D1E5F0&quot; &quot;#FDDBC7&quot; &quot;#EF8A62&quot; &quot;#B2182B&quot; If we use the full palette, we get the following result. tm_shape(nyc.bound) + tm_fill(&quot;colocate&quot;,style=&quot;cat&quot;,palette=pal) + tm_borders() + tm_layout(title = &quot;Co-Location Map&quot;, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) Upon closer examination of the example in the GeoDa Workbook, we see that the color is shifted down for all categories. For example, the color for category 2 should be light blue, not dark blue. To fix this aspect of the legend, we identify those codes that appear in the colocate vector. This is a bit complicated, since those values are factors. As before, first we need to turn them into strings using as.character, and then we turn this into integers by means of as.numeric. We extract the codes that are present with the unique operator. Next, we need to add 1 to those values to be able to refer to positions in a vector (positions start at 1, not 0). Finally, we extract the proper subset of the color palette. mcats &lt;- unique(as.numeric(as.character(nyc.bound$colocate))) mcats &lt;- mcats+1 colpal &lt;- pal[mcats] We now use this palette in our categorical map to completely replicate the result in GeoDa. tm_shape(nyc.bound) + tm_fill(&quot;colocate&quot;,style=&quot;cat&quot;,palette=colpal) + tm_borders() + tm_layout(title = &quot;Co-Location Map&quot;, title.position = c(&quot;right&quot;,&quot;bottom&quot;)) Conditional Map A conditional map, or facet map, or small multiples, is created by the tm_facets command. This largely follows the logic of the facet_grid command in ggplot that we covered in the EDA notes. An extensive set of options is available to customize the facet maps. An in-depth coverage of all the subtleties is beyond our scope (details can be found on the tm_facets documentation page) Whereas in GeoDa, the conditions can be based on the original variables, here, we need to first create the conditioning variables as factors. We follow the same procedure as before, but here we use the standard cut function to create the factors. We specify the breaks argument as 2. We follow the example in the GeoDa Workbook and use hhsiz00 and forhis08 as the conditioning variables. The new factors are cut.hhsiz and cut.forhis. They are added to the nyc.bound layer using the familiar $ operation. For example, for hhsiz00: nyc.bound$cut.hhsiz &lt;- cut(nyc.bound$hhsiz00,breaks=2) nyc.bound$cut.hhsiz ## [1] (2.39,3.2] (2.39,3.2] (2.39,3.2] (2.39,3.2] (2.39,3.2] (2.39,3.2] ## [7] (2.39,3.2] (2.39,3.2] (1.57,2.39] (2.39,3.2] (2.39,3.2] (2.39,3.2] ## [13] (2.39,3.2] (2.39,3.2] (2.39,3.2] (2.39,3.2] (2.39,3.2] (1.57,2.39] ## [19] (1.57,2.39] (1.57,2.39] (1.57,2.39] (1.57,2.39] (1.57,2.39] (2.39,3.2] ## [25] (1.57,2.39] (2.39,3.2] (2.39,3.2] (2.39,3.2] (2.39,3.2] (2.39,3.2] ## [31] (2.39,3.2] (2.39,3.2] (1.57,2.39] (2.39,3.2] (1.57,2.39] (2.39,3.2] ## [37] (2.39,3.2] (2.39,3.2] (2.39,3.2] (2.39,3.2] (1.57,2.39] (1.57,2.39] ## [43] (2.39,3.2] (2.39,3.2] (2.39,3.2] (2.39,3.2] (2.39,3.2] (2.39,3.2] ## [49] (2.39,3.2] (1.57,2.39] (2.39,3.2] (2.39,3.2] (2.39,3.2] (2.39,3.2] ## [55] (2.39,3.2] ## Levels: (1.57,2.39] (2.39,3.2] And, similarly for cut.forhis: nyc.bound$cut.forhis &lt;- cut(nyc.bound$forhis08,breaks=2) nyc.bound$cut.forhis ## [1] (9.63,39.5] (9.63,39.5] (9.63,39.5] (39.5,69.4] (39.5,69.4] (39.5,69.4] ## [7] (39.5,69.4] (9.63,39.5] (39.5,69.4] (39.5,69.4] (39.5,69.4] (39.5,69.4] ## [13] (39.5,69.4] (39.5,69.4] (39.5,69.4] (9.63,39.5] (39.5,69.4] (9.63,39.5] ## [19] (9.63,39.5] (9.63,39.5] (39.5,69.4] (9.63,39.5] (39.5,69.4] (39.5,69.4] ## [25] (39.5,69.4] (9.63,39.5] (39.5,69.4] (9.63,39.5] (9.63,39.5] (39.5,69.4] ## [31] (39.5,69.4] (39.5,69.4] (39.5,69.4] (9.63,39.5] (9.63,39.5] (9.63,39.5] ## [37] (9.63,39.5] (9.63,39.5] (39.5,69.4] (9.63,39.5] (9.63,39.5] (9.63,39.5] ## [43] (9.63,39.5] (9.63,39.5] (9.63,39.5] (9.63,39.5] (39.5,69.4] (39.5,69.4] ## [49] (39.5,69.4] (9.63,39.5] (39.5,69.4] (39.5,69.4] (9.63,39.5] (39.5,69.4] ## [55] (9.63,39.5] ## Levels: (9.63,39.5] (39.5,69.4] The two new factor variables are used as conditioning variables in the by argument of the tm_facets command. The first variable conditions the rows (i.e., is the y-axis), the second variable conditions the columns (i.e., is the x-axis). We illustrate this with the rent2008 variable, using the default map type. Two important options that affect the look of the conditional map are free.coords and drop.units. The default is that each facet map has its own scaling, focused on the relevant observations. Typically, one wants the same spatial layout in each facet, i.e., all the sub-boroughs in our example. This is ensured by setting free.coords=FALSE. In addition, we want all the spatial units to be shown in each facet (the default is to only show those that match the conditions). This is accomplished by setting drop.units=FALSE. The result is as follows: tm_shape(nyc.bound) + tm_fill(&quot;rent2008&quot;) + tm_borders() + tm_facets(by = c(&quot;cut.hhsiz&quot;, &quot;cut.forhis&quot;),free.coords = FALSE,drop.units=FALSE) Cartogram A final map functionality that we replicate from the GeoDa Workbook is the cartogram. GeoDa implements a so-called circular cartogram, where circles represent spatial units and their size is proportional to a specified variable. In R, a useful implementation of different types of cartograms is included in the package cartogram. Specifically, this supports the circular cartogram, as cartogram_dorling, as well as contiguous and non-contiguous cartograms, as, respectively, cartogram_cont and cartogram_ncont. The result of these functions is a simple features layer, which can then be mapped by means of the usual tmap commands. For example, following the GeoDa Workbook, we take the rent2008 variable to construct a circular cartogram. We pass the layer (nyc.bound) and the variable to the cartogram_dorling function. We check that the result is an sf object. carto.dorling &lt;- cartogram_dorling(nyc.bound,&quot;rent2008&quot;) class(carto.dorling) ## [1] &quot;sf&quot; &quot;data.frame&quot; We can now map the cartogram with the usual tmap command. We just use the default settings here, but, obviously, all the customizations we covered before can be applied to the cartogram as well. tm_shape(carto.dorling) + tm_fill(&quot;rent2008&quot;) + tm_borders() As a bonus, we illustrate the contiguous cartogram, which stretches the boundaries such that the spatial units remain contiguous, but take on an area proportional to the variable of interest (the nonlinear algorithm to compute the new boundaries may take a while). Again, the result can be mapped in the usual way. carto.cont &lt;- cartogram_cont(nyc.bound,&quot;rent2008&quot;) tm_shape(carto.cont) + tm_fill(&quot;rent2008&quot;) + tm_borders() And, finally, a non-contiguous cartogram, which has the spatial units floating in space. carto.ncont &lt;- cartogram_ncont(nyc.bound,&quot;rent2008&quot;) tm_shape(carto.ncont) + tm_fill(&quot;rent2008&quot;) + tm_borders() Use setwd(directorypath) to specify the working directory.↩︎ Use install.packages(packagename).↩︎ The older function save_tmap has been deprecated.↩︎ For example, in our first map, the default value for n is 5, but the pretty map that is constructed has 6 categories. Other classifications provide more control over the number of intervals.↩︎ "],["rate-mapping.html", "Chapter 5 Rate Mapping Introduction 5.1 Preliminaries 5.2 Choropleth Map for Rates 5.3 Excess risk 5.4 Empirical Bayes Smoothed Rate Map 5.5 EB rate map", " Chapter 5 Rate Mapping Introduction This notebook covers the functionality of the Rate Mapping section of the GeoDa workbook. We refer to that document for details on the methodology, references, etc. The goal of these notes is to approximate as closely as possible the operations carried out using GeoDa by means of a range of R packages. The notes are written with R beginners in mind, more seasoned R users can probably skip most of the comments on data structures and other R particulars. Also, as always in R, there are typically several ways to achieve a specific objective, so what is shown here is just one way that works, but there often are others (that may even be more elegant, work faster, or scale better). For this notebook, we use Cleveland house price data. Our goal in this lab is show how to assign spatial weights based on different distance functions. 5.0.1 Objectives After completing the notebook, you should know how to carry out the following tasks: Obtain a coordinate reference system Create thematic maps for rates Assess extreme value rate values by means of an excess risk map Understand the principle behind shrinkage estimation or smoothing rates Use base R to compute rates with bivariate operations 5.0.1.1 R Packages used sf: To read in the shapefile. tmap: To create various rate maps and build up the necessary custom specifications for the box map classification geodaData: To load the dataset for the notebook. 5.0.1.2 R Commands used Below follows a list of the commands used in this notebook. For further details and a comprehensive list of options, please consult the R documentation. Base R: install.packages, library, setwd, class, str, sf: plot, st_crs, st_set_geometry tmap: tm_shape, tm_fill, tm_borders, tm_layout, tmap_mode, tm_basemap 5.1 Preliminaries 5.1.1 Load packages First, we load all the required packages using the library command. If you don’t have some of these in your system, make sure to install them first as well as their dependencies.18 You will get an error message if something is missing. If needed, just install the missing piece and everything will work after that. library(sf) library(tmap) library(geodaData) 5.1.2 geodaData All of the data for the R notebooks is available in the geodaData package. We loaded the library earlier, now to access the individual data sets, we use the double colon notation. This works similar to to accessing a variable with $, in that a drop down menu will appear with a list of the datasets included in the package. For this notebook, we use ohio_lung. ohio_lung &lt;- geodaData::ohio_lung 5.2 Choropleth Map for Rates 5.2.1 Spatially extensive and intensive variables We start our discussion of rate maps by illustrating something we should not be doing. This pertains to the important difference between a spatially extensive and a spatially intensive variable. In many applications that use public health data, we typically have access to a count of events, such as the number of cancer cases (a spatially extensive variable), as well as to the relevant population at risk, which allows for the calculation of a rate (a spatially intensive variable). 5.2.1.1 Setting up the Boxmap option Throughout this notebook, we will be using the boxmap function, created in the previous notebook: Basic Mapping. We won’t go into depth on how the function was created, but for this information, check out the Basic Mapping notebook chapter. get.var &lt;- function(vname,df) { # function to extract a variable as a vector out of an sf data frame # arguments: # vname: variable name (as character, in quotes) # df: name of sf data frame # returns: # v: vector with values (without a column name) v &lt;- df[vname] %&gt;% st_set_geometry(NULL) v &lt;- unname(v[,1]) return(v) } boxbreaks &lt;- function(v,mult=1.5) { # break points for box map # arguments: # v: vector with observations # mult: multiplier for IQR (default 1.5) # returns: # bb: vector with 7 break points # compute quartile and fences qv &lt;- unname(quantile(v)) iqr &lt;- qv[4] - qv[2] upfence &lt;- qv[4] + mult * iqr lofence &lt;- qv[2] - mult * iqr # initialize break points vector bb &lt;- vector(mode=&quot;numeric&quot;,length=7) # logic for lower and upper fences if (lofence &lt; qv[1]) { # no lower outliers bb[1] &lt;- lofence bb[2] &lt;- qv[1] } else { bb[2] &lt;- lofence bb[1] &lt;- qv[1] } if (upfence &gt; qv[5]) { # no upper outliers bb[7] &lt;- upfence bb[6] &lt;- qv[5] } else { bb[6] &lt;- upfence bb[7] &lt;- qv[5] } bb[3:5] &lt;- qv[2:4] return(bb) } boxmap &lt;- function(vnam,df,legtitle=NA,mtitle=&quot;Box Map&quot;,mult=1.5){ # box map # arguments: # vnam: variable name (as character, in quotes) # df: simple features polygon layer # legtitle: legend title # mtitle: map title # mult: multiplier for IQR # returns: # a tmap-element (plots a map) var &lt;- get.var(vnam,df) bb &lt;- boxbreaks(var) tm_shape(df) + tm_fill(vnam,title=legtitle,breaks=bb,palette=&quot;-RdBu&quot;, labels = c(&quot;lower outlier&quot;, &quot;&lt; 25%&quot;, &quot;25% - 50%&quot;, &quot;50% - 75%&quot;,&quot;&gt; 75%&quot;, &quot;upper outlier&quot;)) + tm_borders() + tm_layout(title = mtitle, title.position = c(&quot;right&quot;,&quot;bottom&quot;),legend.outside = TRUE, legend.outside.position = &quot;right&quot;) } tmap_mode(&quot;view&quot;) boxmap(&quot;LFW68&quot;, ohio_lung) + tm_basemap(server=&quot;OpenStreetMap&quot;,alpha=0.5) Anyone familiar with the geography of Ohio will recognize the outliers as the counties with the largest populations, i.e., the metropolitan areas of Cincinnati, Columbus, Cleveland, etc. The labels for these cities in the base layer make this clear. This highlights a major problem with spatially extensive variables like total counts, in that they tend to vary with the size (population) of the areal units. So, everything else being the same, we would expect to have more lung cancer cases in counties with larger populations Instead, we opt for a spatially intensive variable, such as the ratio of the number of cases over the population. More formally, if \\(O_i\\) is the number of cancer cases in area i, and \\(P_i\\) is the corresponding population at risk (in our example, the total number of white females), then the raw or crude rate or proportion follows as: \\[r_i = \\frac{O_i}{P_i}\\] 5.2.1.2 Variance instability The crude rate is an estimator for the unknown underlying risk. In our example, that would be the risk of a white woman to be exposed to lung cancer. The crude rate is an unbiased estimator for the risk, which is a desirable property. However, its variance has an undesirable property, namely \\[Var[r_i] = \\frac{\\pi_i(1-\\pi_i)}{P_i}\\] where \\(\\pi_i\\) is the underlying risk in area i. This implies that the larger the population of an area (\\(P_i\\) in the denominator), the smaller the variance for the estimator, or, in other words, the greater the precision. The flip side of this result is that for areas with sparse populations (small \\(P_i\\)), the estimate for the risk will be imprecise (large variance). Moreover, since the population typically varies across the areas under consideration, the precision of each rate will vary as well. This variance instability needs to somehow be reflected in the map, or corrected for, to avoid a spurious representation of the spatial distribution of the underlying risk. This is the main motivation for smoothing rates, to which we return below. 5.2.2 Raw rate map To compute the raw rate, we just divide the count of events by the population for each county. ohio_lung$raw_rate &lt;- ohio_lung$LFW68 / ohio_lung$POPFW68 We don’t need the basemap for the rest of the notebook, so we switch back to plot mode in tmap. This is done with tmap_mode plot. Using the boxmap function from earlier we make a map of the lung cancer counts and the raw population based rates. tmap_mode(&quot;plot&quot;) p1 &lt;- boxmap(&quot;LFW68&quot;, ohio_lung) p2 &lt;- boxmap(&quot;raw_rate&quot;, ohio_lung) p2 With tmap_arrange, we can get a side by side comparison of the two maps. tmap_arrange(p1,p2,ncol = 2) With the adjustment for population, the map becomes more meaningful than just the raw count data. We see different upper outliers and a new spatial distribution after this adjustment. 5.3 Excess risk 5.3.1 Relative risk A commonly used notion in demography and public health analysis is the concept of a standardized mortality rate (SMR), sometimes also referred to as relative risk or excess risk. The idea is to compare the observed mortality rate to a national (or regional) standard. More specifically, the observed number of events is compared to the number of events that would be expected had a reference risk been applied. In most applications, the reference risk is estimated from the aggregate of all the observations under consideration. For example, if we considered all the counties in Ohio, the reference rate would be the sum of all the events over the sum of all the populations at risk. Note that this average is not the average of the county rates. Instead, it is calculated as the ratio of the total sum of all events over the total sum of all populations at risk (e.g., in our example, all the white female deaths in the state over the state white female population). Formally, this is expressed as: \\[\\pi=\\frac{\\Sigma_{i=1}^{i=n}O_i}{\\Sigma_{i=1}^{i=n}P_i}\\] which yields the expected number of events for each area i as: \\[E_i=\\pi*P_i\\] The relative risk then follows as the ratio of the observed number of events (e.g., cancer cases) over the expected number: \\[SMR_i=\\frac{O_i}{E_i}\\] 5.3.2 Excess risk map To calculate the excess risk of each county, we need to do a series of computations first as opposed to GeoDa, which does it automatically. We start by calculating the reference rate, which is just the sum of events over the sum of the population. sum_observed &lt;- sum(ohio_lung$LFW68) sum_population &lt;- sum(ohio_lung$POPFW68) p_i &lt;- sum_observed / sum_population Next we calculate the expected number of events for each county based on the reference rate for the whole state and the population for each county. E_i &lt;- p_i * ohio_lung$POPFW68 Lasty, we divide the actual count by the expected count to get the relative risk rate. This ratio will show us which counties have a higher than expected number of lung cancer cases, and which counties have a lower than expected count. ohio_lung$smr &lt;- ohio_lung$LFW68 / E_i In the excess risk map, blue counties will indicate a risk lower than the state average, or \\(SMR_i &lt; 1\\). Red counties indicate a risk higher than the state average, or \\(SMR_i &gt; 1\\). p1 &lt;- tm_shape(ohio_lung) + tm_fill(&quot;smr&quot;,title=&quot;Excess risk&quot;,breaks=c(-100,.25,.5,1,2,4,1000),labels = c(&quot;&lt;.25&quot;, &quot;.25 - .50&quot;, &quot;.50 - 1.00&quot;,&quot;1.00 - 2.00&quot;, &quot;2.00 - 4.00&quot;, &quot;&gt; 4.00&quot; ), palette = &quot;-RdBu&quot;) + tm_borders() + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) p1 Additionally, we can examine the excess risk rate in the form of a boxmap. The boxmap utilizes the full distribution of the rates to identify outliers, compared to the relative risk map, which identifies them as having a value greater than two. p2 &lt;- boxmap(&quot;smr&quot;,ohio_lung) p2 Here we use tmap_arrange to get a side by side comparison again. tmap_arrange(p1,p2,ncol = 2) 5.4 Empirical Bayes Smoothed Rate Map 5.4.1 Borrowing strength As mentioned in the introduction, rates have an intrinsic variance instability, which may lead to the identification of spurious outliers. In order to correct for this, we can use smoothing approaches (also called shrinkage estimators), which improve on the precision of the crude rate by borrowing strength from the other observations. This idea goes back to the fundamental contributions of James and Stein (the so-called James-Stein paradox), who showed that in some instances biased estimators may have better precision in a mean squared error sense. GeoDa includes three methods to smooth the rates: an Empirical Bayes approach, a spatial averaging approach, and a combination between the two. We will consider the spatial approaches after we discuss distance-based spatial weights. Here, we focus on the Empirical Bayes (EB) method. First, we provide some formal background on the principles behind smoothing and shrinkage estimators. 5.4.2 Bayes law The formal logic behind the idea of smoothing is situated in a Bayesian framework, in which the distribution of a random variable is updated after observing data. The principle behind this is the so-called Bayes Law, which follows from the decomposition of a joint probability (or density) into two conditional probabilities: \\[P[AB] = P[A|B]*P[B] = P[B|A]*P[A]\\] where A and B are random events, and | stands for the conditional probability of one event, given a value for the other. The second equality yields the formal expression of Bayes law as: \\[P[A|B] = \\frac{P[B|A]*P[A]}{P[B]}\\] In most instances in practice, the denominator in this expression can be ignored, and the equality sign is replaced by a proportionality sign: \\[P[A|B]\\propto P[B|A]*P[A]\\] \\[P[\\pi|y]\\propto P[Y|\\pi] * P[\\pi]\\] 5.4.3 The Poisson-Gamma model For each particular estimation problem, we need to specify distributions for the prior and the likelihood in such a way that a proper posterior distribution results. In the context of rate estimation, the standard approach is to specify a Poisson distribution for the observed count of events (conditional upon the risk parameter), and a Gamma distribution for the prior of the risk parameter \\(\\pi\\). This is referred to as the Poisson-Gamma model. In this model, the prior distribution for the (unknown) risk parameter \\(\\pi\\) is \\(Gamma(\\alpha,\\beta)\\), where \\(\\alpha\\) and \\(\\beta\\) are the shape and scale parameters of the Gamma distribution. In terms of the more familiar notions of mean and variance, this implies: \\[E[\\pi] = \\alpha/\\beta\\] and \\[Var[\\pi] = \\alpha/\\beta^2\\] Using standard Bayesian principles, the combination of a Gamma prior for the risk parameter with a Poisson distribution for the count of events (O) yields the posterior distribution as \\(Gamma(O+\\alpha,P + \\beta)\\). The new shape and scale parameters yield the mean and variance of the posterior distribution for the risk parameter as: \\[E[\\pi]= \\frac{O+ \\alpha}{P + \\beta}\\] and \\[Var[\\pi] = \\frac{O + \\alpha}{(P+\\beta)^2}\\] Different values for the \\(\\alpha\\) and \\(\\beta\\) parameters (reflecting more or less precise prior information) will yield smoothed rate estimates from the posterior distribution. In other words, the new risk estimate adjusts the crude rate with parameters from the prior Gamma distribution. 5.4.4 The Empirical Bayes approach In the Empirical Bayes approach, values for \\(\\alpha\\) and \\(\\beta\\) of the prior Gamma distribution are estimated from the actual data. The smoothed rate is then expressed as a weighted average of the crude rate, say r, and the prior estimate, say \\(\\theta\\). The latter is estimated as a reference rate, typically the overall statewide average or some other standard. In essense, the EB technique consists of computing a weighted average between the raw rate for each county and the state average, with weights proportional to the underlying population at risk. Simply put, small counties (i.e., with a small population at risk) will tend to have their rates adjusted considerably, whereas for larger counties the rates will barely change. More formally, the EB estimate for the risk in location i is: \\[\\pi_i^{EB}=w_ir_i + (1-w_i)\\theta\\] In this expression, the weights are: \\[w_i = \\frac{\\sigma^2}{(\\sigma^2 + \\mu/P_i)}\\] with \\(P_i\\) as the population at risk in area i, and \\(\\mu\\) and \\(\\sigma^2\\) as the mean and variance of the prior distribution. In the empirical Bayes approach, the mean \\(\\mu\\) and variance \\(\\sigma^2\\) of the prior (which determine the scale and shape parameters of the Gamma distribution) are estimated from the data. For \\(\\mu\\) this estimate is simply the reference rate(the same reference used in the computation of SMR), \\(\\Sigma_{i=1}^{i=n}O_i/\\Sigma_{i=1}^{i=n}P_i\\). The estimate of the variance is a bit more complex: \\[\\sigma^2=\\frac{\\Sigma_{i=1}^{i=n}P_i(r_i-\\mu)^2}{\\Sigma_{i=1}^{i=n}P_i}-\\frac{\\mu}{\\Sigma_{i=1}^{i=n}P_i/n}\\] While easy to calculate, the estimate for the variance can yield negative values. In such instances, the conventional approach is to set \\(\\sigma^2\\) to zero. As a result, the weight \\(w_i\\) becomes zero, which in essence equates the smoothed rate estimate to the reference rate. 5.5 EB rate map We start by computing all of the necessary parameters for the variance formula above. This includes \\(\\mu\\), n , the crude rate, \\(r_i\\), \\(O_i\\), and \\(P_i\\). mu &lt;- sum(ohio_lung$LFW68) / sum(ohio_lung$POPFW68) O_i &lt;- ohio_lung$LFW68 P_i &lt;- ohio_lung$POPFW68 n &lt;- length(ohio_lung$POPFW68) r_i &lt;- O_i / P_i Next we compute the variance, to similify the code, we compute the top left portion or \\[\\Sigma_{i=1}^{i=n}P_i(r_i-\\mu)^2\\] as top_left, then compute the variance with the next line. top_left &lt;- sum(P_i * (r_i - mu)^2) variance &lt;- top_left / sum(P_i) - mu / sum(P_i / n) Now that we have the variance, we can compute the \\(w_i\\) values. w_i &lt;- variance / (variance + mu/P_i) Here we use the final formula for the smoothed rates. ohio_lung$eb_bayes &lt;- w_i * r_i + (1 - w_i) * mu ohio_lung$eb_bayes ## [1] 1.150783e-04 1.065711e-04 1.079228e-04 1.029445e-04 1.169062e-04 ## [6] 1.025309e-04 9.454283e-05 1.038335e-04 9.849086e-05 1.064611e-04 ## [11] 1.109825e-04 1.096707e-04 1.058050e-04 1.009664e-04 9.588061e-05 ## [16] 1.166735e-04 1.048742e-04 1.143876e-04 1.091126e-04 1.175319e-04 ## [21] 1.069775e-04 1.191585e-04 1.176190e-04 1.068460e-04 1.069400e-04 ## [26] 1.071089e-04 1.036440e-04 1.119544e-04 9.280937e-05 1.153709e-04 ## [31] 1.071074e-04 1.035920e-04 1.026519e-04 1.086969e-04 1.123365e-04 ## [36] 1.143454e-04 1.087786e-04 1.084304e-04 1.023274e-04 1.243397e-04 ## [41] 1.114789e-04 1.228748e-04 1.050183e-04 1.089482e-04 1.063903e-04 ## [46] 1.115513e-04 1.095161e-04 1.037763e-04 1.128505e-04 1.070939e-04 ## [51] 1.158590e-04 1.074788e-04 1.082377e-04 1.179104e-04 1.167411e-04 ## [56] 1.044656e-04 1.010171e-04 1.071996e-04 1.157866e-04 1.041863e-04 ## [61] 9.693106e-05 1.063645e-04 1.130748e-04 1.027615e-04 1.122900e-04 ## [66] 1.070648e-04 1.113361e-04 1.156885e-04 1.024824e-04 1.015521e-04 ## [71] 1.021452e-04 1.035587e-04 1.064004e-04 1.055062e-04 1.074256e-04 ## [76] 1.208589e-04 1.339407e-04 1.122428e-04 1.113038e-04 1.107139e-04 ## [81] 1.089123e-04 1.090464e-04 1.058359e-04 1.048680e-04 1.176496e-04 ## [86] 1.124378e-04 1.147444e-04 9.951653e-05 Lastly, we plot the EB smoothed rates with the boxmap function. boxmap(&quot;eb_bayes&quot;,ohio_lung) In comparison to the box map for the crude rates and the excess rate map, none of the original outliers remain identified as such in the smoothed map. Instead, a new outlier is shown in the very southwestern corner of the state (Hamilton county). Since many of the original outlier counties have small populations at risk (check in the data table), their EB smoothed rates are quite different (lower) from the original. In contrast, Hamilton county is one of the most populous counties (it contains the city of Cincinnati), so that its raw rate is barely adjusted. Because of that, it percolates to the top of the distribution and becomes an outlier. Use install.packages(packagename).↩︎ "],["contiguity-based-spatial-weights.html", "Chapter 6 Contiguity-Based Spatial Weights Introduction Preliminaries Contiguity Weights Higher Order Contiguity Visualizing Contiguity Neighbors Saving Neighbors", " Chapter 6 Contiguity-Based Spatial Weights Introduction This notebook covers the functionality of the Contiguity-Based Spatial Weights section of the GeoDa workbook. We refer to that document for details on the methodology, references, etc. The goal of these notes is to approximate as closely as possible the operations carried out using GeoDa by means of a range of R packages. The notes are written with R beginners in mind, more seasoned R users can probably skip most of the comments on data structures and other R particulars. Also, as always in R, there are typically several ways to achieve a specific objective, so what is shown here is just one way that works, but there often are others (that may even be more elegant, work faster, or scale better). For this notebook, we use U.S. Homicide data. Our goal in this lab is show how to implement contiguity based spatial weights Objectives After completing the notebook, you should know how to carry out the following tasks: Construct rook and queen contiguity-based spatial weights Compute higher order contiguity weights Save weights information Assess the characteristics of spatial weights Visualize the graph structure of spatial weights Identify the neighbors of selected observations R Packages used sf: To read in the shapefile, add centroids, and create the neighbors lists purrr: Used to map a function over each element of a vector ggplot2: To make a connectivity histogram spdep: Save weights files and create neighbors lists of higher order **geodaData: Load the data for the notebook. R Commands used Below follows a list of the commands used in this notebook. For further details and a comprehensive list of options, please consult the R documentation. Base R: install.packages, library, setwd, class, str, lapply, attributes, summary, head, seq sf: plot, st_centroid, st_relate purrr: map_dbl ggplot2: ggplot, geom_histogram, aes, xlab spdep: write.nb.gal, nblag, nblag_cumul, card Preliminaries Before starting, make sure to have the latest version of R and of packages that are compiled for the matching version of R (this document was created using R 3.5.1 of 2018-07-02). Also, optionally, set a working directory, even though we will not actually be saving any files.19 Load packages First, we load all the required packages using the library command. If you don’t have some of these in your system, make sure to install them first as well as their dependencies.20 You will get an error message if something is missing. If needed, just install the missing piece and everything will work after that. library(sf) library(spdep) library(purrr) library(ggplot2) library(geodaData) geodaData All of the data for the R notebooks is available in the geodaData package. We loaded the library earlier, now to access the individual data sets, we use the double colon notation. This works similar to to accessing a variable with $, in that a drop down menu will appear with a list of the datasets included in the package. For this notebook, we use ncovr. us.bound &lt;- geodaData::ncovr Contiguity Weights Contiguity means that two spatial units share a common border of non-zero length. Operationally, we can further distinguish between a rook and a queen criterion of contiguity, in analogy to the moves allowed for the such-named pieces on a chess board. The rook criterion defines neighbors by the existence of a common edge between two spatial units. The queen criterion is somewhat more encompassing and defines neighbors as spatial units sharing a common edge or a common vertex.4 Therefore, the number of neighbors according to the queen criterion will always be at least as large as for the rook criterion. In practice, the construction of the spatial weights from the geometry of the data cannot be done by visual inspection or manual calculation, except in the most trivial of situations. To assess whether two polygons are contiguous requires the use of explicit spatial data structures to deal with the location and arrangement of the polygons. This is implemented through the spatial weights functionality in GeoDa. We will do this with sf and spdep libraries. We will create our neighbors using sf first, as the spdep library doesn’t allow us to create neighbors lists directly as with the older sp library. When we create neighbors in sf we will get a class of sgbp(sparse geometry binary predicate), which is similar to the standard nb class used in spdep, but is not quite compatible. We will have to convert from sgbp to nb, which is not too difficult, as the classes are very similar. It is important to keep in mind that the spatial weights are critically dependent on the quality of the spatial data source (GIS) from which they are constructed. Problems with the topology in the GIS (e.g., slivers) will result in inaccuracies for the neighbor relations included in the spatial weights. In practice, it is essential to check the characteristics of the weights for any evidence of problems. When problems are detected, the solution is to go back to the GIS and fix or clean the topology of the data set. Editing of spatial layers is not implemented in GeoDa, but this is a routine operation in most GIS software. Rook Contiguity We first start with rook contiguity, which are neighbors that are connected by a common side. To create our neighbors list we start by making a function that will do the rook contiguity. To do this we start with the function command and use a, a = b for our inputs. We do this reduce unnecessary typing, as the two parameters are both going to be our sf object. From here we just need st_relate. st_relate computes relationships between pairs of geometries, or matches it to a given pattern. This function also has a parameter for a specified pattern. This pattern will refer to a DE9-IM relationship between x[i] and y[j]. We don’t need to go into detail on this to utilize rook contiguity, but you can check out the basics of DE9-IM at DE9-IM. All we need for rook contiguity is the pattern input \"F***1****\". This gives us the correct DE9-IM relationship for rook contiguity. For more documentation on st_relate check out st_relate documentation st_rook = function(a, b = a) st_relate(a, b, pattern = &quot;F***1****&quot;) sf.sgbp.rook &lt;- st_rook(us.bound) We now check the class of our resulting object from the the st_rook function with the base R function class. It is sgbp, which we will have to convert in order to work with spdep and the plot function later on. class(sf.sgbp.rook) ## [1] &quot;sgbp&quot; &quot;list&quot; Now we will start our conversion from class sgbp to nb. To do this, we need to change the class explicitly, and take the precaution to represent observations with no neighbors with the integer 0. Our data set doesn’t have any observations without neighbors, but in ones with these, it will mess everything up, if not dealt with. We start with the function operator. The input for our function will be an object of class sgbp, denoted by x. We store the attributes in attrs, as we will need to reapply them later. Now we deal we observation with no neighbors. We will use lapply, which applies a function to each element of a list or vector. As for the input function, we make one that checks the length of an element of our list for 0(meaning no neighbors) and returns 0 if the element is empty and the element otherwise. This can be a bit confusing, for more information on lapply, check out lapply documentation From here we will have dealt with observation with no neighbors, but will need to reapply our attributes to the resulting structure from the lapply function. This is done by calling the attributes function and assigning our stored attributes. The final step is to explicitly change the class to nb by using the class function and assigning \"nb\". We then return our object x. as.nb.sgbp &lt;- function(x, ...) { attrs &lt;- attributes(x) x &lt;- lapply(x, function(i) { if(length(i) == 0L) 0L else i } ) attributes(x) &lt;- attrs class(x) &lt;- &quot;nb&quot; x } Now we use the function we created to convert our sgbp object to nb. sf.nb.rook &lt;- as.nb.sgbp(sf.sgbp.rook) Now that we are converted to nb, we can use the summary command to give us more useful information about the neighbors list. summary(sf.nb.rook) ## Neighbour list object: ## Number of regions: 3085 ## Number of nonzero links: 17188 ## Percentage nonzero weights: 0.1805989 ## Average number of links: 5.571475 ## Link number distribution: ## ## 1 2 3 4 5 6 7 8 9 10 11 13 ## 24 41 108 377 863 1007 484 136 37 6 1 1 ## 24 least connected regions: ## 45 49 585 643 837 1197 1377 1402 1442 1464 1470 1523 1531 1532 1543 1596 1605 1653 1737 1767 1775 2892 2893 2919 with 1 link ## 1 most connected region: ## 606 with 13 links We use the class command to check and make sure we have nb. class(sf.nb.rook) ## [1] &quot;nb&quot; We check the length to make sure it corresponds with the GeoDa tutorial example. length(sf.nb.rook) ## [1] 3085 Queen Contiguity We proceed in the same fashion to construct queen contiguity weights. The difference between the rook and queen criterion to determine neighbors is that the latter also includes common vertices. This makes the greatest difference for regular grids (square polygons), where the rook criterion will result in four neighbors (except for edge cases) and the queen criterion will yield eight. For irregular polygons (like most areal units encountered in practice), the differences will be slight. In order to deal with potential inaccuracies in the polygon file (such as rounding errors), using the queen criterion is recommended in practice. Hence it is also the default for contiguity weights. To make our queen contiguity weights, we make a function using st_relate and specifying a DE9-IM pattern. For queen contiguity, our pattern is \"F***T****\". We we don’t really need to go into why the pattern is what it is for our purposes, but DE9-IM and st_relate documentation can help explain this to a degree. st_queen &lt;- function(a, b = a) st_relate(a, b, pattern = &quot;F***T****&quot;) Now we use our st_queen function to get another sgbp neighbor list sf.sgbp.queen &lt;- st_queen(us.bound) To convert to type nb we use as.nb.sgbp, which we created earlier. sf.nb.queen &lt;- as.nb.sgbp(sf.sgbp.queen) Higher Order Contiguity Now we move on to higher order contiguity weights. To make these we will need the spdep package. We will use the nblag and nblag_cumul functions to compute the higher order weights. The nblag function takes a neighbor list of class nb and an order as parameters. It will give us a list of neighbors lists. One for 1st order neighbors and one for second order neighbors. We can select from this data structure by double bracket notation, for instance [[1]] will give the nb object for first order neighbors. Using a 2 instead of a 1 will give us the second order neighbors. second.order.queen &lt;- nblag(sf.nb.queen, 2) We first take a comprehensive look at second.order.queen to see the resulting data structure from nblag. As said earlier, it is a list of 2 nb objects. One for 1st order neighbors and one for 2nd order. It is important to examine this, so we can make use of specific elements of the data structure in our visualizations of the neighbors. str(second.order.queen) ## List of 2 ## $ :List of 3085 ## ..$ : int [1:3] 23 31 41 ## ..$ : int [1:3] 3 4 70 ## ..$ : int [1:4] 2 5 63 70 ## ..$ : int [1:7] 2 28 32 43 56 69 70 ## ..$ : int [1:4] 3 6 29 63 ## ..$ : int [1:3] 5 7 29 ## ..$ : int [1:4] 6 8 29 50 ## ..$ : int [1:9] 7 9 44 50 60 64 71 87 89 ## ..$ : int [1:3] 8 10 44 ## ..$ : int [1:3] 9 11 44 ## ..$ : int [1:4] 10 12 44 47 ## ..$ : int [1:3] 11 24 47 ## ..$ : int [1:4] 14 27 33 36 ## ..$ : int [1:3] 13 15 33 ## ..$ : int [1:5] 14 16 30 33 37 ## ..$ : int [1:4] 15 17 30 34 ## ..$ : int [1:4] 16 18 34 42 ## ..$ : int [1:3] 17 19 42 ## ..$ : int [1:5] 18 20 39 42 46 ## ..$ : int [1:4] 19 21 39 40 ## ..$ : int [1:3] 20 22 40 ## ..$ : int [1:4] 21 23 38 40 ## ..$ : int [1:4] 1 22 38 41 ## ..$ : int [1:4] 12 25 47 79 ## ..$ : int [1:5] 24 26 67 79 88 ## ..$ : int [1:5] 25 27 36 61 67 ## ..$ : int [1:3] 13 26 36 ## ..$ : int [1:2] 4 32 ## ..$ : int [1:7] 5 6 7 50 62 63 66 ## ..$ : int [1:5] 15 16 34 37 76 ## ..$ : int [1:4] 1 35 41 73 ## ..$ : int [1:4] 4 28 43 48 ## ..$ : int [1:7] 13 14 15 36 37 57 59 ## ..$ : int [1:6] 16 17 30 42 74 76 ## ..$ : int [1:5] 31 51 73 121 134 ## ..$ : int [1:7] 13 26 27 33 57 59 61 ## ..$ : int [1:6] 15 30 33 57 76 78 ## ..$ : int [1:7] 22 23 40 41 53 54 55 ## ..$ : int [1:5] 19 20 40 46 52 ## ..$ : int [1:7] 20 21 22 38 39 52 53 ## ..$ : int [1:9] 1 23 31 38 55 65 73 97 100 ## ..$ : int [1:7] 17 18 19 34 46 74 75 ## ..$ : int [1:6] 4 32 48 56 81 90 ## ..$ : int [1:6] 8 9 10 11 47 60 ## ..$ : int 58 ## ..$ : int [1:6] 19 39 42 52 75 77 ## ..$ : int [1:8] 11 12 24 44 60 79 82 99 ## ..$ : int [1:4] 32 43 49 81 ## ..$ : int 48 ## ..$ : int [1:7] 7 8 29 62 64 87 96 ## ..$ : int [1:2] 35 2892 ## ..$ : int [1:7] 39 40 46 53 77 83 84 ## ..$ : int [1:6] 38 40 52 54 84 85 ## ..$ : int [1:8] 38 53 55 65 68 85 94 95 ## ..$ : int [1:5] 38 41 54 65 68 ## ..$ : int [1:4] 4 43 69 90 ## ..$ : int [1:8] 33 36 37 59 78 102 103 104 ## ..$ : int [1:3] 45 86 93 ## ..$ : int [1:6] 33 36 57 61 80 102 ## ..$ : int [1:5] 8 44 47 71 82 ## ..$ : int [1:6] 26 36 59 67 80 114 ## ..$ : int [1:7] 29 50 66 96 101 118 125 ## ..$ : int [1:7] 3 5 29 66 70 101 111 ## ..$ : int [1:3] 8 50 87 ## ..$ : int [1:6] 41 54 55 95 100 115 ## ..$ : int [1:4] 29 62 63 101 ## ..$ : int [1:7] 25 26 61 88 114 126 127 ## ..$ : int [1:2] 54 55 ## ..$ : int [1:8] 4 56 70 90 110 120 139 140 ## ..$ : int [1:7] 2 3 4 63 69 110 111 ## ..$ : int [1:7] 8 60 82 89 119 133 162 ## ..$ : int [1:2] 86 108 ## ..$ : int [1:5] 31 35 41 97 121 ## ..$ : int [1:6] 34 42 75 76 106 107 ## ..$ : int [1:7] 42 46 74 77 91 105 106 ## ..$ : int [1:8] 30 34 37 74 78 92 107 109 ## ..$ : int [1:5] 46 52 75 83 91 ## ..$ : int [1:6] 37 57 76 92 104 122 ## ..$ : int [1:8] 24 25 47 88 99 135 137 138 ## ..$ : int [1:4] 59 61 102 114 ## ..$ : int [1:4] 43 48 90 108 ## ..$ : int [1:5] 47 60 71 99 119 ## ..$ : int [1:6] 52 77 84 91 105 112 ## ..$ : int [1:6] 52 53 83 85 112 113 ## ..$ : int [1:5] 53 54 84 94 113 ## ..$ : int [1:5] 58 72 93 108 117 ## ..$ : int [1:9] 8 50 64 89 96 125 128 145 146 ## ..$ : int [1:5] 25 67 79 127 137 ## ..$ : int [1:6] 8 71 87 128 162 180 ## ..$ : int [1:6] 43 56 69 81 108 120 ## ..$ : int [1:4] 75 77 83 105 ## ..$ : int [1:5] 76 78 109 122 123 ## ..$ : int [1:5] 58 86 117 131 132 ## ..$ : int [1:6] 54 85 95 113 115 116 ## ..$ : int [1:4] 54 65 94 115 ## ..$ : int [1:4] 50 62 87 125 ## ..$ : int [1:8] 41 73 100 121 129 130 173 174 ## ..$ : int [1:4] 160 161 169 229 ## ..$ : int [1:5] 47 79 82 119 138 ## .. [list output truncated] ## ..- attr(*, &quot;predicate&quot;)= chr &quot;relate_pattern&quot; ## ..- attr(*, &quot;region.id&quot;)= chr [1:3085] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## ..- attr(*, &quot;remove_self&quot;)= logi FALSE ## ..- attr(*, &quot;retain_unique&quot;)= logi FALSE ## ..- attr(*, &quot;ncol&quot;)= int 3085 ## ..- attr(*, &quot;class&quot;)= chr &quot;nb&quot; ## ..- attr(*, &quot;sym&quot;)= logi TRUE ## $ :List of 3085 ## ..$ : int [1:8] 22 35 38 55 65 73 97 100 ## ..$ : int [1:9] 5 28 32 43 56 63 69 110 111 ## ..$ : int [1:8] 4 6 29 66 69 101 110 111 ## ..$ : int [1:10] 3 48 63 81 90 110 111 120 139 140 ## ..$ : int [1:8] 2 7 50 62 66 70 101 111 ## ..$ : int [1:6] 3 8 50 62 63 66 ## ..$ : int [1:12] 5 9 44 60 62 63 64 66 71 87 ... ## ..$ : int [1:16] 6 10 11 29 47 62 82 96 119 125 ... ## ..$ : int [1:9] 7 11 47 50 60 64 71 87 89 ## ..$ : int [1:4] 8 12 47 60 ## ..$ : int [1:7] 8 9 24 60 79 82 99 ## ..$ : int [1:7] 10 25 44 60 79 82 99 ## ..$ : int [1:6] 15 26 37 57 59 61 ## ..$ : int [1:7] 16 27 30 36 37 57 59 ## ..$ : int [1:8] 13 17 34 36 57 59 76 78 ## ..$ : int [1:7] 14 18 33 37 42 74 76 ## ..$ : int [1:7] 15 19 30 46 74 75 76 ## ..$ : int [1:7] 16 20 34 39 46 74 75 ## ..$ : int [1:8] 17 21 34 40 52 74 75 77 ## ..$ : int [1:7] 18 22 38 42 46 52 53 ## ..$ : int [1:6] 19 23 38 39 52 53 ## ..$ : int [1:8] 1 20 39 41 52 53 54 55 ## ..$ : int [1:10] 21 31 40 53 54 55 65 73 97 100 ## ..$ : int [1:11] 11 26 44 60 67 82 88 99 135 137 ... ## ..$ : int [1:12] 12 27 36 47 61 99 114 126 127 135 ... ## ..$ : int [1:11] 13 24 33 57 59 79 80 88 114 126 ... ## ..$ : int [1:7] 14 25 33 57 59 61 67 ## ..$ : int [1:6] 2 43 48 56 69 70 ## ..$ : int [1:10] 3 8 64 70 87 96 101 111 118 125 ## ..$ : int [1:10] 14 17 33 42 57 74 78 92 107 109 ## ..$ : int [1:9] 23 38 51 55 65 97 100 121 134 ## ..$ : int [1:7] 2 49 56 69 70 81 90 ## ..$ : int [1:11] 16 26 27 30 61 76 78 80 102 103 ... ## ..$ : int [1:11] 15 18 19 37 46 75 78 92 106 107 ... ## ..$ : int [1:9] 1 41 97 130 136 168 181 189 2892 ## ..$ : int [1:11] 14 15 25 37 67 78 80 102 103 104 ... ## ..$ : int [1:14] 13 14 16 34 36 59 74 92 102 103 ... ## ..$ : int [1:15] 1 20 21 31 39 52 65 68 73 84 ... ## ..$ : int [1:10] 18 21 22 38 42 53 75 77 83 84 ## ..$ : int [1:10] 19 23 41 46 54 55 77 83 84 85 ## ..$ : int [1:13] 22 35 40 53 54 68 95 115 121 129 ... ## ..$ : int [1:11] 16 20 30 39 52 76 77 91 105 106 ... ## ..$ : int [1:7] 2 28 49 69 70 108 120 ## ..$ : int [1:11] 7 12 24 50 64 71 79 82 87 89 ... ## ..$ : int [1:2] 86 93 ## ..$ : int [1:12] 17 18 20 34 40 53 74 83 84 91 ... ## ..$ : int [1:10] 8 9 10 25 71 88 119 135 137 138 ## ..$ : int [1:5] 4 28 56 90 108 ## ..$ : int [1:3] 32 43 81 ## ..$ : int [1:15] 5 6 9 44 60 63 66 71 89 101 ... ## ..$ : int [1:4] 31 73 121 134 ## ..$ : int [1:13] 19 20 21 22 38 42 54 75 85 91 ... ## ..$ : int [1:16] 20 21 22 23 39 41 46 55 65 68 ... ## ..$ : int [1:10] 22 23 40 41 52 84 100 113 115 116 ## ..$ : int [1:13] 1 22 23 31 40 53 73 85 94 95 ... ## ..$ : int [1:11] 2 28 32 48 70 81 108 110 120 139 ... ## ..$ : int [1:14] 13 14 15 26 27 30 61 76 80 92 ... ## ..$ : int [1:5] 72 108 117 131 132 ## ..$ : int [1:12] 13 14 15 26 27 37 67 78 103 104 ... ## ..$ : int [1:15] 7 9 10 11 12 24 50 64 79 87 ... ## ..$ : int [1:10] 13 25 27 33 57 88 102 126 127 144 ## ..$ : int [1:11] 5 6 7 8 63 64 87 111 145 156 ... ## ..$ : int [1:14] 2 4 6 7 50 62 69 110 118 139 ... ## ..$ : int [1:13] 7 9 29 44 60 62 71 89 96 125 ... ## ..$ : int [1:13] 1 23 31 38 53 68 73 85 94 97 ... ## ..$ : int [1:10] 3 5 6 7 50 70 96 111 118 125 ## ..$ : int [1:14] 24 27 36 59 79 80 102 137 144 163 ... ## ..$ : int [1:7] 38 41 53 65 85 94 95 ## ..$ : int [1:16] 2 3 28 32 43 63 81 108 111 132 ... ## ..$ : int [1:17] 5 28 29 32 43 56 66 90 101 118 ... ## ..$ : int [1:15] 7 9 44 47 50 64 87 99 128 138 ... ## ..$ : int [1:7] 58 81 90 93 117 120 132 ## ..$ : int [1:15] 1 23 38 51 55 65 100 129 130 134 ... ## ..$ : int [1:16] 16 17 18 19 30 37 46 77 78 91 ... ## ..$ : int [1:13] 17 18 19 34 39 52 76 83 107 112 ... ## ..$ : int [1:12] 15 16 17 33 42 57 75 104 106 122 ... ## ..$ : int [1:10] 19 39 40 42 53 74 84 105 106 112 ## ..$ : int [1:15] 15 30 33 34 36 59 74 102 103 107 ... ## ..$ : int [1:12] 11 12 26 44 60 67 82 119 127 163 ... ## ..$ : int [1:8] 26 33 36 57 67 103 126 144 ## ..$ : int [1:10] 4 32 49 56 69 72 86 117 120 132 ## ..$ : int [1:12] 8 11 12 24 44 79 89 133 138 162 ... ## ..$ : int [1:11] 39 40 46 53 75 85 106 113 148 150 ... ## ..$ : int [1:13] 38 39 40 46 54 77 91 94 105 116 ... ## ..$ : int [1:13] 38 40 52 55 65 68 83 95 112 115 ... ## ..$ : int [1:6] 45 81 90 120 131 132 ## ..$ : int [1:17] 7 9 29 44 60 62 71 118 156 162 ... ## ..$ : int [1:13] 24 26 47 61 99 114 126 135 138 163 ... ## ..$ : int [1:17] 7 9 44 50 60 64 82 96 119 125 ... ## ..$ : int [1:13] 4 32 48 70 72 86 110 117 132 139 ... ## ..$ : int [1:9] 42 46 52 74 84 106 112 148 150 ## ..$ : int [1:12] 30 34 37 57 74 104 107 141 149 151 ... ## ..$ : int [1:7] 45 72 108 120 170 171 172 ## ..$ : int [1:13] 38 53 55 65 68 84 100 112 129 142 ... ## ..$ : int [1:11] 38 41 53 55 68 85 100 113 116 129 ... ## ..$ : int [1:13] 7 8 29 64 66 89 101 118 128 145 ... ## ..$ : int [1:16] 1 23 31 35 38 55 65 115 134 142 ... ## ..$ : int [1:4] 230 307 312 343 ## ..$ : int [1:13] 11 12 24 25 44 60 71 88 133 135 ... ## .. [list output truncated] ## ..- attr(*, &quot;class&quot;)= chr &quot;nb&quot; ## ..- attr(*, &quot;region.id&quot;)= chr [1:3085] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## ..- attr(*, &quot;sym&quot;)= logi TRUE ## - attr(*, &quot;call&quot;)= language nblag(neighbours = sf.nb.queen, maxlag = 2) Now we can look at, specifically, the 2nd order neighbors by using the double bracket selection. str(second.order.queen[[2]]) ## List of 3085 ## $ : int [1:8] 22 35 38 55 65 73 97 100 ## $ : int [1:9] 5 28 32 43 56 63 69 110 111 ## $ : int [1:8] 4 6 29 66 69 101 110 111 ## $ : int [1:10] 3 48 63 81 90 110 111 120 139 140 ## $ : int [1:8] 2 7 50 62 66 70 101 111 ## $ : int [1:6] 3 8 50 62 63 66 ## $ : int [1:12] 5 9 44 60 62 63 64 66 71 87 ... ## $ : int [1:16] 6 10 11 29 47 62 82 96 119 125 ... ## $ : int [1:9] 7 11 47 50 60 64 71 87 89 ## $ : int [1:4] 8 12 47 60 ## $ : int [1:7] 8 9 24 60 79 82 99 ## $ : int [1:7] 10 25 44 60 79 82 99 ## $ : int [1:6] 15 26 37 57 59 61 ## $ : int [1:7] 16 27 30 36 37 57 59 ## $ : int [1:8] 13 17 34 36 57 59 76 78 ## $ : int [1:7] 14 18 33 37 42 74 76 ## $ : int [1:7] 15 19 30 46 74 75 76 ## $ : int [1:7] 16 20 34 39 46 74 75 ## $ : int [1:8] 17 21 34 40 52 74 75 77 ## $ : int [1:7] 18 22 38 42 46 52 53 ## $ : int [1:6] 19 23 38 39 52 53 ## $ : int [1:8] 1 20 39 41 52 53 54 55 ## $ : int [1:10] 21 31 40 53 54 55 65 73 97 100 ## $ : int [1:11] 11 26 44 60 67 82 88 99 135 137 ... ## $ : int [1:12] 12 27 36 47 61 99 114 126 127 135 ... ## $ : int [1:11] 13 24 33 57 59 79 80 88 114 126 ... ## $ : int [1:7] 14 25 33 57 59 61 67 ## $ : int [1:6] 2 43 48 56 69 70 ## $ : int [1:10] 3 8 64 70 87 96 101 111 118 125 ## $ : int [1:10] 14 17 33 42 57 74 78 92 107 109 ## $ : int [1:9] 23 38 51 55 65 97 100 121 134 ## $ : int [1:7] 2 49 56 69 70 81 90 ## $ : int [1:11] 16 26 27 30 61 76 78 80 102 103 ... ## $ : int [1:11] 15 18 19 37 46 75 78 92 106 107 ... ## $ : int [1:9] 1 41 97 130 136 168 181 189 2892 ## $ : int [1:11] 14 15 25 37 67 78 80 102 103 104 ... ## $ : int [1:14] 13 14 16 34 36 59 74 92 102 103 ... ## $ : int [1:15] 1 20 21 31 39 52 65 68 73 84 ... ## $ : int [1:10] 18 21 22 38 42 53 75 77 83 84 ## $ : int [1:10] 19 23 41 46 54 55 77 83 84 85 ## $ : int [1:13] 22 35 40 53 54 68 95 115 121 129 ... ## $ : int [1:11] 16 20 30 39 52 76 77 91 105 106 ... ## $ : int [1:7] 2 28 49 69 70 108 120 ## $ : int [1:11] 7 12 24 50 64 71 79 82 87 89 ... ## $ : int [1:2] 86 93 ## $ : int [1:12] 17 18 20 34 40 53 74 83 84 91 ... ## $ : int [1:10] 8 9 10 25 71 88 119 135 137 138 ## $ : int [1:5] 4 28 56 90 108 ## $ : int [1:3] 32 43 81 ## $ : int [1:15] 5 6 9 44 60 63 66 71 89 101 ... ## $ : int [1:4] 31 73 121 134 ## $ : int [1:13] 19 20 21 22 38 42 54 75 85 91 ... ## $ : int [1:16] 20 21 22 23 39 41 46 55 65 68 ... ## $ : int [1:10] 22 23 40 41 52 84 100 113 115 116 ## $ : int [1:13] 1 22 23 31 40 53 73 85 94 95 ... ## $ : int [1:11] 2 28 32 48 70 81 108 110 120 139 ... ## $ : int [1:14] 13 14 15 26 27 30 61 76 80 92 ... ## $ : int [1:5] 72 108 117 131 132 ## $ : int [1:12] 13 14 15 26 27 37 67 78 103 104 ... ## $ : int [1:15] 7 9 10 11 12 24 50 64 79 87 ... ## $ : int [1:10] 13 25 27 33 57 88 102 126 127 144 ## $ : int [1:11] 5 6 7 8 63 64 87 111 145 156 ... ## $ : int [1:14] 2 4 6 7 50 62 69 110 118 139 ... ## $ : int [1:13] 7 9 29 44 60 62 71 89 96 125 ... ## $ : int [1:13] 1 23 31 38 53 68 73 85 94 97 ... ## $ : int [1:10] 3 5 6 7 50 70 96 111 118 125 ## $ : int [1:14] 24 27 36 59 79 80 102 137 144 163 ... ## $ : int [1:7] 38 41 53 65 85 94 95 ## $ : int [1:16] 2 3 28 32 43 63 81 108 111 132 ... ## $ : int [1:17] 5 28 29 32 43 56 66 90 101 118 ... ## $ : int [1:15] 7 9 44 47 50 64 87 99 128 138 ... ## $ : int [1:7] 58 81 90 93 117 120 132 ## $ : int [1:15] 1 23 38 51 55 65 100 129 130 134 ... ## $ : int [1:16] 16 17 18 19 30 37 46 77 78 91 ... ## $ : int [1:13] 17 18 19 34 39 52 76 83 107 112 ... ## $ : int [1:12] 15 16 17 33 42 57 75 104 106 122 ... ## $ : int [1:10] 19 39 40 42 53 74 84 105 106 112 ## $ : int [1:15] 15 30 33 34 36 59 74 102 103 107 ... ## $ : int [1:12] 11 12 26 44 60 67 82 119 127 163 ... ## $ : int [1:8] 26 33 36 57 67 103 126 144 ## $ : int [1:10] 4 32 49 56 69 72 86 117 120 132 ## $ : int [1:12] 8 11 12 24 44 79 89 133 138 162 ... ## $ : int [1:11] 39 40 46 53 75 85 106 113 148 150 ... ## $ : int [1:13] 38 39 40 46 54 77 91 94 105 116 ... ## $ : int [1:13] 38 40 52 55 65 68 83 95 112 115 ... ## $ : int [1:6] 45 81 90 120 131 132 ## $ : int [1:17] 7 9 29 44 60 62 71 118 156 162 ... ## $ : int [1:13] 24 26 47 61 99 114 126 135 138 163 ... ## $ : int [1:17] 7 9 44 50 60 64 82 96 119 125 ... ## $ : int [1:13] 4 32 48 70 72 86 110 117 132 139 ... ## $ : int [1:9] 42 46 52 74 84 106 112 148 150 ## $ : int [1:12] 30 34 37 57 74 104 107 141 149 151 ... ## $ : int [1:7] 45 72 108 120 170 171 172 ## $ : int [1:13] 38 53 55 65 68 84 100 112 129 142 ... ## $ : int [1:11] 38 41 53 55 68 85 100 113 116 129 ... ## $ : int [1:13] 7 8 29 64 66 89 101 118 128 145 ... ## $ : int [1:16] 1 23 31 35 38 55 65 115 134 142 ... ## $ : int [1:4] 230 307 312 343 ## $ : int [1:13] 11 12 24 25 44 60 71 88 133 135 ... ## [list output truncated] ## - attr(*, &quot;class&quot;)= chr &quot;nb&quot; ## - attr(*, &quot;region.id&quot;)= chr [1:3085] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## - attr(*, &quot;sym&quot;)= logi TRUE We store the second order neighbors in sf.nb.queen2 to make things simpler for our visuals. sf.nb.queen2 &lt;- second.order.queen[[2]] Now if we want to include both 1st and 2nd order neighbors in our visualizations, we need the nblag_cumul function. This will give us an nb object with both 1st and 2nd order neighbors, instead of separating them as in the result of nblag. To use this function, we need the resulting object from the nblag function. nblag_cumul combines the two separate nb objects from the nblag function into one. second.order.queen.cumul &lt;- nblag_cumul(second.order.queen) Here we take another in depth look at result. We can see that, we now have one nb object with both 1st and 2nd order neighbors. str(second.order.queen.cumul) ## List of 3085 ## $ : int [1:11] 22 23 31 35 38 41 55 65 73 97 ... ## $ : int [1:12] 3 4 5 28 32 43 56 63 69 70 ... ## $ : int [1:12] 2 4 5 6 29 63 66 69 70 101 ... ## $ : int [1:17] 2 3 28 32 43 48 56 63 69 70 ... ## $ : int [1:12] 2 3 6 7 29 50 62 63 66 70 ... ## $ : int [1:9] 3 5 7 8 29 50 62 63 66 ## $ : int [1:16] 5 6 8 9 29 44 50 60 62 63 ... ## $ : int [1:25] 6 7 9 10 11 29 44 47 50 60 ... ## $ : int [1:12] 7 8 10 11 44 47 50 60 64 71 ... ## $ : int [1:7] 8 9 11 12 44 47 60 ## $ : int [1:11] 8 9 10 12 24 44 47 60 79 82 ... ## $ : int [1:10] 10 11 24 25 44 47 60 79 82 99 ## $ : int [1:10] 14 15 26 27 33 36 37 57 59 61 ## $ : int [1:10] 13 15 16 27 30 33 36 37 57 59 ## $ : int [1:13] 13 14 16 17 30 33 34 36 37 57 ... ## $ : int [1:11] 14 15 17 18 30 33 34 37 42 74 ... ## $ : int [1:11] 15 16 18 19 30 34 42 46 74 75 ... ## $ : int [1:10] 16 17 19 20 34 39 42 46 74 75 ## $ : int [1:13] 17 18 20 21 34 39 40 42 46 52 ... ## $ : int [1:11] 18 19 21 22 38 39 40 42 46 52 ... ## $ : int [1:9] 19 20 22 23 38 39 40 52 53 ## $ : int [1:12] 1 20 21 23 38 39 40 41 52 53 ... ## $ : int [1:14] 1 21 22 31 38 40 41 53 54 55 ... ## $ : int [1:15] 11 12 25 26 44 47 60 67 79 82 ... ## $ : int [1:17] 12 24 26 27 36 47 61 67 79 88 ... ## $ : int [1:16] 13 24 25 27 33 36 57 59 61 67 ... ## $ : int [1:10] 13 14 25 26 33 36 57 59 61 67 ## $ : int [1:8] 2 4 32 43 48 56 69 70 ## $ : int [1:17] 3 5 6 7 8 50 62 63 64 66 ... ## $ : int [1:15] 14 15 16 17 33 34 37 42 57 74 ... ## $ : int [1:13] 1 23 35 38 41 51 55 65 73 97 ... ## $ : int [1:11] 2 4 28 43 48 49 56 69 70 81 ... ## $ : int [1:18] 13 14 15 16 26 27 30 36 37 57 ... ## $ : int [1:17] 15 16 17 18 19 30 37 42 46 74 ... ## $ : int [1:14] 1 31 41 51 73 97 121 130 134 136 ... ## $ : int [1:18] 13 14 15 25 26 27 33 37 57 59 ... ## $ : int [1:20] 13 14 15 16 30 33 34 36 57 59 ... ## $ : int [1:22] 1 20 21 22 23 31 39 40 41 52 ... ## $ : int [1:15] 18 19 20 21 22 38 40 42 46 52 ... ## $ : int [1:17] 19 20 21 22 23 38 39 41 46 52 ... ## $ : int [1:22] 1 22 23 31 35 38 40 53 54 55 ... ## $ : int [1:18] 16 17 18 19 20 30 34 39 46 52 ... ## $ : int [1:13] 2 4 28 32 48 49 56 69 70 81 ... ## $ : int [1:17] 7 8 9 10 11 12 24 47 50 60 ... ## $ : int [1:3] 58 86 93 ## $ : int [1:18] 17 18 19 20 34 39 40 42 52 53 ... ## $ : int [1:18] 8 9 10 11 12 24 25 44 60 71 ... ## $ : int [1:9] 4 28 32 43 49 56 81 90 108 ## $ : int [1:4] 32 43 48 81 ## $ : int [1:22] 5 6 7 8 9 29 44 60 62 63 ... ## $ : int [1:6] 31 35 73 121 134 2892 ## $ : int [1:20] 19 20 21 22 38 39 40 42 46 53 ... ## $ : int [1:22] 20 21 22 23 38 39 40 41 46 52 ... ## $ : int [1:18] 22 23 38 40 41 52 53 55 65 68 ... ## $ : int [1:18] 1 22 23 31 38 40 41 53 54 65 ... ## $ : int [1:15] 2 4 28 32 43 48 69 70 81 90 ... ## $ : int [1:22] 13 14 15 26 27 30 33 36 37 59 ... ## $ : int [1:8] 45 72 86 93 108 117 131 132 ## $ : int [1:18] 13 14 15 26 27 33 36 37 57 61 ... ## $ : int [1:20] 7 8 9 10 11 12 24 44 47 50 ... ## $ : int [1:16] 13 25 26 27 33 36 57 59 67 80 ... ## $ : int [1:18] 5 6 7 8 29 50 63 64 66 87 ... ## $ : int [1:21] 2 3 4 5 6 7 29 50 62 66 ... ## $ : int [1:16] 7 8 9 29 44 50 60 62 71 87 ... ## $ : int [1:19] 1 23 31 38 41 53 54 55 68 73 ... ## $ : int [1:14] 3 5 6 7 29 50 62 63 70 96 ... ## $ : int [1:21] 24 25 26 27 36 59 61 79 80 88 ... ## $ : int [1:9] 38 41 53 54 55 65 85 94 95 ## $ : int [1:24] 2 3 4 28 32 43 56 63 70 81 ... ## $ : int [1:24] 2 3 4 5 28 29 32 43 56 63 ... ## $ : int [1:22] 7 8 9 44 47 50 60 64 82 87 ... ## $ : int [1:9] 58 81 86 90 93 108 117 120 132 ## $ : int [1:20] 1 23 31 35 38 41 51 55 65 97 ... ## $ : int [1:22] 16 17 18 19 30 34 37 42 46 75 ... ## $ : int [1:20] 17 18 19 34 39 42 46 52 74 76 ... ## $ : int [1:20] 15 16 17 30 33 34 37 42 57 74 ... ## $ : int [1:15] 19 39 40 42 46 52 53 74 75 83 ... ## $ : int [1:21] 15 30 33 34 36 37 57 59 74 76 ... ## $ : int [1:20] 11 12 24 25 26 44 47 60 67 82 ... ## $ : int [1:12] 26 33 36 57 59 61 67 102 103 114 ... ## $ : int [1:14] 4 32 43 48 49 56 69 72 86 90 ... ## $ : int [1:17] 8 11 12 24 44 47 60 71 79 89 ... ## $ : int [1:17] 39 40 46 52 53 75 77 84 85 91 ... ## $ : int [1:19] 38 39 40 46 52 53 54 77 83 85 ... ## $ : int [1:18] 38 40 52 53 54 55 65 68 83 84 ... ## $ : int [1:11] 45 58 72 81 90 93 108 117 120 131 ... ## $ : int [1:26] 7 8 9 29 44 50 60 62 64 71 ... ## $ : int [1:18] 24 25 26 47 61 67 79 99 114 126 ... ## $ : int [1:23] 7 8 9 44 50 60 64 71 82 87 ... ## $ : int [1:19] 4 32 43 48 56 69 70 72 81 86 ... ## $ : int [1:13] 42 46 52 74 75 77 83 84 105 106 ... ## $ : int [1:17] 30 34 37 57 74 76 78 104 107 109 ... ## $ : int [1:12] 45 58 72 86 108 117 120 131 132 170 ... ## $ : int [1:19] 38 53 54 55 65 68 84 85 95 100 ... ## $ : int [1:15] 38 41 53 54 55 65 68 85 94 100 ... ## $ : int [1:17] 7 8 29 50 62 64 66 87 89 101 ... ## $ : int [1:24] 1 23 31 35 38 41 55 65 73 100 ... ## $ : int [1:8] 160 161 169 229 230 307 312 343 ## $ : int [1:18] 11 12 24 25 44 47 60 71 79 82 ... ## [list output truncated] ## - attr(*, &quot;region.id&quot;)= chr [1:3085] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## - attr(*, &quot;call&quot;)= language nblag_cumul(nblags = second.order.queen) ## - attr(*, &quot;class&quot;)= chr &quot;nb&quot; Visualizing Contiguity Neighbors Connectivity Graph A connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids We will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid. We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation To get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid. longitude &lt;- map_dbl(us.bound$geometry, ~st_centroid(.x)[[1]]) We do the same for latitude with one key difference. We access the second value per each centroid with [[2]]. latitude &lt;- map_dbl(us.bound$geometry, ~st_centroid(.x)[[2]]) Now that we have latitude and longitude, we use cbind to put longitude and latitude into the same object. coords &lt;- cbind(longitude, latitude) We check the first few observations to see if things are formatted correctly. head(coords) ## longitude latitude ## [1,] -94.90337 48.77173 ## [2,] -118.51718 48.46959 ## [3,] -117.85532 48.39591 ## [4,] -119.73944 48.54843 ## [5,] -117.27400 48.53280 ## [6,] -116.47029 48.76502 Rook Connectivity Graphs Now that we have coordinates for each of our observations, we can plot the neighbors through a connectivity graph. We just need the basic plot function and to enter our neighbor list as the first parameter, then our coordinates as the second. We add extra customizations, so we can actually see what is going on in the graph. We use lwd = .2 to make the line length less thick, we set the col =\"blue\", and finally cex = .5 to make the point symbols smaller. The default options make the graph near indecipherable, so it is important to add these extra parameters. plot(sf.nb.rook, coords, lwd=.2, col=&quot;blue&quot;, cex = .5) Queen Connectivity Graphs Now we will do the same for queen contiguity. The queen variation will have more lines, as there are more neighbors due to shared vertices counting in the neighbors relationship. It is hard to spot the differences between the two plots, as they are both visually busy First Order The same form is followed for queen contiguity as we are working with the same class: a nb object and a corresponding list of coordinates. plot(sf.nb.queen, coords, lwd=.2, col=&quot;blue&quot;, cex = .5) Second Order Here we take a look at a connectivity graph for second order neighbors. All we need is the second order nb object that we created earlier and our coordinates. This graph will look significantly different from the ones we’ve made so far, as second order tends to have more neighbors than first order. plot(sf.nb.queen2, coords, lwd=.2, col=&quot;blue&quot;, cex = .5) Cumulative We will take a look a plot with both second and first order neighbors here. It will be a very busy graph. plot(second.order.queen.cumul, coords, lwd=.2, col=&quot;blue&quot;, cex = .5) Connectivity Histogram A connectivity histogram shows the number of observations for each cardinality of neighbors(how many observations have the same number of neighbors). We first have to get the cardinality before we can make the histograms. We can do this by using the card function from the spdep package. This will give us a vector of 3085 observations with each one containing the corresponding number of neighbors from the input neighbors list. rook.card &lt;- card(sf.nb.rook) We take a look at the first few observation with the head function and as we can see, it is a vector with number of neighbors for each observation of the nb object. head(rook.card) ## [1] 3 3 4 7 4 3 Now we check the length and it is what we expected. length(rook.card) ## [1] 3085 We can visualize the cardinality through a ggplot2 histogram. For this we just need to call the ggplot function and add geom_histogram as a layer. We specify the aes() in geom_histogram to be our rook cardinality. This will give us a basic plot, which we will customize further a little later. ggplot() + geom_histogram(aes(x=rook.card)) Here we will add a few customizations to make the plot nicer. The most important is with the breaks = parameter, as the bars are spaced out kind of weirdly in our opening plot. We can avoid manually typing in the breaks by using the seq function, which will give us a list of numbers 0 to 13, incrementing by 1 each time. We specify the range in the first two arguments and then use by = to pick the number to increment by. In our case, 1 makes the most sense as you cannot have half of a neighbor. ggplot() + geom_histogram(aes(x=rook.card), breaks = seq(0,13, by = 1)) + xlab(&quot;number of neighbors&quot;) With the summary command we can get a look at the summary statistics for the cardinality of the neighbors list. summary(rook.card) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 5.000 6.000 5.571 6.000 13.000 Saving Neighbors To save our neighbors list, we use the write.nb.gal function from the spdep package. The file format is a GAL Lattice file. We input our neighbors list, and the the filename second. We have two options from this point. We can save the file with the old style or the new GeoDa header style. Oldstyle The oldstyle just saves the neighbor list with then number of observations at the top of the file. We can save in this format by setting oldstyle = to TRUE. write.nb.gal(sf.nb.rook, &quot;rook_contiguity_old.gal&quot;, oldstyle = TRUE) GeoDA Header Format The new GeoDA header style, also takes the shapefile name taken from GAL file for the dataset and the region id indicator variable name. It puts this information in a header for the file. All we have to do is set oldstyle = to FALSE and enter names for shpfile and ind parameters. write.nb.gal(sf.nb.rook, &quot;rook_contiguity_new.gal&quot;, oldstyle = FALSE, shpfile =&quot;NAT.shp&quot;, ind =&quot;region id&quot;) Use setwd(directorypath) to specify the working directory.↩︎ Use install.packages(packagename).↩︎ "],["distance-based-spatial-weights.html", "Chapter 7 Distance-Based Spatial Weights Introduction Preliminaries Visualizing point data Distance-Band Weights K-Nearest Neighbor Weights Generalizing the Concept of Contiguity Distance-based weights for polygons", " Chapter 7 Distance-Based Spatial Weights Introduction This notebook cover the functionality of the Distance-Based Spatial Weights section of the GeoDa workbook. We refer to that document for details on the methodology, references, etc. The goal of these notes is to approximate as closely as possible the operations carried out using GeoDa by means of a range of R packages. The notes are written with R beginners in mind, more seasoned R users can probably skip most of the comments on data structures and other R particulars. Also, as always in R, there are typically several ways to achieve a specific objective, so what is shown here is just one way that works, but there often are others (that may even be more elegant, work faster, or scale better). For this notebook, we use Cleveland homesale point data. Our goal in this lab is show how to implement distance-band spatial weights Objectives After completing the notebook, you should know how to carry out the following tasks: Construct distance band spatial weights Assess the characteristics of distance-based weights Assess the effect of the max-min distance cut-off Identify isolates Construct k-nearest neighbor spatial weights Create Thiessen polygons from a point layer Construct contiguity weights for points and distance weights for polygons Understand the use of great circle distance R Packages used tmap: To plot our points on a base map sf: Used to read the shapefiles in, make contiguity weights, and convert from sp, spdep: Used to create distance neighbors and contiguity neighbors ggplot2: To make connectivity histograms deldir: To make Thiessen polygons. sp: Used to get an intermediate data structure to get the Thiessen polygons to sf class _ purr: Used for a mapping function geodaData: To get the data for this notebook R Commands used Below follows a list of the commands used in this notebook. For further details and a comprehensive list of options, please consult the R documentation. Base R: install.packages, library, setwd, head, str, summary, class, cbind, unlist, max, attributes, class, list, lapply, rbind, sapply, length, seq, as.character, vector, data.frame tmap: tm_shape, tmap_mode, tm_dots sf: st_read, plot, st_as_sf, st_relate spdep: knn2nb, knearneigh, dnearneigh, nbdists, card ggplot2: ggplot, geom_histogram, xlab deldir: deldir, tile_list sp: Polygon, Polygons, SpatialPolygons, SpatialPolygonsDataFrame _ purr: map_dbl Preliminaries Before starting, make sure to have the latest version of R and of packages that are compiled for the matching version of R (this document was created using R 3.5.1 of 2018-07-02). Also, optionally, set a working directory, even though we will not actually be saving any files.21 Load packages First, we load all the required packages using the library command. If you don’t have some of these in your system, make sure to install them first as well as their dependencies.22 You will get an error message if something is missing. If needed, just install the missing piece and everything will work after that. library(tmap) library(sf) library(spdep) library(ggplot2) library(deldir) library(sp) library(purrr) library(geodaData) geodaData All of the data for the R notebooks is available in the geodaData package. We loaded the library earlier, now to access the individual data sets, we use the double colon notation. This works similar to to accessing a variable with $, in that a drop down menu will appear with a list of the datasets included in the package. For this notebook, we use ncovr. clev.points &lt;- geodaData::clev_pts plot(clev.points) Visualizing point data To get a cursory look at the point data with a basemap, we will use tmap. To get this done we first have to switch from mode plot to mode view with the function tmap_mode. From there we use tm_shape and tm_dots to display our points on the basemap. The only argument we need to pass is the shapefile into the tm_shape function. tmap_mode(&quot;view&quot;) tm_shape(clev.points) + tm_dots() Now we switch back to plot mode for the rest of the notebook with tmap_mode(\"plot\"). tmap_mode(&quot;plot&quot;) Distance-Band Weights Concepts Distance Metric The core input into the determination of a neighbor relation for distance-based spatial weights is a formal measure of distance, or a distance metric. The most familiar special case is the Euclidean or straight line distance, \\(d_{ij}\\), as the crow flies: \\[ d_{ij} = \\sqrt{(x_i-x_j)^2 + (y_i - y_j)^2}\\] for two points i and j with respective coordinates \\((x_i,y_i)\\) and \\((x_j,y_j)\\). Great Circle distance Euclidean inter-point distances are only meaningful when the coordinates are recorded on a plane, i.e., for projected points. In practice, one often works with unprojected points, expressed as degrees of latitude and longitude, in which case using a straight line distance measure is inappropriate, since it ignores the curvature of the earth. This is especially the case for longer distances, such as from the East Coast to the West Coast in the U.S. The proper distance measure in this case is the so-called arc distance or great circle distance. This takes the latitude and longitude in decimal degrees as input into a conversion formula.3 Decimal degrees are obtained from the degree-minute-second value as degrees + minutes/60 + seconds/3600. The latitude and longitude in decimal degrees are converted into radians as: \\[Lat_r = (Lat_d - 90) * \\pi/180\\] \\[Lon_r = Lon_d * \\pi/180\\] where the subscripts d and r refer respectively to decimal degrees and radians, and π=3.14159… With \\(\\Delta Lon = Lon_r(j)- Lon_r(i)\\), the expression for the arc distance is: \\[d_{ij} = R * arccos[cos(\\Delta Lon) * sin(Lat_{r(i)}) * sin(Lat_{r(j)}) + cos(Lat_{r(i)}) * cos(Lat_{r(j)})]\\] or equivalently: \\[d_{ij} = R * arccos[cos(\\Delta Lon) * cos(Lat_{r(i)}) * cos(Lat_{r(j)}) + sin(Lat_{r(i)}) * sin(Lat_{r(j)})]\\] where R is the radius of the earth. In GeoDa, the arc distance is obtained in miles with R = 3959, and in kilometers with R = 6371. These calculated distance values are only approximate, since the radius of the earth is taken at the equator. A more precise measure would take into account the actual latitude at which the distance is measured. In addition, the earth’s shape is much more complex than a sphere, but the approximation serves our purposes. Distance-band weights The most straightforward spatial weights matrix constructed from a distance measure is obtained when i and j are considered neighbors whenever j falls within a critical distance band from i. More precisely, \\(w_{ij}=1\\) when $d_{ij} $ , and \\(w_{ij}=0\\) otherwise, where \\(\\delta\\) is a preset critical distance cutoff. In order to avoid isolates (islands) that would result from too stringent a critical distance, the distance must be chosen such that each location has at least one neighbor. Such a distance conforms to a max-min criterion, i.e., it is the largest of the nearest neighbor distances. In practice, the max-min criterion often leads to too many neighbors for locations that are somewhat clustered, since the critical distance is determined by the points that are furthest apart. This problem frequently occurs when the density of the points is uneven across the data set, such as when some of the points are clustered and others more spread out. We revisit this problem in the illustrations below. Creating distance-band weights In order to start the distance based neighbors, we first need to compute a threshold value(The minimum distance that gives each point at least one neighbor). We can find this using k-nearest neighbors, which will be covered in more depth later in the notebook. We find the list of k-nearest neighbors for k = 1 then find the max distance between two neighbors in this list, which we then use as the upper distance parameter in the dnearneigh. Computing Critical Threshold Before we move forward, we need to x and y coordinates in a matrix. This is easily done by using cbind on the x and y coordinate columns of clev.points. cbind puts together vectors as columns in a matrix, so it is perfect for this task. coords &lt;- cbind(clev.points$x,clev.points$y) To find the critical threshold, we first find the k-nearest neighbors for k = 1. This will give us a list, where each point has exactly one neighbor. To do this, we use the ’knearneigh` function from spdep library. This will give a class of knn, which is similar to class nb, but will need to be converted anyways. knn1 &lt;- knearneigh(coords) Here we take a comprehensive look at our resulting knn object with the str command. str(knn1) ## List of 5 ## $ nn : int [1:205, 1] 2 1 5 5 4 5 8 7 10 9 ... ## $ np : int 205 ## $ k : num 1 ## $ dimension: int 2 ## $ x : num [1:205, 1:2] 2177340 2177090 2182100 2181090 2181090 ... ## - attr(*, &quot;class&quot;)= chr &quot;knn&quot; ## - attr(*, &quot;call&quot;)= language knearneigh(x = coords) Now we convert to nb. This simple, as there is a built in function for it in the spdep library: knn2nb. k1 &lt;- knn2nb(knn1) Computing the critical threshold will require a few functions, now that we have a neighbors list. First step is to get the distances between each point and it’s closest neighbor. This can be done with the nbdists. With these distances, we just need to find the maximum. For this we use the max command. However, we cannot do this with lists, so we must first get a data type that works for the max command, in our case, we use unlist critical.threshold &lt;- max(unlist(nbdists(k1,coords))) critical.threshold ## [1] 3598.055 Computing distance-band weights With our critical threshold, we have a baseline value to work with for our distance-band neighbors. To make this neighbor’s list we use dnearneigh from the spdep package. The parameters necessary are the coordinates, the lower distance bound, and the upper distance bound. We enter these in the the above order. Another important parameter is the longlat. This is used for point data in longitude and latitude form. It is necessary to use this to get great circle distance instead of euclidean for accuracy purposes. nb.dist.band &lt;- dnearneigh(coords, 0, critical.threshold) Weights characteristics We can examine the characteristics of these weights through a connectivity graph, connectivity histogram, and summary statistics. Weights summary We can use the base R summary command to get a comprehensive look at our distance-band weights object. This will give us a lot of useful information, that we cannot get with the str command. summary(nb.dist.band) ## Neighbour list object: ## Number of regions: 205 ## Number of nonzero links: 2592 ## Percentage nonzero weights: 6.167757 ## Average number of links: 12.6439 ## Link number distribution: ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ## 6 6 9 5 5 10 8 10 13 12 11 6 17 8 9 11 13 6 10 7 2 4 6 2 1 1 ## 27 28 29 30 32 ## 1 2 1 1 2 ## 6 least connected regions: ## 59 114 115 116 117 198 with 1 link ## 2 most connected regions: ## 82 88 with 32 links Connectivity histogram Our method for making connectivity histograms will be the same as for contiguity based weights. We first need to get the cardinality for each observation(the number of neighbors for each observation). This is done by the card function from the spdep library. Our result from this function will be a vector of the number of neighbors for each location. dist.band.card &lt;- card(nb.dist.band) dist.band.card ## [1] 9 9 15 15 15 17 15 15 19 18 20 20 20 19 14 17 17 19 13 3 9 10 7 6 6 ## [26] 12 10 10 10 13 16 17 12 16 14 17 9 11 13 11 16 14 17 17 18 13 13 17 18 16 ## [51] 16 24 27 22 17 8 7 6 1 11 11 11 11 11 13 13 13 13 13 16 16 14 14 13 13 ## [76] 10 10 9 10 23 25 32 28 28 26 23 23 32 23 23 21 24 18 18 17 21 15 20 20 22 ## [101] 19 22 22 19 16 18 20 19 19 23 19 29 30 1 1 1 1 3 5 3 3 4 9 9 9 ## [126] 9 11 10 11 13 11 8 9 12 16 10 9 7 6 6 3 6 8 8 7 7 7 4 6 5 ## [151] 5 3 2 2 2 7 8 8 10 8 6 3 2 2 3 2 6 7 8 10 14 14 12 17 16 ## [176] 16 20 19 19 13 17 13 15 12 11 14 5 8 9 8 10 12 6 5 4 3 4 1 9 4 ## [201] 13 13 15 15 17 Once, we have our cardinality, we can make a histogram to see the distribution of the number of neighbors. we will do this with ggplot2, though base R also has a built in histogram function. To make our histogram, we start with the ggplot function, then add a histogram layer. We add the additional layer with the + operator. Next, we use geom_histogram to add our histogram. The only parameter we need is an aes. We use the aes function and enter our cardinality(dist.band.card) as the variable for the x-axis. Beyond this, we change the x-axis label to be more informative with the xlab function. ggplot() + geom_histogram(aes(x=dist.band.card)) + xlab(&quot;Number of Neighbors&quot;) Connectivity graph For our connectivity graph, we follow the same steps as for the contiguity weights. We use the plot command and use our neighbors list and coordinates as inputs. We use additional parameters to make the graph easier to look at. We shorten the line width with lwd =.2 and make the points smaller with cex = .5. The default points are much to big, and it is hard to get sense of the graphs structure. plot(nb.dist.band, coords, lwd=.2, col=&quot;blue&quot;, cex = .5) We can see that we have no isolates from our graph, as we expect from using a critical threshold approach. Our graphs structure consists of two subgraphs and two pairs of points. Isolates So far, we have used the default cut-off value for the distance band. However, the function is flexible enough that we can type in any value for the cut-off, or use the movable button to drag to any value larger than the minimum. Sometimes, theoretical or policy considerations suggest a specific value for the cut-off that may be smaller than the max-min distance. From our example we know the critical threshold is 3598, so we can pick a value lower to get some isolates in our distance-band weights. We will use 1500 ft, same as the corresponding GeoDa workbook example. The only change we need to make to do this, is make the upper distance bound 1500, instead of critical.threshold. dist.band.iso &lt;- dnearneigh(coords, 0, 1500) Isolates in the connectivity histogram We can get a measure of the number of isolates by looking at the left most bar of a connectivity histogram. To make this, we follow the same procedure as the earlier connectivity histograms. We first get the cardinality, then use ggplot2 to make the histogram. iso.card &lt;- card(dist.band.iso) ggplot() + geom_histogram(aes(x=iso.card)) + xlab(&quot;Number of Neighbors&quot;) Our resulting histogram has 24 isolates, and is much more compact than the original one with the critical threshold as the upper distance bound in our distance-band weights. Isolates in the connectivity graph The most dramatic visualization of the isolates is given by the connectivity graph. The 24 points without an edge to another point are easily identified plot(dist.band.iso, coords, lwd=.2, col=&quot;blue&quot;, cex = .5) How to deal with isolates Since the isolated observations are not included in the spatial weights (in effect, the corresponding row in the spatial weights matrix consists of zeros), they are not accounted for in any spatial analysis, such as tests for spatial autocorrelation, or spatial regression. For all practical purposes, they should be removed from such analysis. However, they are fine to be included in a traditional non-spatial data analysis. Ignoring isolates may cause problems in the calculation of spatially lagged variables, or measures of local spatial autocorrelation. By construction, the spatially lagged variable will be zero, which may suggest spurious correlations. Alternatives where isolates are avoided by design are the K-nearest neighbor weights and contiguity weights constructed from the Thiessen polygons for the points. They are discussed next. K-Nearest Neighbor Weights Concept As mentioned, an alternative type of distance-based spatial weights that avoids the problem of isolates are k-nearest neighbor weights. In contrast to the distance band, this is not a symmetric relation. The fact that B is the nearest neighbor to A does not imply that A is the nearest neighbor to B. There may be another point C that is actually closer to B than A. This asymmetry can cause problems in analysis that depend on the intrinsic symmetry of the weights (e.g., some algorithms to estimate spatial regression models). One solution is to replace the original weights matrix W by (W+W′)/2, which is symmetric by construction. GeoDa currently does not implement this approach. A potential issue with k-nearest neighbor weights is the occurrence of ties, i.e., when more than one location j has the same distance from i. A number of solutions exist to break the tie, from randomly selecting one of the k-th order neighbors, to including all of them. In GeoDa, random selection is implemented. Creating KNN Weights To create our KNN weights, we need two functions from the spdep library: knearneigh and knn2nb. We first use knearneigh to get a class of knn, as we did earlier to find the critical threshold. This time we assign k = a value of 6. This means each observation will get a list of the 6 closest points. We then use knn2nb to convert from class knn to class nb. k6 &lt;- knn2nb(knearneigh(coords, k = 6)) Properties of KNN weights One drawback of the k-nearest neighbor approach is that it ignores the distances involved. The first k neighbors are selected, irrespective of how near or how far they may be. This suggests a notion of distance decay that is not absolute, but relative, in the sense of intervening opportunities (e.g., you consider the two closest grocery stores, irrespective of how far they may be). Again, we can also use the connectivity histogram and the connectivity map to inspect the neighbor characteristics of the observations. However, in this case, the histogram doesn’t make much sense, since all observations have the same number of neighbors, as shown by our histogram. k6.card &lt;- card(k6) ggplot() + geom_histogram(aes(x=k6.card), binwidth = .01) + xlab(&quot;Number of Neighbors&quot;) In contrast, the connectivity graph, clearly demonstrates how each point is connected to six other points. In our example, this yields a fully connected graph instead of the collection of sub-graphs for the distance band. plot(k6, coords, lwd=.2, col=&quot;blue&quot;, cex = .5) Generalizing the Concept of Contiguity In GeoDa, the concept of contiguity can be generalized to point layers by converting the latter to a tessellation, specifically Thiessen polygons. Queen or rook contiguity weights can then be created for the polygons, in the usual way. Similarly, the concepts of distance-band weights and k-nearest neighbor weights can be generalized to polygon layers. The layers are represented by their central points and the standard distance computations are applied. We can apply these concepts through R computations, in a similar manner as GeoDa, but it will take more steps. Contiguity-based weights for points Thiessen polygons An alternative solution to deal with the problem of the uneven distribution of neighbor cardinality for distance-band weights is to compute a measure of contiguity. This is accomplished by turning the points into Thiessen polygons. These are also referred to as Voronoi diagrams or Delaunay triangulations. In general terms, a Thiessen polygon is a tessellation (a way to divide an area into regular subareas) that encloses all locations that are closer to the central point than to any other point. In economic geography, this is a (simplistic) notion of a market area, in the sense that all consumers in the polygon would patronize the seller located at the central point. The polygons are constructed by combining lines perpendicular at the midpoint of a line that connects a point to its nearest neighbors. From this, the most compact polygon is created. Creating Thiessen polygons from a point layer To create the Thiessen polygons, we will use the deldir package, then will convert them to a data form that we can use for the contiguity weights. We won’t go into too much detail about the deldir package, other than how to create the polygons and convert them to sp, then sf. For more information on the deldir, visit deldir documentation. To create our Thiessen polygons from the deldir function, we just need a vector with x coordinates and a vector with y coordinates. From there we can use the plot command to visualize these because here is a built in method for deldir Thiessen polygons. To get a good visual of our polygons, we need a few extra parameters. These are: wlines, wpoints, and lty. wlines = \"tess\" gets us the basic polygons. wpoints = \"none\" keeps points off the map. lty = 1 specifies a solid line type. vtess &lt;- deldir(clev.points$x, clev.points$y) plot(vtess, wlines=&quot;tess&quot;, wpoints=&quot;none&quot;, lty=1) The function below converts class deldir to sp. We will not go in depth on the structure and set up of this function because it goes to heavy into sp, and these notebooks are primarily focused on sf. voronoipolygons = function(thiess) { w = tile.list(thiess) polys = vector(mode=&#39;list&#39;, length=length(w)) for (i in seq(along=polys)) { pcrds = cbind(w[[i]]$x, w[[i]]$y) pcrds = rbind(pcrds, pcrds[1,]) polys[[i]] = Polygons(list(Polygon(pcrds)), ID=as.character(i)) } SP = SpatialPolygons(polys) voronoi = SpatialPolygonsDataFrame(SP, data=data.frame(dummy = seq(length(SP)), row.names=sapply(slot(SP, &#39;polygons&#39;), function(x) slot(x, &#39;ID&#39;)))) } The result of our function is a SpatialPolygonsDataFrame, which is a class from the sp package. We will convert again to get to the sf class, since sf is more efficient and accurate. We check our function by plotting the result and get something similar to what we plotted earlier, just now it is a different data structure. v &lt;- voronoipolygons(vtess) plot(v) Using the sf function st_as_sf, we can convert to sf from our SpatialPolygonsDataFrame. Again, we use the plot command to check our result. vtess.sf &lt;- st_as_sf(v) plot(vtess.sf$geometry) Contiguity weights for Thiessen polygons To make our queen contiguity weights, we make a function using st_relate and specifying a DE9-IM pattern. For queen contiguity, our pattern is \"F***T****\". We don’t really need to go into why the pattern is what it is for our purposes, but DE9-IM and st_relate documentation can help explain this to a degree. st_queen &lt;- function(a, b = a) st_relate(a, b, pattern = &quot;F***T****&quot;) Here we use our function to get a neighbor list of class sgbp, which stands for sparse geometry binary predicate. In order to use the spdep library, we will have to convert to class nb, as done in the contiguity weights notebook. queen.sgbp &lt;- st_queen(vtess.sf) Now we will we start our conversion from class sgbp to nb. To do this, we need to change the class explicitly and take the precaution to represent observations with no neighbors with the integer 0. Our data set doesn’t have any observations without neighbors, but in ones with these, it will mess everything up, if not dealt with. We start with the function operator. The input for our function will be an object of class sgbp, denoted by x. We store the attributes in attrs, as we will need to reapply them later. Now we deal we observation with no neighbors. We will use lapply, which applies a function to each element of a list or vector. As for the input function, we make one that checks the length of an element of our list for 0(meaning no neighbors) and returns 0 if the element is empty and the element otherwise. This can be a bit confusing, for more information on lapply, check out lapply documentation From here we will have dealt with observations with no neighbors, but will need to reapply our attributes to the resulting structure from the lapply function. This is done by calling the attributes function and assigning the stored attributes. The final step is to explicitly change the class to nb by using the class function and assigning \"nb\". We then return our object x. as.nb.sgbp &lt;- function(x, ...) { attrs &lt;- attributes(x) x &lt;- lapply(x, function(i) { if(length(i) == 0L) 0L else i } ) attributes(x) &lt;- attrs class(x) &lt;- &quot;nb&quot; x } With our conversion function, we can covert to class nb. queen.nb &lt;- as.nb.sgbp(queen.sgbp) We can take a look at the distribution of the number of neighbors in our queen contiguity for the Thiessen polygons with a connectivity histogram. The steps are the same as earlier for all the other connectivity histograms queen.nb.card &lt;- card(queen.nb) ggplot() + geom_histogram(aes(x=queen.nb.card)) + xlab(&quot;Number of Neighbors&quot;) summary(queen.nb) ## Neighbour list object: ## Number of regions: 205 ## Number of nonzero links: 1154 ## Percentage nonzero weights: 2.745985 ## Average number of links: 5.629268 ## Link number distribution: ## ## 2 3 4 5 6 7 8 9 10 ## 1 11 34 55 53 31 11 5 4 ## 1 least connected region: ## 141 with 2 links ## 4 most connected regions: ## 36 57 87 96 with 10 links The histogram and summary statistics represents a much more symmetric and compact distribution of the neighbor cardinalities, very similar to the typical shape of the histogram for first order contiguity between polygons. The median number of neighbors is 6 and the average 5.6, with a limited spread around these values. In many instances where the point distribution is highly uneven, this approach provides a useful compromise between the distance-band and the k-nearest neighbors. This will be further illustrated through a connectivity graph with a far more balanced structure. plot(queen.nb,coords, lwd=.2, col=&quot;blue&quot;, cex = .5) Distance-based weights for polygons As we can do contiguity-based weights for point data, we use distance-based weights for polygons. To illustrate this, we will use U.S homicide data from a previous notebook(the contiguity-based weights notebook). To implement this we start by computing shape center coordinates for the polygons. Once we get the coordinates, we will follow the procedure for making distance-band weights. There will be one key difference, as we will be working with nonprojected data. We will have to use great circle distance instead of euclidean. The formulas for these were went over earlier, in the notebook, but working with this difference requires only a few changes in the steps to make distance-band neighbors. Getting the data To get the data, we will use the geodaData package again. Additionally, the data for this notebook can be found at US Homicides. If the data is downloaded directly, then you must put it in your working directory and load it with st_read. us.bound &lt;- geodaData::ncovr Computing the centroids Getting our coordinate data is a little more complicate than just using st_centroid on our shapefile. We need to do this to get coordinates in a supported format for the neighbor functions in the *spdep** package. We are going to get vectors for latitude and longitude, then will bind the columns for later use. To get our centroid coordinates, we will map the function st_centroid over the geometry column of us.bound. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid. We will use the map_dbl variation of map from the purrr package. For more documentation, check out map documentation latitude &lt;- map_dbl(us.bound$geometry, ~st_centroid(.x)[[1]]) We do the same for latitude, using [[2]] to get the latitude values. longitude &lt;- map_dbl(us.bound$geometry, ~st_centroid(.x)[[2]]) Now we bind the coordinates together with cbind, so we can enter them as parameters in the neighbors functions. center.coords &lt;- cbind(latitude,longitude) Computing the critical threshold To get the critical threshold, we follow the same steps as earlier, with one difference. This time we will set longlat = TRUE in three of the functions. This is important because our data is not projected and to get accurate results we must work with great circle distance. Again we begin with finding the first nearest neighbors and convert from class knn to class nb, and set longlat = TRUE. k.poly &lt;- knn2nb(knearneigh(center.coords, longlat = TRUE)) After we have computed the first nearest neighbors, we find the maximum distance in this neighbors list, so our distance-band weights will not have isolates. For our distance function: nbdists we also need to set longlat = TRUE. This is needed here, as we are calculating the distances between each set of points. critical.threshold.poly &lt;- max(unlist(nbdists(k.poly,center.coords, longlat = TRUE))) Computing distance-band neighbors We now have all the components necessary to do the distance-band weight for our polygon data. Again we use dnearneigh to get our distance-band weights. We input the centroid coordinates, a distance lower bound(0), a distance upper bound(the critical threshold), and set longlat = TRUE. nb.dist.band.poly &lt;- dnearneigh(center.coords, 0, critical.threshold.poly, longlat = TRUE) Once we have calculated the distance-band weights, we can assess the distribution of the number of neighbors through a connectivity histogram, following the same procedure used through the notebook. poly.nb.card &lt;- card(nb.dist.band.poly) ggplot() + geom_histogram(aes(x=poly.nb.card)) + xlab(&quot;Number of Neighbors&quot;) Use setwd(directorypath) to specify the working directory.↩︎ Use install.packages(packagename).↩︎ "],["spatial-weights-as-distance-functions.html", "Chapter 8 Spatial Weights as Distance Functions Introduction Preliminaries Inverse Distance Weights Kernal Weights", " Chapter 8 Spatial Weights as Distance Functions Introduction This notebook covers the functionality of the Spatial Weights as Distance Functions section of the GeoDa workbook. We refer to that document for details on the methodology, references, etc. The goal of these notes is to approximate as closely as possible the operations carried out using GeoDa by means of a range of R packages. The notes are written with R beginners in mind, more seasoned R users can probably skip most of the comments on data structures and other R particulars. Also, as always in R, there are typically several ways to achieve a specific objective, so what is shown here is just one way that works, but there often are others (that may even be more elegant, work faster, or scale better). For this notebook, we use Cleveland house price data. Our goal in this lab is show how to assign spatial weights based on different distance functions. Objectives After completing the notebook, you should know how to carry out the following tasks: Compute inverse distance functions Compute kernal weights functions Assess the characteristics of weights based on distance functions R Packages used sf: To read in the shapefile. spdep: To create k-nearest neighbors and distance-band neighbors, calculate distances between neighbors, convert to a weights structure, and coercion methods to sparse matrices. geodaData: To access the data for this notebook. R Commands used Below follows a list of the commands used in this notebook. For further details and a comprehensive list of options, please consult the R documentation. Base R: install.packages, library, setwd, class, str, lapply, attributes, summary, head, seq, as, cbind, max, unlist, length, sqrt, exp, diag, sort, append sf: st_read, plot spdep: knn2nb, dnearneigh, knearneigh, nb2listw, mat2listw Preliminaries Before starting, make sure to have the latest version of R and of packages that are compiled for the matching version of R (this document was created using R 3.5.1 of 2018-07-02). Also, optionally, set a working directory, even though we will not actually be saving any files.23 Load packages First, we load all the required packages using the library command. If you don’t have some of these in your system, make sure to install them first as well as their dependencies.24 You will get an error message if something is missing. If needed, just install the missing piece and everything will work after that. library(sf) library(spdep) library(geodaData) library(spatialreg) Obtaining the Data from the GeoDa website All of the data for the R notebooks is available in the geodaData package. We loaded the library earlier, now to access the individual data sets, we use the double colon notation. This works similar to to accessing a variable with $, in that a drop down menu will appear with a list of the datasets included in the package. For this notebook, we use clev_pts. Otherwise, Tt get the data for this notebook, you will and to go to Cleveland Home Sales The download format is a zipfile, so you will need to unzip it by double clicking on the file in your file finder. From there move the resulting folder titled: nyc into your working directory to continue. Once that is done, you can use the sf function: st_read() to read the shapefile into your R environment. clev.points &lt;- geodaData::clev_pts Inverse Distance Weights Concepts One can readily view spatial weights based on a distance cut-off as representing a step function, with a value of 1 for neighbors with \\(d_{ij} &lt; \\delta\\), and a value of 0 for others. As before, \\(d_{ij}\\) stands for the distance between observations i and j, and \\(\\delta\\) is the bandwidth. A straightforward extension of this principle is to consider a continuous parameterized function of distance itself: \\[w_{ij}=f(d_{ij},\\theta)\\] with f as a functional form and \\(\\theta\\) a vector of parameters. In order to conform to Tobler’s first law of geography, a distance decay effect must be respected. In other words, the value of the function of distance needs to decrease with a growing distance. More formally, the partial derivative of the distance function with respect to distance should be negative, \\(\\partial{}w_{ij}/\\partial{}d_{ij}&lt;0\\) . Commonly used distance functions are the inverse, with \\(w_{ij}=1/d_{ij}^\\alpha\\)(and \\(\\alpha\\) as a parameter), and the negative exponential, with \\(w_{ij}=e^{-\\beta d_{ij}}\\)(and \\(\\beta\\) as a parameter). The functions are often combined with a distance cut-off criterion, such that \\(w_{ij}=0\\) for \\(d_{ij}&gt;\\delta\\). In practice, the parameters are seldom estimated, but typically set to a fixed value, such as \\(\\alpha=1\\) for inverse distance weights (\\(1/d_{ij}\\)), and \\(\\alpha=2\\) for gravity weights (\\(1/d_{ij}^2\\)). By convention, the diagonal elements of the spatial weights are set to zero and not computed. Plugging in a value of \\(d_{ii}=0\\) would yield division by zero for inverse distance weights. The distance-based weights depend not only on the parameter value and functional form, but also on the metric used for distance. Since the weights are inversely related to distance, large values for the latter will yield small values for the former, and vice versa. This may be a problem in practice when the distances are so large (i.e., measured in small units) that the corresponding inverse distance weights become close to zero, possibly resulting in a zero spatial weights matrix. In addition, a potential problem may occur when the distance metric is such that distances take on values less than one. As a consequence, some inverse distance values may be larger than one, which is typically not a desired result. Rescaling of the coordinates will fix both problems. 8.0.1 Creating inverse distance functions for distance bands To create our inverse disatnce weights, we follow the steps involved with creating distance-band neighbors along with a few additional steps to calculate and assign the weight values. Here we will go over a basic outline of the steps to create the inverse distance weights. First we calculate our distance-band neighbors. Next we get the distances between each neighbors stored in the same format as the neighbors data structure. Then we apply a function to each element in this structure, giving us the inverse distances. Finally we assign these as the weight values when converting from class nb to class listw. We begin by putting our coordinates in a separate matrix from clev.points coords &lt;- cbind(clev.points$x,clev.points$y) In order to calulate our distance-band neighbors, we need an upper and lower distance bound. The lower is always 0 for the most part. We can put anything for the upper, but we will pick a value, that keeps isolates out of our distance-band-neighbors. To do this we need to find the k-nearest neighbors for k = 1, then get the maximum distance between points. This is covered in the distance-band spatial weights notebook, but we will go through the steps here. To get the k-nearest neighbors for k = 1, we need two function from the spdep library: knn2nb and knearneigh. knearneigh calculates the neighbors and stores the information in class knn, and knn2nb converts the class to nb, so we can work with it further. k1 &lt;- knn2nb(knearneigh(coords)) Computing the critcal threshold will require a few functions now that we have a neighbors list. First step is to get the distances between each point and it’s closest neighbor. This can be done with the nbdists. With these distances, we just need to find the maximum. For this we use the max command. However, we cannot do this with lists, so we must first get a data type that works for the max command, in our case, we use unlist critical.threshold &lt;- max(unlist(nbdists(k1,coords))) critical.threshold ## [1] 3598.055 We have all the necessary components to calculate the distance-band neighbors. To get these we use dnearneigh. The parameters needed are the coordinates, a lower distance bound and an upper distance bound. nb.dist.band &lt;- dnearneigh(coords, 0, critical.threshold) To get inverse distance, we need to calculate the distances between all of the neighbors. for this we will use nbdists, which gives us the distances in a similar structure to our input neighbors list. To use this function we need to input the neighbors list and the coordinates. distances &lt;- nbdists(nb.dist.band,coords) distances[1] ## [[1]] ## [1] 385.161 3013.071 1160.312 1858.904 3367.150 2525.503 3253.025 3390.735 ## [9] 3369.644 Calculating the inverse distances will require a function that applies 1/x over the entire distances data structure. We will use lapply to accomplish this. The parameters needed are the distances, and a function which we specify in lapply. We use the function operator with (1/x) to get the appropriate function. invd1 &lt;- lapply(distances, function(x) (1/x)) Here we check the length of the inverse distances to make sure it lines up with our neighbors list. length(invd1) ## [1] 205 We check the first element of the resulting data structure to make sure it is in line with the neighbors list structure. This is important because we will need the structures to correspond in order to assign the inverse distances as the weight values when converting from a neighbors list or class nb to a weight structure: class listw. invd1[1] ## [[1]] ## [1] 0.0025963168 0.0003318873 0.0008618371 0.0005379514 0.0002969871 ## [6] 0.0003959608 0.0003074062 0.0002949213 0.0002967673 A key insight from the first element of the inverse distance structure is that the values are very small, or too close to zero. The unit of distance for our dataset is in feet. This means distance values between points can be quite large and result in small inverses. To correct for this scale dependence, we can rescale the distances by repeating the inverse calculations, while adjusting the scale. We can make this adjustment by dividing x in our function by 100, before calculating the inverses. invd1a &lt;- lapply(distances, function(x) (1/(x/100))) invd1a[1] ## [[1]] ## [1] 0.25963168 0.03318873 0.08618371 0.05379514 0.02969871 0.03959608 0.03074062 ## [8] 0.02949213 0.02967673 Now that we have properly scaled inverse distances, we can assign them as weight values. This is done in the conversion function nb2listw. To assign the weights, we use the glist = argument. For this to work we also have to specify style = \"B\", otherwise the listw function will use the default row standardization. invd.weights &lt;- nb2listw(nb.dist.band,glist = invd1a,style = &quot;B&quot;) Here we take a cursory look at our weights with summary to get basic imformation and statistics. summary(invd.weights) ## Characteristics of weights list object: ## Neighbour list object: ## Number of regions: 205 ## Number of nonzero links: 2592 ## Percentage nonzero weights: 6.167757 ## Average number of links: 12.6439 ## Link number distribution: ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ## 6 6 9 5 5 10 8 10 13 12 11 6 17 8 9 11 13 6 10 7 2 4 6 2 1 1 ## 27 28 29 30 32 ## 1 2 1 1 2 ## 6 least connected regions: ## 59 114 115 116 117 198 with 1 link ## 2 most connected regions: ## 82 88 with 32 links ## ## Weights style: B ## Weights constants summary: ## n nn S0 S1 S2 ## B 205 42025 180.2882 145.9202 1018.442 We can check the values of the weights by using $weights to access the values. invd.weights$weights[1] ## [[1]] ## [1] 0.25963168 0.03318873 0.08618371 0.05379514 0.02969871 0.03959608 0.03074062 ## [8] 0.02949213 0.02967673 Properties of inverse distance weights Since the properties only pertain to the connectivity structure implied by the weights, they are identical to the ones obtained for the standard distance-band weights. It is important to keep in mind that the actual values for the weights are ignored in this operation. plot(invd.weights, coords, lwd=.2, col=&quot;blue&quot;, cex = .5) The connectivity map and the connectivity graph associated with the weights are the same as before as well. Using non-geographical coordinates So far we have been using x and y coordinates for the inputs into distance calculates, but it is important to note that you can use any two variables contained in the dataset in place of x and y coordinates. For example, this allows for the computation of so-called socio-economic weights, where the difference between two locations on any two variables can be used as the distance metric. We don’t do this in this notebook, as the only meaningful variable in our dataset is housing prices. Creating inverse distance functions for k-nearest neighbors We can compute inverse distance weights for k-nearest neighbors using the same approach as for distance-band neighbors. The only difference being that we don’t have to calculate a critical threshold for k-nearest neighbors. We start by getting the k-nearest neighbors for k = 6. We do this with knearneigh and knn2nb. k6 &lt;- knn2nb(knearneigh(coords, k = 6)) Now that we have the neighbors list we need all of the distances between neighbors in a similar data structure, which we use nbdist for again. k.distances &lt;- nbdists(k6, coords) Here we calculate the inverse distances, keeping in mind the scale from the distance-band weights from earlier. invd2a &lt;- lapply(k.distances, function(x) (1/(x/100))) invd2a[1] ## [[1]] ## [1] 0.25963168 0.03318873 0.08618371 0.05379514 0.03959608 0.03074062 Lastly, we assign the weight values with the glist = parameter and speficy the style as “B” to avoid default computations. invd.weights.knn &lt;- nb2listw(k6,glist = invd2a,style = &quot;B&quot;) invd.weights.knn$weights[1] ## [[1]] ## [1] 0.25963168 0.03318873 0.08618371 0.05379514 0.03959608 0.03074062 Kernal Weights Concepts Kernel weights are used in non-parametric approaches to model spatial covariance, such as in the HAC method for heteroskedastic and spatial autocorrelation consistent variance estimates. The kernel weights are defined as a function K(z) of the ratio between the distance dij from i to j, and the bandwidth \\(h_i\\), with \\(z=d_{ij}/h_i\\). This ensures that z is always less than 1. For distances greater than the bandwidth, K(z)=0. We will go over five different kernal weights functions that are supported by GeoDa: Uniform, \\(K(z) = 1/2\\) for \\(\\mid z \\mid &lt; 1\\) Triangular, \\(K(z) = (1 - \\mid z \\mid )\\) for \\(\\mid z \\mid &lt; 1\\) Quadratic or Epanechnikov, \\(K(z) = (3/4)(1 - z^2)\\) for \\(\\mid z \\mid &lt; 1\\) Quartic, \\(K(z) = (15/16)(1 - z^2)^2\\) for \\(\\mid z \\mid &lt; 1\\) Gaussian, \\(K(z) = (2\\pi)^{1/2}\\exp(-z^2/2)\\) Typically, the value for the diagonal elements of the weights is set to 1, although GeoDa allows for the actual kernel value to be used as well. We will go through both of these options too. Many careful decisions must be made in selecting a kernel weights function. Apart from the choice of a functional form for K( ), a crucial aspect is the selection of the bandwidth. In the literature, the latter is found to be more important than the functional form. A drawback of fixed bandwidth kernel weights is that the number of non-zero weights can vary considerably, especially when the density of the point locations is not uniform throughout space. This is the same problem encountered for the distance band spatial weights. In GeoDa, there are two types of fixed bandwidths for kernel weights. One is the max-min distance used earlier (the largest of the nearest-neighbor distances). The other is the maximum distance for a given specification of k-nearest neighbors. For example, with knn set to a given value, this is the distance between the selected k-nearest neighbors pairs that are the farthest apart. Creating Kernal weights In creating kernal weights, we will cover two important options: the fixed bandwidth and the variable bandwidth. For the fixed bandwidth, we will be using distance-band neighbors. For the variable bandwidth we will need kth-nearest neighbors. To start, we will compute a new distance-band neighbors list with the critcial threshold, calculated earlier in the notebook. kernal.nb &lt;- dnearneigh(coords, 0, critical.threshold) Before we start computing kernal weights, we need to add the diagonal elements to our neighbors list. We do this because in the kernal weights methods, the diagonal element is either assigned a value of 1 or is computed in the kernal function with a distance of 0. It is important to note that the diagonal element means a point is a neighbor of its own self when include in the neighbors list. spdep has a built in function for this. include.self can be used to add the diagonal elements to a neighbors list of class nb. include.self(kernal.nb) ## Neighbour list object: ## Number of regions: 205 ## Number of nonzero links: 2797 ## Percentage nonzero weights: 6.655562 ## Average number of links: 13.6439 kernal.nb[[2]] ## [1] 1 6 7 8 9 10 31 32 34 With the diagonal elements, we can proceed further. To compute the kernal weight values, we need the corresponding distances for each neighbor. We do this with nbdists, same as earlier. kernalw.distances &lt;- nbdists(kernal.nb, coords) kernalw.distances[1] ## [[1]] ## [1] 385.161 3013.071 1160.312 1858.904 3367.150 2525.503 3253.025 3390.735 ## [9] 3369.644 When checking the first row of the distances, we see a 0. This is the distance value for the diagonal element. Uniform \\(K(z) = 1/2\\) for \\(\\mid z \\mid &lt; 1\\) To get uniform weights, we use a similar method to the inverse disatnce weights. We use lapply to apply a function to all elements of our distance structure. The function, in this case, is 0 * x + .5. We do this to assign uniform weights of .5, the 0*x is a necessary addition to get lapply to work properly. uniform &lt;- lapply(kernalw.distances, function(x) x*0 + .5) uniform[1] ## [[1]] ## [1] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 Then to assign the weights, we use the same procedure as the inverse distance weights. We use the glist argument to explicity assign the weight we calculated above. uniform.weights &lt;- nb2listw(kernal.nb,glist = uniform,style = &quot;B&quot;) Triangular \\(K(z) = (1 - \\mid z \\mid )\\) for \\(\\mid z \\mid &lt; 1\\) Same process, for triangular, we just apply a different function to the distances. We use abs to get the absolute value in our caluculations. triangular &lt;- lapply(kernalw.distances, function(x) 1- abs((x/critical.threshold))) triangular[1] ## [[1]] ## [1] 0.89295300 0.16258344 0.67751688 0.48335866 0.06417492 0.29809225 0.09589360 ## [8] 0.05762014 0.06348183 triang.weights &lt;- nb2listw(kernal.nb,glist = triangular,style = &quot;B&quot;) triang.weights$weights[1] ## [[1]] ## [1] 0.89295300 0.16258344 0.67751688 0.48335866 0.06417492 0.29809225 0.09589360 ## [8] 0.05762014 0.06348183 Epanechnikov Quadratic or Epanechnikov, \\(K(z) = (3/4)(1 - z^2)\\) for \\(\\mid z \\mid &lt; 1\\) epanechnikov &lt;- lapply(kernalw.distances, function(x) .75*(1-(x/critical.threshold)^2)) epanechnikov[1] ## [[1]] ## [1] 0.74140570 0.22405013 0.67200348 0.54981129 0.09317357 0.38049413 0.13694371 ## [8] 0.08394016 0.09220029 epan.weights &lt;- nb2listw(kernal.nb,glist = epanechnikov,style = &quot;B&quot;) epan.weights$weights[1] ## [[1]] ## [1] 0.74140570 0.22405013 0.67200348 0.54981129 0.09317357 0.38049413 0.13694371 ## [8] 0.08394016 0.09220029 Quartic \\(K(z) = (15/16)(1 - z^2)^2\\) for \\(\\mid z \\mid &lt; 1\\) quartic &lt;- lapply(kernalw.distances, function(x) (15/16)*(1-(x/critical.threshold)^2)^2) quartic[1] ## [[1]] ## [1] 0.91613736 0.08366410 0.75264779 0.50382076 0.01446886 0.24129297 0.03125597 ## [8] 0.01174325 0.01416816 quartic.weights &lt;- nb2listw(kernal.nb,glist = quartic,style = &quot;B&quot;) quartic.weights$weights[1] ## [[1]] ## [1] 0.91613736 0.08366410 0.75264779 0.50382076 0.01446886 0.24129297 0.03125597 ## [8] 0.01174325 0.01416816 Gaussian \\(K(z) = (2\\pi)^{1/2}\\exp(-z^2/2)\\) For this formula we need the sqrt function and the exp function, but other than that, it is a similar contruction as the others. gaussian.w &lt;- lapply(kernalw.distances, function(x) sqrt(2*pi)*exp((-(x/critical.threshold)^2)/2)) gaussian.w[1] ## [[1]] ## [1] 2.492308 1.765273 2.379620 2.193458 1.617779 1.959327 1.665681 1.607851 ## [9] 1.616730 gaussian.weights &lt;- nb2listw(kernal.nb,glist = gaussian.w,style = &quot;B&quot;) gaussian.weights$weights[1] ## [[1]] ## [1] 2.492308 1.765273 2.379620 2.193458 1.617779 1.959327 1.665681 1.607851 ## [9] 1.616730 Variable bandwidth Now that we have covered the 5 types of kernal weight function, implemented by GeoDa, we will work to emulate the example from the corresponding GeoDa workbook in R. The options in this example are conveniently done with GeoDa, but in our case there will be more leg work to get this done. We will be doiing a variable bandwidth with diagonal elements set to a value of 1 for a triangular kernal. For the variable bandwidth, we will be using k6: a k-nearest neighbors list, created earlier in the notebook. We already have the associated disatnces in k.distances. We will be directly altering the distance object, so we will assign a copy k.disatnces1. k.distances1 &lt;- k.distances In order to implement our variable bandwidth, we will need to loop through each element of k.distances, find the maximum distance of each row, then apply the triangular kernal weight function of that row with the bandwidth being used to calculate the z values for the K(z) function. To begin, we make a for loop using the in operator. The range we specify is 1 to the length of k.distances. We get this length with length. This will allow us to excute the statements in the loop on i-values 1 to 205. The first thing we need in the loop is the variable bandwidth value for the ith row. This is easily done by callling the max function on the row. We get the associated row by k.distances[[i]]. Next we compute the new row with our triagular kernal function. We use the abs function for absolute value. Lastly, we assign the new row values to the the ith row of the k.distances structure. for (i in 1:length(k.distances1)){ bandwidth &lt;- max(k.distances1[[i]]) new_row &lt;- 1- abs(k.distances1[[i]] / bandwidth) k.distances1[[i]] &lt;- new_row } k.distances1[[1]] ## [1] 0.88159911 0.07376327 0.64331286 0.42856135 0.22364475 0.00000000 There is one potential issue with what we have done so far for the variable bandwidth. Our bandwidth is the same as the largest distance in each row, so one neighbor will get 0 weight in the resulting weight structure for most of our functions. To give weight to this value, we will need to adjust the associated bandwidths, by getting a value that is between the 6th nearest neighbor and the 7th nearest neighbor. We will do this by taking the average of the two values for our bandwith calculations. This will require a few extra steps and adjustments to our for loop. The first thing we need to implement this is the k-nearest neighbors for k = 7. This is the same process as our previous calculations for k-nearest neighbors. k7 &lt;- knn2nb(knearneigh(coords, k = 7)) Next we get the associated distances using nbdists. k7.distances &lt;- nbdists(k7, coords) To avoid altering the original k.distances, we will assign a new variable to hold the necessary information. k.distances2 &lt;- k.distances Here we remake the previous for loop with a few changes. Now we loop through and find the max distance for both the 7th nearest neighbors and 6th nearest neighbors, then get the average between the two before computing the kernal weight function. for (i in 1:length(k.distances)){ maxk6 &lt;- max(k.distances2[[i]]) maxk7 &lt;- max(k7.distances[[i]]) bandwidth &lt;- (maxk6 + maxk7) /2 new_row &lt;- 1- abs(k.distances2[[i]] / bandwidth) k.distances2[[i]] &lt;- new_row } k.distances2[[1]] ## [1] 0.88364023 0.08973071 0.64946181 0.43841241 0.23702838 0.01723905 var.band.weights &lt;- nb2listw(k6,glist = k.distances2,style = &quot;B&quot;) var.band.weights$weights[[1]] ## [1] 0.88364023 0.08973071 0.64946181 0.43841241 0.23702838 0.01723905 With our new weights structure all the neighbors included have a nonzero weight. Treatment of diagonal elements As of now, we have just been applying the kernal function to the diagonal elements. The default in GeoDa is to assign a value of 1 to these elements. For us to do this, we need a little extra work. We will take advantage of the coercion methods that spdep provides from class listw to RsparseMatrix of the Matrix package. Once converted to class **RsparseMatrix, we can assign values of 1 to the diagonal elements, then convert back. To start we use the as function with var.band.weights as the first parameter. We specify the class to convert to with the string: “RsparseMatrix”. We use var.band.weights to remake the GeoDa workbook example. B &lt;- as(var.band.weights, &quot;RsparseMatrix&quot;) Now that we have converted, we can assign values of 1 to the diagonal elements with the diag function. diag(B) &lt;- 1 With this, we can now convert back to class listw with the spdep function mat2listw. The function is pretty self explanatory, as it converts from a matrix the listw. We need one extra step to accomplish the conversion. We first need to convert B to class dgRMatrix before we can use the mat2listw function. var.band.w2 &lt;- mat2listw(as(B, &quot;dgRMatrix&quot;)) Properties of kernal weights The connectivity plot will be the same for kernal weights as the 6th nearest neighbors structure. This is because they have the same neighbors list and connectivity ignores the weights themselves. While our connectivity plot will be the same, the histogram and summary stats will be different from the ones in GeoDa. This is because we have to add the diagonal elements to the neighbors structure before moving forward. This is seen below when our 6th-nearest neighbors has an average of 7 links. To get a more accurate view of the connectivity properties, the structure will have to be examined before the diagonal elements are added. summary(var.band.w2) ## Characteristics of weights list object: ## Neighbour list object: ## Number of regions: 205 ## Number of nonzero links: 1435 ## Percentage nonzero weights: 3.414634 ## Average number of links: 7 ## Non-symmetric neighbours list ## Link number distribution: ## ## 7 ## 205 ## 205 least connected regions: ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 with 7 links ## 205 most connected regions: ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 with 7 links ## ## Weights style: M ## Weights constants summary: ## n nn S0 S1 S2 ## M 205 42025 622.5036 844.1885 7916.201 plot(var.band.w2, coords, lwd=.2, col=&quot;blue&quot;, cex = .5) Use setwd(directorypath) to specify the working directory.↩︎ Use install.packages(packagename).↩︎ "],["applications-of-spatial-weights.html", "Chapter 9 Applications of Spatial Weights Introduction Preliminaries Spatially lagged variables Spatially lagged variables from inverse distance weights Spatially lagged variables from kernel weights Spatial rate smoothing Spatial Empirical Bayes smoothing", " Chapter 9 Applications of Spatial Weights Introduction This notebook cover the functionality of the Applications of Spatial Weights section of the GeoDa workbook. We refer to that document for details on the methodology, references, etc. The goal of these notes is to approximate as closely as possible the operations carried out using GeoDa by means of a range of R packages. The notes are written with R beginners in mind, more seasoned R users can probably skip most of the comments on data structures and other R particulars. Also, as always in R, there are typically several ways to achieve a specific objective, so what is shown here is just one way that works, but there often are others (that may even be more elegant, work faster, or scale better). For this notebook, we use Cleveland house price data. Our goal in this lab is show how to assign spatial weights based on different distance functions. Objectives After completing the notebook, you should know how to carry out the following tasks: Create a spatially lagged variable as an average or sum of the neighbors Create a spatially lagged variable as a window sum or average Create a spatially lagged variable based on inverse distance weights Create a spatially lagged variable based on kernel weights Rescaling coordinates to obtain inverse distance weights Compute and map spatially smoothed rates Compute and map spatial Empirical Bayes smoothed rates R Packages used sf: To read in the shapefile. spdep: To create k-nearest neighbors and distance-band neighbors, calculate distances between neighbors, convert to a weights structure, and coercion methods to sparse matrices. knitr: To make nicer looking tables ggplot2: To make a scatterplot comparing two different rate variables tmap: To make maps of different spatial rate variables broom: To get regression statistics in a tidy format GGally: To make a parallel coordinates plot purrr: To map a centroid function of the geometry column of an sf data structure geodaData: To access the data for this notebook tidyverse: Basic data frame manipulation R Commands used Below follows a list of the commands used in this notebook. For further details and a comprehensive list of options, please consult the R documentation. Base R: install.packages, library, setwd, class, str, lapply, attributes, summary, head, seq, as, cbind, max, unlist, length, sqrt, exp, diag, sort, append, sum sf: st_read, plot spdep: knn2nb, dnearneigh, knearneigh, nb2listw, mat2listw, EBlocal, lag.listw, include.self knitr: kable ggplot2: ggplot, geom_smooth, geom_point, ggtitle tmap: tm_shape, tm_fill, tm_border, tm_layout broom: tidy GGally: ggparcoord purrr: map_dbl tidyverse: mutate Preliminaries Before starting, make sure to have the latest version of R and of packages that are compiled for the matching version of R (this document was created using R 3.5.1 of 2018-07-02). Also, optionally, set a working directory, even though we will not actually be saving any files.25 Load packages First, we load all the required packages using the library command. If you don’t have some of these in your system, make sure to install them first as well as their dependencies.26 You will get an error message if something is missing. If needed, just install the missing piece and everything will work after that. library(sf) library(spdep) library(tmap) library(ggplot2) library(GGally) library(broom) library(knitr) library(tidyverse) library(purrr) geodaData All of the data for the R notebooks is available in the geodaData package. We loaded the library earlier, now to access the individual data sets, we use the double colon notation. This works similar to to accessing a variable with $, in that a drop down menu will appear with a list of the datasets included in the package. For this notebook, we use clev_pts. If you choose to download the data into your working directory, you will need to go to Cleveland Home Sales The download format is a zipfile, so you will need to unzip it by double clicking on the file in your file finder. From there move the resulting folder titled: nyc into your working directory to continue. Once that is done, you can use the sf function: st_read() to read the shapefile into your R environment. clev.points &lt;- geodaData::clev_pts Spatially lagged variables With a neighbor structure defined by the non-zero elements of the spatial weights matrix W, a spatially lagged variable is a weighted sum or a weighted average of the neighboring values for that variable. In most commonly used notation, the spatial lag of y is then expressed as Wy. . Formally, for observation i, the spatial lag of \\(y_i\\), referred to as \\([Wy]_i\\) (the variable Wy observed for location i) is: \\[[Wy]_i = w_{i1}y_1 + w_{i2}y_2 + ... + w_{in}y_n\\] or, \\[[Wy]_i = \\sum_{j=1}^nw_{ij}y_j\\] where the weights \\(w_{ij}\\) consist of the elements of the i-th row of the matrix W, matched up with the corresponding elements of the vector y. In other words, the spatial lag is a weighted sum of the values observed at neighboring locations, since the non-neighbors are not included (those i for which \\(w_{ij}=0\\)). Typically, the weights matrix is very sparse, so that only a small number of neighbors contribute to the weighted sum. For row-standardized weights, with\\(\\sum_jw_{ij} = 1\\), the spatially lagged variable becomes a weighted average of the values at neighboring observations. In matrix notation, the spatial lag expression corresponds to the matrix product of the n × n spatial weights matrix W with the n × 1 vector of observations y, or W × y. The matrix W can therefore be considered to be the spatial lag operator on the vector y. In a number of applied contexts, it may be useful to include the observation at location i itself in the weights computation. This implies that the diagonal elements of the weights matrix must be non-zero, i.e., \\(w_{ii}≠0\\). Depending on the context, the diagonal elements may take on the value of one or equal a specific value (e.g., for kernel weights where the kernel function is applied to the diagonal). We will highlight this issue in the specific illustrations that follow. Creating a spatially lagged variable In this section, we will go through the four different spatial lag options covered in the Geoda workbook. These are Spatial lag with row-standardized weights, ** Spatial lag as a sum of neighboring values, Spatial window average, and Spatial window sum**. Creating the weights To start, we will need to make our weights in order to follow along with the Geoda workbook example. This will require making a k-nearest neighbors for k = 6 with row standarized weights. The process for making these weights is the same as covered in earlier notebooks. However, to summarize, we start by getting the coordinates in a separate matrix with the base R function cbind, then use that with the spdep functions knn2nb and knearneigh to compute a neighbors list. From there we convert the neighbors list to class listw with nb2listw. This will result in row standardized weights. It is important to note that we will have to work with neighbors structure in some cases to get the desired functionality, which will require creating more than one weights object for the k-nearest neighbors weights used in this section of the notebook. # getting coordinates coords &lt;- cbind(clev.points$x,clev.points$y) # creating neighbors list k6 &lt;- knn2nb(knearneigh(coords, k = 6)) # converting to weights structure from neighbors list k6.weights1 &lt;- nb2listw(k6) k6.weights1$weights[[1]] ## [1] 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 Spatial lag with row-standardized weights The default case in Geoda is to use row-standardized weights and to leave out the diagonal element. Implementing this will be relatively simple, as we will only need one line of code with the weights structure: k6.weights1. To create the lag variable, the spdep function, lag.listw is used. The inputs require a weights structure and a variable with the same length as the weights structure. The length should not be a problem if the variable comes from the same dataset used to create your weights. We create a new data frame of the sale_price variable and the lag variable to get a side by side look. To get a brief look at the first few observations of both variables, we use head. Then to get a nicer looking table, we use kable from the knitr package. lag1 &lt;- lag.listw(k6.weights1, clev.points$sale_price) df &lt;- data.frame(sale_price = clev.points$sale_price, lag1) kable(head(df)) sale_price lag1 235500 79858.33 65000 90608.33 92000 71041.67 5000 91375.00 116250 72833.33 120000 74041.67 The values match with the example from the Geoda workbook, only difference being in that it is rounded to less significant figures than GeoDa. The lag variables are calculated in this instance, by taking an average of the neighboring values. We will illustrate this by finding the neighboring sale price values for the first observation. To get these values, we get the neighbor ids for the first observation by double bracket notation. We then select the sale price values by indexing with the resulting numeric vector. nb1 &lt;- k6[[1]] nb1 &lt;- clev.points$sale_price[nb1] nb1 ## [1] 65000 120000 131650 81500 76000 5000 We now verify the value for the spatial lag listed in the table. It is obtained as the average of the sales price for the six neighbors, or (131650 + 65000 + 81500 + 76000 + 120000 + 5000)/6 = 79858.33. We can quickly assess the effect of spatial averaging by comparing the descriptive statistics for the lag variable with those of the original sale price variable. summary(clev.points$sale_price) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1049 9000 20000 41897 48500 527409 summary(lag1) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 6583 16142 26500 39818 54417 229583 As seen above, the range for the original sale price variable is 1,049 to 527,409. The range for the lag variable is much more compressed, being 6,583 to 229,583. The typical effect of spatial lag is a compression of the range and variance of a variable. We can see this further when examing the standard deviation. sd(clev.points$sale_price) ## [1] 60654.34 sd(lag1) ## [1] 36463.76 The standard deviation for home sales gets significantly condensed from 60654.34 to 36463.76 in the creation of the lag variable. PCP To get a more dramatic view of the influence of high-valued or low-valued neighbors on the spatial lag, we use a parallel coordinates plot. This will be done using GGally, which is an R package that builds off of the ggplot2 package for some more complicated plots, ie the parallel coordinates plot and the scatter plot matrix. We won’t go in depth about the different options this package offers in this notebook, but if interested, check out GGally documentation. To make the pcp with just the two variables, we will need to put them in a separate matrix, or data frame. We use cbind to do this, as with the coordinates. From there we use ggparcoord from the GGally package. The necessary inputs are the data(pcp.vars) and the scale parameter. We use `scale = “globalminmax” to keep the same price values as our lag variable and sale price variable. The default option scales it down, which is not what we are looking for in this case. sale.price &lt;- clev.points$sale_price pcp.vars &lt;- cbind(sale.price,lag1) ggparcoord(data = pcp.vars,scale =&quot;globalminmax&quot;) + ggtitle(&quot;Parallel Coordinate Plot&quot;) Spatial lag as a sum of neighboring values We can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights. We start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure. binary.weights &lt;- lapply(k6, function(x) 0*x + 1) k6.weights2 &lt;- nb2listw(k6, glist = binary.weights, style = &quot;B&quot;) With the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and sale price data. lag2 &lt;- lag.listw(k6.weights2, clev.points$sale_price) df &lt;- df %&gt;% mutate(lag2 = lag2) kable(head(df)) sale_price lag1 lag2 235500 79858.33 479150 65000 90608.33 543650 92000 71041.67 426250 5000 91375.00 548250 116250 72833.33 437000 120000 74041.67 444250 Spatial window average The spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights. To begin we assign k6 to a new variable because we will directly alter its structure to add the diagonal elements. k6a &lt;- k6 To add the diagonal element to the neighbors list, we just need include.self from spdep. include.self(k6a) ## Neighbour list object: ## Number of regions: 205 ## Number of nonzero links: 1435 ## Percentage nonzero weights: 3.414634 ## Average number of links: 7 ## Non-symmetric neighbours list Now we obtain weights with nb2listw k6.weights3 &lt;- nb2listw(k6a) Lastly, we just need to create the lag variable from our weight structure and sale_price variable. lag3 &lt;- lag.listw(k6.weights3, clev.points$sale_price) df &lt;- df %&gt;% mutate(lag3 = lag3) kable(head(df)) sale_price lag1 lag2 lag3 235500 79858.33 479150 79858.33 65000 90608.33 543650 90608.33 92000 71041.67 426250 71041.67 5000 91375.00 548250 91375.00 116250 72833.33 437000 72833.33 120000 74041.67 444250 74041.67 Spatial window sum The spatial window sum is the counter part of the window average, but without using row-standardized weights. To do this we assign binary weights to the neighbor structure that includes the diagonal element. binary.weights2 &lt;- lapply(k6a, function(x) 0*x + 1) binary.weights2[1] ## [[1]] ## [1] 1 1 1 1 1 1 Again we use nb2listw and glist = to explicitly assign weight values. k6.weights4 &lt;- nb2listw(k6a, glist = binary.weights2, style = &quot;B&quot;) With our new weight structure, we can compute the lag variable with lag.listw. lag4 &lt;- lag.listw(k6.weights4, clev.points$sale_price) df &lt;- df %&gt;% mutate(lag4 = lag4) kable(head(df)) sale_price lag1 lag2 lag3 lag4 235500 79858.33 479150 79858.33 479150 65000 90608.33 543650 90608.33 543650 92000 71041.67 426250 71041.67 426250 5000 91375.00 548250 91375.00 548250 116250 72833.33 437000 72833.33 437000 120000 74041.67 444250 74041.67 444250 Spatially lagged variables from inverse distance weights Principle The spatial lag operation can also be applied using spatial weights calculated from the inverse distance between observations. As mentioned in our earlier discussion, the magnitude of these weights is highly scale dependent (depends on the scale of the coordinates). An uncritical application of a spatial lag operation with these weights can easily result in non-sensical values. More specifically, since the resulting weights can take on very small values, the spatial lag could end up being essentially zero. Formally, the spatial lag operation amounts to a weighted average of the neighboring values, with the inverse distance function as the weights: \\[[Wy]_i = \\sum_jy_j/d_{ij}^\\alpha\\] where in our implementation, \\(\\alpha\\) is either 1 or 2. In the latter case (a so-called gravity model weight), the spatial lag is sometimes referred to as a potential in geo-marketing analyses. It is a measure of how accessible location i is to opportunities located in the neighboring locations (as defined by the weights). Default approach inverse distance for k = 6 nearest neighbors To calculate the inverse distances, we must first compute the distances between each neighbor. This is done with nbdists. k6.distances &lt;- nbdists(k6,coords) Now we apply the function 1/x to each element of the distance structure, which gives us the inverse distances. invd1a &lt;- lapply(k6.distances, function(x) (1/x)) invd1a[1] ## [[1]] ## [1] 0.0025963168 0.0003318873 0.0008618371 0.0005379514 0.0003959608 ## [6] 0.0003074062 Now we explicitly assign the inverse distance weight with glist = in the nb2listw function. invd.weights &lt;- nb2listw(k6,glist = invd1a,style = &quot;B&quot;) The default case with row-standardized weights off and no diagonal elements is put into the variable IDRSND. The lag values here are very different from the sale price because the weight values are relatively small, the largest being .0026 idrsnd &lt;- lag.listw(invd.weights, clev.points$sale_price) kable(head(idrsnd)) x 397.5210 805.2941 359.3411 966.1106 444.5489 361.3308 idnrwd &lt;- clev.points$sale_price + idrsnd kable(head(idnrwd)) x 235897.521 65805.294 92359.341 5966.111 116694.549 120361.331 The above would be the result for when the diagonal is included. This amounts to the original sale price being added to the the lag value, which is the equivalent of: \\[[W_y]_i = y_i + \\Sigma_j\\frac{y_i}{d_{ij}^\\alpha}\\] Spatial lags with row-standardized inverse distance weights row_stand &lt;- function(x){ row_sum &lt;- sum(x) scalar &lt;- 1/row_sum x * scalar } row.standw &lt;- lapply(invd1a, row_stand) df &lt;- data.frame(inverse_distance = invd1a[[1]], row_stand_invd = row.standw[[1]]) kable(df) inverse_distance row_stand_invd 0.0025963 0.5160269 0.0003319 0.0659637 0.0008618 0.1712931 0.0005380 0.1069197 0.0003960 0.0786986 0.0003074 0.0610980 sum(row.standw[[1]]) ## [1] 1 invd.weights2 &lt;- nb2listw(k6,glist = row.standw,style = &quot;B&quot;) invd.weights2$weights[[1]] ## [1] 0.51602688 0.06596374 0.17129309 0.10691969 0.07869856 0.06109804 lag6 &lt;- lag.listw(invd.weights2, clev.points$sale_price) head(lag6) ## [1] 79008.67 159632.92 70716.60 101979.20 45949.65 65842.27 Spatially lagged variables from kernel weights Spatially lagged variables can also be computed from kernel weights. However, in this instance, only one of the options with respect to row-standardization and diagonal weights makes sense. Since the kernel weights are the result of a specific kernel function, they should not be altered. Also, each kernel function results in a specific value for the diagonal element, which should not be changed either. As a result, the only viable option to create spatially lagged variables based on kernel weights is to have no row-standardization and have the diagonal elements included. k6a.distances &lt;- nbdists(k6a,coords) Here we apply the Epanechnikov kernal weight function to the distance structure, but calculate z as a variable bandwidth rather than the critical threshold. For the variable bandwidth, we take the maximum distance between a location and it’s set of 6 neighbors. Then this is the distance used to calculate the weights for the neighbors of that location. for (i in 1:length(k6a.distances)){ maxk6 &lt;- max(k6a.distances[[i]]) bandwidth &lt;- maxk6 new_row &lt;- .75*(1-(k6a.distances[[i]] / bandwidth)^2) k6a.distances[[i]] &lt;- new_row } k6a.distances[[1]] ## [1] 0.7394859 0.1065641 0.6545807 0.5050934 0.2979544 0.0000000 Now we create the weights structure with nb2listw, and directly assign the weights values to the neighbors structure with glist =. epan.weights &lt;- nb2listw(k6a,glist = k6a.distances,style = &quot;B&quot;) epan.weights$weights[[1]] ## [1] 0.7394859 0.1065641 0.6545807 0.5050934 0.2979544 0.0000000 Lastly, we use lag.listw to create the lag variable from our Epanechnikov kernal weights, then take a brief look at the resulting lag variable, epalag. epalag &lt;- lag.listw(epan.weights, clev.points$sale_price) df &lt;- data.frame(clev.points$sale_price,epalag) kable(head(df)) clev.points.sale_price epalag 235500 210839.48 65000 320223.68 92000 62295.77 5000 272183.78 116250 189258.60 120000 99958.83 \\[[W_y]_i = \\Sigma_jK_{ij}y_j\\] Spatial rate smoothing Principle A spatial rate smoother is a special case of a nonparameteric rate estimator, based on the principle of locally weighted estimation. Rather than applying a local average to the rate itself, as in an application of a spatial window average, the weighted average is applied separately to the numerator and denominator. The spatially smoothed rate for a given location i is then given as: \\[\\pi_i = \\frac{\\Sigma_{j=1}^nw_{ij}O_j} {\\Sigma_{j=1}^nw_{ij}P_j}\\] where \\(O_j\\) is the event count in location j, \\(P_j\\) is the population at risk, and \\(w_{ij}\\) are spatial weights (typically with \\(w_{ii} = 0\\), i.e. including the diagonal) Different smoothers are obtained for different spatial definitions of neighbors and/or different weights applied to those neighbors (e.g., contiguity weights, inverse distance weights, or kernel weights). The window average is not applied to the rate itself, but it is computed separately for the numerator and denominator. The simplest case boils down to applying the idea of a spatial window sum to the numerator and denominator (i.e., with binary spatial weights in both, and including the diagonal term): \\[\\pi_i = \\frac{O_i + \\Sigma_{j=1}^nO_j} {O_i +\\Sigma_{j=1}^nP_j}\\] \\(\\pi_i = \\frac{O_i+\\Sigma_{j=1}^J_iO_j}{O_i + \\Sigma_{j=1}^J_iP_j}\\) where \\(J_i\\) is a reference set (neighbors) for observation i. In practice, this is achieved by using binary spatial weights for both numerator and denominator, and including the diagonal in both terms, as in the expression above. A map of spatially smoothed rates tends to emphasize broad spatial trends and is useful for identifying general features of the data. However, it is not useful for the analysis of spatial autocorrelation, since the smoothed rates are autocorrelated by construction. It is also not very useful for identifying outlying observations, since the values portrayed are really regional averages and not specific to an individual location. By construction, the values shown for individual locations are determined by both the events and the population sizes of adjoining spatial units, which can lead to misleading impressions. Often, inverse distance weights are applied to both the numerator and denominator Preliminaries We return to the rate smoothing examples using the Ohio county lung cancer data. Therefore, we need to close the current project and load the ohlung data set. Next, we need to create the spatial weights files we will use if we don’t have them already stored in a project file. In order to make sure that some smoothing will occur, we take a fairly wide definition of neighbors. Specifically, we will create a second order queen contiguity, inclusive of first order neighbors, inverse distance weights based on knn = 10 nearest neighbor weights, and Epanechnikov kernel weights, using the same 10 nearest neighbors and with the kernel applied to the diagonal (its value will be 0.75). To proceed, we will be using procedures outlined in earlier notebooks to create the weights. The process will be less in depth than the ones in the distance and contiguity weight notebooks, but will be enough to make and review the process of making the weights. geodaData All of the data for the R notebooks is available in the geodaData package. We loaded the library earlier, now to access the individual data sets, we use the double colon notation. This works similar to to accessing a variable with $, in that a drop down menu will appear with a list of the datasets included in the package. For this notebook, we use ohio_lung. Otherwise, to get the data for this notebook, you will need to go to Ohio Lung Cancer The download format is a zipfile, so you will need to unzip it by double clicking on the file in your file finder. From there move the resulting folder titled: nyc into your working directory to continue. Once that is done, you can use the sf function: st_read() to read the shapefile into your R environment. ohio_lung &lt;- geodaData::ohio_lung Creating a Crude Rate In addition to the spatial weights, we also need an example for crude rates. If not already saved in the data set, we compute the crude rate for lung cancer among white females in 68. To get the rate we make a new variable, LRATE and calculated the rate through bivariate operation. We multiply the ratio by 10,000 to the crude rate of lung cancer per 10,000 white women in 1968. ohio_lung$LRATE &lt;- ohio_lung$LFW68 / ohio_lung$POPFW68 * 10000 tm_shape(ohio_lung) + tm_fill(&quot;LRATE&quot;,palette = &quot;-RdBu&quot;,style =&quot;sd&quot;,title = &quot;Standard Deviation: LRATE&quot;) + tm_borders() + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) Creating the Weights Queen Contiguity To create the queen contiguity weights, we will use the spdep package and sf package to make a neighbors list with both first and second order neighbors included. To begin we create a queen contiguity function with the corresponding pattern. We use the st_relate function to accomplish this. For a more in depth explanation of the specified pattern, check the contiguity based weights notebook. st_queen &lt;- function(a, b = a) st_relate(a, b, pattern = &quot;F***T****&quot;) Here we define a function needed to convert the resulting data structure from the st_relate function. Specifically from sgbp to nb. It’s fairly straightforward, we just change the class name, transfer the attributes, and check for observations without a neighbor. as.nb.sgbp &lt;- function(x, ...) { attrs &lt;- attributes(x) x &lt;- lapply(x, function(i) { if(length(i) == 0L) 0L else i } ) attributes(x) &lt;- attrs class(x) &lt;- &quot;nb&quot; x } Now we create a comprehnsive neighbors list with both first and second order neighbors. To start, we use the two functions we created to get a 1st order neighbors list, then we use nblag to get the first and second order neighbors. The next step is reoragnize the data structre so that first and second order neighbors are not separated, this is done with nblag_cumul. With these steps, we have a neighbors list of first and second order neighbors. queen.sgbp &lt;- st_queen(ohio_lung) queen.nb &lt;- as.nb.sgbp(queen.sgbp) second.order.queen &lt;- nblag(queen.nb, 2) second.order.queen.cumul &lt;- nblag_cumul(second.order.queen) Here we add the diagonal elements to the neighbors list because we will need them later on in computations. include.self(second.order.queen.cumul) ## Neighbour list object: ## Number of regions: 88 ## Number of nonzero links: 1376 ## Percentage nonzero weights: 17.7686 ## Average number of links: 15.63636 Now, we just use the nb2listw function to convert the neighbors list to a weights structure, which is row-standardized by default. queen.weights &lt;- nb2listw(second.order.queen.cumul) queen.weights$weights[[1]] ## [1] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 K-nearest neighbors Here we will make the inverse distance weights for the 10th-nearest neighbors. To start, we need the coordinates in a separate data structure before we can proceed with the neighbor and distance calculations. We use the same method to extract these values as the distance-band weights notebook. longitude &lt;- map_dbl(ohio_lung$geometry, ~st_centroid(.x)[[1]]) latitude &lt;- map_dbl(ohio_lung$geometry, ~st_centroid(.x)[[2]]) coords &lt;- cbind(longitude, latitude) From here we just use knearneigh and knn2nb to get our neighbors structure. k10 &lt;- knn2nb(knearneigh(coords, k = 10)) Now we need the distances between each observation and its neighbors. Having this will allow us to calculate the inverse later and assign these values as weights when converting the neighbors structure to a weights structure. k10.dists &lt;- nbdists(k10,coords) Here we apply a function to the distances of the 10th nearest neighbors and change the scale by an order of 10,000. The purpose of changing the scale is to get weight values that are not approximently zero. invdk10 &lt;- lapply(k10.dists, function(x) (1/(x/10000))) invdk10[1] ## [[1]] ## [1] 0.2537074 0.1289088 0.2415022 0.3469379 0.1935753 0.2138694 0.1299890 ## [8] 0.1415348 0.1449763 0.1291574 With the nb2listw function, we can assign non-default weights to the new weights structure by means of the glist argument. Here we just assign the scaled inverse weights we created above. k10.weights &lt;- nb2listw(k10,glist = invdk10, style = &quot;B&quot;) k10.weights$weights[[1]] ## [1] 0.2537074 0.1289088 0.2415022 0.3469379 0.1935753 0.2138694 0.1299890 ## [8] 0.1415348 0.1449763 0.1291574 Epanechnikov kernel For the Epanechnikov kernal weights, things are a bit more complicated, as we need to add in diagonal elements. This is best done with neighbors list. We start with the 10-nearest neighbors and assign to a new object because we will be altering the structure to get the resulting weights. This gives us the distances between neighbors in the same structure as the neighbors list. k10.distances &lt;- nbdists(k10,coords) This four loop gives us the epanechnikov weights in the the distance data structure that we computed above. for (i in 1:length(k10.distances)){ maxk10 &lt;- max(k10.distances[[i]]) bandwidth &lt;- maxk10 new_row &lt;- .75*(1-(k10.distances[[i]] / bandwidth)^2) k10.distances[[i]] &lt;- new_row } k10.distances[[1]] ## [1] 0.556375699 0.000000000 0.536310124 0.646456537 0.417397027 0.477523632 ## [7] 0.012413252 0.127843776 0.157031189 0.002884936 Now we can assign our epanechnikov weight values as weights with the nb2listw function throught the glist = parameter. epan.weights &lt;- nb2listw(k10,glist = k10.distances,style = &quot;B&quot;) epan.weights$weights[[1]] ## [1] 0.556375699 0.000000000 0.536310124 0.646456537 0.417397027 0.477523632 ## [7] 0.012413252 0.127843776 0.157031189 0.002884936 Simple window average of rates This is an illustration of how not to proceed with the spatial rate calculation. Here we use our crude rate and create a spatial lag variable directly from the second order queen contiguity weights. ohio_lung$W_LRATE &lt;- lag.listw(queen.weights,ohio_lung$LRATE) We then plot W_LRATE with a standard deviation map. tm_shape(ohio_lung) + tm_fill(&quot;W_LRATE&quot;,palette = &quot;-RdBu&quot;,style =&quot;sd&quot;,title = &quot;Standard Deviation: W_LRATE&quot;) + tm_borders() + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) Characteristic of the spatial averaging, several larger groupings of similarly classified observations appear. The pattern is quite different from that displayed for the crude rate. For example, the upper outliers have disappeared, and there is now one new lower outlier. Spatially Smoothed Rates As mentioned, applying the spatial averaging directly to the crude rates is not the proper way to operate. This approach ignores the differences between the populations of the different counties and the associated variance instability of the rates. To create the spatially smoothed rate, we need to make lag variables of the event variable and base variable, then divide them to get our rate. In GeoDa we just select our variables and the software does it for us, in R we need some extra steps. To get this rate we create lag variables for both population and event count with lag.listw. To get the rate, we then divide the event count lag variable by the the population lag variable. lag_lfw68 &lt;- lag.listw(queen.weights,ohio_lung$LFW68) lag_popfw68 &lt;- lag.listw(queen.weights,ohio_lung$POPFW68) ohio_lung$R_SPAT_R &lt;- lag_lfw68 / lag_popfw68 We map the results with tmap functions. tm_shape(ohio_lung) + tm_fill(&quot;R_SPAT_R&quot;,palette = &quot;-RdBu&quot;,style =&quot;sd&quot;,title = &quot;SRS-Smoothed LFW68 over POPFW68&quot;) + tm_borders() + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) We rescale the spatial rate variable by 10,000 to make it comparable to W_LRATE. Then we add all of the rate to a data frame with data.frame and take a brief side by side look with head and kable. ohio_lung$R_SPAT_RT &lt;- ohio_lung$R_SPAT_R * 10000 df &lt;- data.frame(LRATE = ohio_lung$LRATE,W_LRATE = ohio_lung$W_LRATE,R_SPAT_RT= ohio_lung$R_SPAT_RT) kable(head(df)) LRATE W_LRATE R_SPAT_RT 1.2266817 0.6859424 0.6787945 0.6113217 0.7315576 0.9793153 0.9636697 0.9364070 1.0274976 0.0000000 0.9414058 1.0600851 1.1988872 1.1059237 0.9678452 0.0000000 1.0709655 1.1050227 To get a more quantifiable comparason the table above, we can make a scatterplot with gplot2 functions. We will not go into depth on the different options here, but will create a basic scatterplot of W_LRATE and R_SPAT_RT. geom_point add the points to the plot, and geom_smooth(method=lm) add a linear regression line. ggplot(data=ohio_lung,aes(x=W_LRATE,y=R_SPAT_RT)) + geom_point() + geom_smooth(method=lm) lm calculates the associated regression statistics with the scatterplot above. We then use tidy and kable to display these statistics in a neat table. lmfit &lt;- lm(R_SPAT_RT ~ W_LRATE,data = ohio_lung) kable(tidy(lmfit)) term estimate std.error statistic p.value (Intercept) 0.4213586 0.0613816 6.864573 0 W_LRATE 0.6421174 0.0586340 10.951276 0 Inverse Distance Smoothed Rate For the inverse distance weights, we proceed with a different approach. The k10.weights do not include diagonal elements, so we will need to address this in our computation of the rate. We begin with the lag variables for population and event count with our inverse distance weights. To add the diagonal element in our rate calculation, we add the event count variable to lag event count variable and do the same with population before dividing to compute the rate. This works because the diagonal element would be assigned 1 in the weight structure and as a result would just add the original sale proce from the location to the lag sale price, if originally included in the weight’s structure. lag_lfw68 &lt;- lag.listw(k10.weights,ohio_lung$LFW68) lag_popfw68 &lt;- lag.listw(k10.weights,ohio_lung$POPFW68) ohio_lung$ID_RATE &lt;- (lag_lfw68 + ohio_lung$LFW68)/ (lag_popfw68 + ohio_lung$POPFW68) * 10000 tm_shape(ohio_lung) + tm_fill(&quot;ID_RATE&quot;,palette = &quot;-RdBu&quot;,style =&quot;sd&quot;,title = &quot;Standard Deviation IDRATE&quot;) + tm_borders() + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) Kernal Smoothed Rate We follow the same process for the kernal weights calculation as the inverse distance, except rescale the diagonal element by .75 as done in epanechnikov kernal function. lag_lfw68 &lt;- lag.listw(epan.weights,ohio_lung$LFW68) lag_popfw68 &lt;- lag.listw(epan.weights,ohio_lung$POPFW68) ohio_lung$KERN_RATE &lt;- (lag_lfw68 + .75 * ohio_lung$LFW68)/ (lag_popfw68 + .75 * ohio_lung$POPFW68) * 10000 ohio_lung$KERN_RATE &lt;- lag_lfw68 / lag_popfw68 * 10000 tm_shape(ohio_lung) + tm_fill(&quot;KERN_RATE&quot;,palette = &quot;-RdBu&quot;,style =&quot;sd&quot;,title = &quot;Standard Deviation: KERN_RATE&quot;) + tm_borders() + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) df &lt;- df %&gt;% mutate(ID_RATE = ohio_lung$ID_RATE, KERN_RATE = ohio_lung$KERN_RATE) kable(head(df)) LRATE W_LRATE R_SPAT_RT ID_RATE KERN_RATE 1.2266817 0.6859424 0.6787945 1.1004473 0.4102768 0.6113217 0.7315576 0.9793153 0.9106611 0.9880350 0.9636697 0.9364070 1.0274976 1.0250171 1.0632788 0.0000000 0.9414058 1.0600851 0.7552625 0.9394942 1.1988872 1.1059237 0.9678452 1.1197279 0.8257277 0.0000000 1.0709655 1.1050227 0.8583623 1.0165777 It is important to keep in mind that both the inverse distance and kernel weights spatially smoothed rates are based on a particular trade-off between the value at the location and its neighbors. This trade-off depends critically on the distance metric used in the calculations (or, on the scale in which the coordinates are expressed). There is no right answer, and a thorough sensitivity analysis is advised. For example, we can observe that the three spatially smoothed maps point to some elevated rates in the south of the state, but the extent of the respective regions and the counties on which they are centered differ slightly. Also, the general regional patterns are roughly the same, but there are important differences in terms of the specific counties affected. Spatial Empirical Bayes smoothing Principle The second option for spatial rate smoothing is Spatial Empirical Bayes. This operates in the same way as the standard Empirical Bayes smoother (covered in the rate mapping Chapter), except that the reference rate is computed for a spatial window for each individual observation, rather than taking the same overall reference rate for all. This only works well for larger data sets, when the window (as defined by the spatial weights) is large enough to allow for effective smoothing. Similar to the standard EB principle, a reference rate (or prior) is computed. However, here, this rate is estimated from the spatial window surrounding a given observation, consisting of the observation and its neighbors. The neighbors are defined by the non-zero elements in the row of the spatial weight matrix (i.e., the spatial weights are treated as binary). Formally, the reference mean for location i is then: \\[\\mu_i = \\frac{\\Sigma_jw_{ij}O_j}{\\Sigma_jw_{ij}P_j}\\] with \\(w_{ij}\\) as binary spatial weights, and \\(w_{ii}=1\\) The local estimate of the prior variance follows the same logic as for EB, but replacing the population and rates by their local counterparts: \\[\\sigma_i^2 = \\frac{\\Sigma_jw_{ij}[P_j(r_i - \\mu_i)^2]}{\\Sigma_jw_{ij}P_j} - \\frac{\\mu_i}{\\Sigma_jw_{ij}P_i/(k_i +1)}\\] Note that the average population in the second term pertains to all locations within the window, therefore, this is divided by \\(k_i +1\\)(with \\(k_i\\) as the number of neighbors of i). As in the case of the standard EB rate, it is quite possible (and quite common) to obtain a negative estimate for the local variance, in which case it is set to zero. The spatial EB smoothed rate is computed as a weighted average of the crude rate and the prior, in the same manner as for the standard EB rate For reference the EB rate in this case is as denoted below: \\[w_i = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\mu_i / P_i}\\] \\[\\pi_i^{EB}= w_ir_i + (1-w_i)\\theta\\] Spatial EB rate smoother The spatial Empirical Bayes is included in the spdep package. It even has a geoda = that calculates the rate in the same manner as GeoDa. To do this, we use the EBlocal function from spdep and input the two variables with a neighbors list. We then set geoda = to be TRUE. eb_risk &lt;- EBlocal(ohio_lung$LFW68, ohio_lung$POPFW68, second.order.queen.cumul, geoda = TRUE) With our Empirical Bayes calculation, we can now examine it with a choropleth map. Before we proceed, we add eb_risk to the data frame. We need to select the est not the raw to get the correct figures. ohio_lung &lt;- ohio_lung %&gt;% mutate(eb_risk = eb_risk$est) tm_shape(ohio_lung) + tm_fill(&quot;eb_risk&quot;,palette = &quot;-RdBu&quot;,style =&quot;sd&quot;,title = &quot;SEBS-Smoothed LFW68 over POPFW68&quot;) + tm_borders() + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) We do the same for the 10-nearest neighbors: eb_risk_k10 &lt;- EBlocal(ohio_lung$LFW68, ohio_lung$POPFW68, k10, geoda = TRUE) ohio_lung &lt;- ohio_lung %&gt;% mutate(eb_risk_k10 = eb_risk_k10$est) tm_shape(ohio_lung) + tm_fill(&quot;eb_risk_k10&quot;,palette = &quot;-RdBu&quot;,style =&quot;sd&quot;,title = &quot;SEBS-Smoothed LFW68 over POPFW68&quot;) + tm_borders() + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) As for the spatial rate smoothing approaches, a careful sensitivity analysis is in order. The results depend considerably on the range used for the reference rate. The larger the range, the more the smoothed rates will be similar to the global EB rate. For a narrow range, the estimate of the local variance will often result in a zero imputed value, which makes the EB approach less meaningful. Use setwd(directorypath) to specify the working directory.↩︎ Use install.packages(packagename).↩︎ "],["global-spatial-autocorrelation-1.html", "Chapter 10 Global Spatial Autocorrelation 1 Introduction 10.1 Preliminaries 10.2 The Moran Scatter Plot 10.3 Spatial Correlogram", " Chapter 10 Global Spatial Autocorrelation 1 Introduction This notebook cover the functionality of the Global Spatial Autocorrelation 1 section of the GeoDa workbook. We refer to that document for details on the methodology, references, etc. The goal of these notes is to approximate as closely as possible the operations carried out using GeoDa by means of a range of R packages. The notes are written with R beginners in mind, more seasoned R users can probably skip most of the comments on data structures and other R particulars. Also, as always in R, there are typically several ways to achieve a specific objective, so what is shown here is just one way that works, but there often are others (that may even be more elegant, work faster, or scale better). For this notebook, we use Cleveland house price data. Our goal in this lab is show how to assign spatial weights based on different distance functions. 10.0.1 Objectives After completing the notebook, you should know how to carry out the following tasks: Visualize Moran’s I with a Moran scatterplot Carry out inference using the permutation approach Make analysis reproducible with the random seed Create a LOWESS smooth of the Moran scatter plot Conduct a Chow test with the Moran scatterplot Analyze the range of spatial autocorrelation by means of a spatial correlogram 10.0.1.1 R Packages used sf: To read in the shapefile and make queen contiguity weights spdep: To create k-nearest neighbors and distance-band neighbors, calculate distances between neighbors, convert to a weights structure, and coercion methods to sparse matrices. ggplot2: To make customized plots such as a Moran’s I scatter plot and spatial correlogram. Hmisc: To get LOWESS smoother functionality in ggplot2. robustHD: To compute standarized scores for variables and lag variables. in construction of a Moran’s I scatterplot deldir: To create voronoi polygons. tidyverse: For basic data frame manipulation. gap: To compute chow test statistics. gridExtra: To pack multiple plots into one, mainly used to construct the spatial correlogram geodaData: To access the data for this tutorial 10.0.1.2 R Commands used Below follows a list of the commands used in this notebook. For further details and a comprehensive list of options, please consult the R documentation. Base R: install.packages, library, setwd, rep, sd, mean, summary, attributes, lapply, class, length, which, data.frame, plot sf: st_read, st_relate, st_as_sf spdep: dnearneigh, nb2listw, sp.correlogram, Szero ggplot2: ggplot, geom_smooth, geom_point, xlim, ylim, geom_hline, geom_vline, geom_line, ggtitle, scale_x_continous Hmisc: stat_plsmo robustHD: standardized deldir: deldir tidyverse: filter gap: chow.test gridExtra: grid.arrange 10.1 Preliminaries Before starting, make sure to have the latest version of R and of packages that are compiled for the matching version of R (this document was created using R 3.5.1 of 2018-07-02). Also, optionally, set a working directory, even though we will not actually be saving any files.27 10.1.1 Load packages First, we load all the required packages using the library command. If you don’t have some of these in your system, make sure to install them first as well as their dependencies.28 You will get an error message if something is missing. If needed, just install the missing piece and everything will work after that. library(sf) library(spdep) library(ggplot2) library(deldir) library(robustHD) library(Hmisc) library(tidyverse) library(gap) library(gridExtra) library(geodaData) geodaData website All of the data for the R notebooks is available in the geodaData package. We loaded the library earlier, now to access the individual data sets, we use the double colon notation. This works similar to to accessing a variable with $, in that a drop down menu will appear with a list of the datasets included in the package. For this notebook, we use clev_pts. Otherwise, to get the data for this notebook, you will and to go to Cleveland Home Sales The download format is a zipfile, so you will need to unzip it by double clicking on the file in your file finder. From there move the resulting folder titled: nyc into your working directory to continue. Once that is done, you can use the sf function: st_read() to read the shapefile into your R environment. clev.points &lt;- geodaData::clev_pts 10.1.2 Making the weights The weights used for this notebook are queen contiguity, based off voronoi polygons contructed from the point data for this notebook. In order to make the weights, we must first construct voronoi polygons from the cleveland point data. There are a number of ways to do this. We will be using the deldir package as a starting point. We will need to convert the result from the deldir package to class sf, which we have been working with throughout the notebooks. The only function we need from deldir is deldir, which outputs a data structure with voronoi polygons. The only inputs needed are a vector of the x coordinates and a vector of the y coordinates. The base R plot function can give us a preliminary look at the voronoi polygons. We will need a few additional parameters other than vtess, so the plot is legitable. Set wlines = \"tess\", wpoints = \"none\" and lty = 1. vtess &lt;- deldir(clev.points$x, clev.points$y) plot(vtess, wlines=&quot;tess&quot;, wpoints=&quot;none&quot;, lty=1) This function will be used to convert the deldir voronoi polygons to sp, where we can easily convert them to sf. We are not going to cover the individual steps of this function because it is outside the scope of these notebooks. The important thing to note here is that this function converts deldir voronoi polygons to sp. voronoipolygons = function(thiess) { w = tile.list(thiess) polys = vector(mode=&#39;list&#39;, length=length(w)) for (i in seq(along=polys)) { pcrds = cbind(w[[i]]$x, w[[i]]$y) pcrds = rbind(pcrds, pcrds[1,]) polys[[i]] = Polygons(list(Polygon(pcrds)), ID=as.character(i)) } SP = SpatialPolygons(polys) voronoi = SpatialPolygonsDataFrame(SP, data=data.frame(dummy = seq(length(SP)), row.names=sapply(slot(SP, &#39;polygons&#39;), function(x) slot(x, &#39;ID&#39;)))) } Again, we can use the base R plot function to take a look at voronoi polygons. Now that they are class sp, we don’t need the extra parameters in plot. v &lt;- voronoipolygons(vtess) plot(v) With the voronoi polygons in sp class, we can easily conert them to sf with st_as_sf. Again we use the base R plot function to view the polygons. vtess.sf &lt;- st_as_sf(v) plot(vtess.sf$geometry) Now that we have the voronoi polygons as an sf object, we can use the queen contiguity process outline in the Contiguity Based Weights notebook. We will briefly cover each step of the process. For more indepth information please see the Contiguity Based Weights notebook. To start we create a function for queen contiguity, which is just st_relate with the specified pattern for queen contiguity which is F***T**** st_queen &lt;- function(a, b = a) st_relate(a, b, pattern = &quot;F***T****&quot;) We apply the queen contiguity function to the voronoi polygons and see that the class of the output is sgbp. This structure is close to the nb structure, but has a few differences that we will need to correct to use the rest of spdep functionality. queen.sgbp &lt;- st_queen(vtess.sf) class(queen.sgbp) ## [1] &quot;sgbp&quot; &quot;list&quot; This function converts type sgbp to nb. It is covered in more depth in the Contiguity Based Weight notebook. In short, it explicitly changes the name of the class and deals with the observations that have no neighbors. as.nb.sgbp &lt;- function(x, ...) { attrs &lt;- attributes(x) x &lt;- lapply(x, function(i) { if(length(i) == 0L) 0L else i } ) attributes(x) &lt;- attrs class(x) &lt;- &quot;nb&quot; x } We use as.nb.sgbp to convert neighbor types and then check the class with class. queen.nb &lt;- as.nb.sgbp(queen.sgbp) class(queen.nb) ## [1] &quot;nb&quot; To go from neighbors object to weights object, we use nb2listw, with default parameters, we will get row standardized weights. queen.weights &lt;- nb2listw(queen.nb) 10.2 The Moran Scatter Plot 10.2.1 Concept 10.2.1.1 Moran’s I Moran’s I statistic is arguably the most commonly used indicator of global spatial autocorrelation. It was initially suggested by Moran (1948), and popularized through the classic work on spatial autocorrelation by Cliff and Ord (1973). In essence, it is a cross-product statistic between a variable and its spatial lag, with the variable expressed in deviations from its mean. For an observation at location i, this is expressed as \\(z_i = x_i - \\bar{x}\\), where \\(\\bar{x}\\)is the mean of variable x. Moran’s I statistic is then: \\[I = \\frac{\\Sigma_i\\Sigma_jw_{ij}z_iz_j/S_0}{\\Sigma_iz_i^2/n}\\] with \\(w_{ij}\\) as elements of the spatial weights matrix, \\(S_0 = \\Sigma_i\\Sigma_jw_{ij}\\) as the sum of all of the weights and n as the number of observations. 10.2.1.2 Permutation inference Inference for Moran’s I is based on a null hypothesis of spatial randomness. The distribution of the statistic under the null can be derived using either an assumption of normality (independent normal random variates), or so-called randomization (i.e., each value is equally likely to occur at any location). An alternative to an analytical derivation is a computational approach based on permutation. This calculates a reference distribution for the statistic under the null hypothesis of spatial randomness by randomly permuting the observed values over the locations. The statistic is computed for each of these randomly reshuffled data sets, which yields a reference distribution. This distribution is then used to calculate a so-called pseudo p-value. This is found as \\[p = \\frac{R +1}{M+1}\\] where R is the number of times the computed Moran’s I from the spatial random data sets (the permuted data sets) is equal to or more extreme than the observed statistic. M equals the number of permutations. The latter is typically taken as 99, 999, etc., to yield nicely rounded pseudo p-values. The pseudo p-value is only a summary of the results from the reference distribution and should not be interpreted as an analytical p-value. Most importantly, it should be kept in mind that the extent of significance is determined in part by the number of random pemutations. More precisely, a result that has a p-value of 0.01 with 99 permutations is not necessarily more significant than a result with a p-value of 0.001 with 999 permutations. 10.2.1.3 Moran scatter plot The Moran scatter plot, first outlined in Anselin (1996), consists of a plot with the spatially lagged variable on the y-axis and the original variable on the x-axis. The slope of the linear fit to the scatter plot equals Moran’s I. We consider a variable z, given in deviations from the mean. With row-standardized weights, the sum of all the weights (S0) equals the number of obsevations (n). As a result, the expression for Moran’s I simplifies to: \\[I= \\frac{\\Sigma_i\\Sigma_jw_{ij}z_iz_j}{\\Sigma_iz_i^2} = \\frac{\\Sigma_i(z_i*\\Sigma_jw_{ij}z_j)}{\\Sigma_iz_i^2}\\] Upon closer examination, this turns out to be the slope of a regression of \\(\\Sigma_jw_{ij}z_i\\) on \\(z_i\\) This is the principle underlying the Moran scatter plot. An important aspect of the visualization in the Moran scatter plot is the classification of the nature of spatial autocorrelation into four categories. Since the plot is centered on the mean (of zero), all points to the right of the mean have \\(z_i&gt;0\\) and all points to the left have \\(z_i&lt;0\\). We refer to these values respectively as high and low, in the limited sense of higher or lower than average. Similarly, we can classify the values for the spatial lag above and below the mean as high and low. The scatter plot is then easily decomposed into four quadrants. The upper-right quadrant and the lower-left quadrant correspond with positive spatial autocorrelation (similar values at neighboring locations). We refer to them as respectively high-high and low-low spatial autocorrelation. In contrast, the lower-right and upper-left quadrant correspond to negative spatial autocorrelation (dissimilar values at neighboring locations). We refer to them as respectively high-low and low-high spatial autocorrelation. The classification of the spatial autocorrelation into four types begins to make the connection between global and local spatial autocorrelation. However, it is important to keep in mind that the classification as such does not imply significance. This is further explored in our discussion of local indicators of spatial association (LISA). 10.2.2 Creating a Moran scatter plot Before we create the Moran’s I scatterplot, we will get the statistic using moran from spdep. For this function, we need the a variable to do the Moran’s I on, a weights structure, the length of the dataset, and then Szero of the queen weights, which calculates the constants needed for tests of spatial autocorrelation. moran &lt;- moran(clev.points$sale_price, queen.weights, length(queen.nb), Szero(queen.weights)) moran$I ## [1] 0.2810649 We get a value of .281, which is the Moran’s I statistic, which also corresponds to the slope of the Moran’s I scatter plot. In creating the Moran’s I scatterplot, we will need to to create a lag variable of sale price from our queen weights. This is just done with the function lag.listw, which takes a weights structure and a variable of equal length to create a lag variable from. clev.points$lagged_sale_price &lt;- lag.listw(queen.weights,clev.points$sale_price) clev.points$lagged_sale_price ## [1] 83287.500 112912.500 80178.571 108550.000 58375.000 96816.667 ## [7] 115600.000 41503.571 59732.143 80025.000 48785.714 110750.000 ## [13] 103000.000 72300.000 113050.000 141918.167 112900.000 55142.857 ## [19] 140594.143 88871.429 213734.833 176844.143 201950.000 134800.000 ## [25] 67350.000 46371.375 88125.000 137600.000 107625.000 28357.143 ## [31] 26218.750 9025.000 18291.667 25950.000 7002.667 41863.300 ## [37] 28187.500 17300.000 10250.000 25333.333 18418.750 21437.500 ## [43] 26430.000 20771.000 60539.000 43412.500 102080.375 24936.667 ## [49] 29619.143 28094.200 16657.000 18691.500 22193.625 21575.667 ## [55] 17804.571 128700.000 35710.400 45935.250 36411.500 42500.000 ## [61] 82133.333 71680.000 81100.000 55733.333 69066.667 66600.000 ## [67] 39300.000 59750.000 58000.000 48625.000 45593.750 15441.667 ## [73] 47174.750 33416.667 35150.000 34083.500 51762.500 36304.750 ## [79] 31783.600 12908.000 14676.600 15166.667 22483.333 19541.667 ## [85] 20426.600 21100.000 36212.000 20900.000 32775.000 29946.600 ## [91] 44350.333 24431.286 21279.750 62380.000 21683.667 25914.900 ## [97] 42369.667 8481.000 10904.800 13657.000 13830.000 14412.250 ## [103] 24264.143 14487.333 32960.571 12226.800 8708.500 12655.667 ## [109] 15714.286 20383.333 21151.667 10258.571 18451.667 131651.500 ## [115] 20420.000 182072.714 140650.000 22034.000 43550.000 53100.000 ## [121] 41488.889 26500.000 13095.714 8764.444 9208.667 6156.667 ## [127] 11761.333 5529.500 7409.600 40510.000 34388.286 9250.000 ## [133] 9128.000 15470.000 12699.714 18842.000 14440.000 10125.000 ## [139] 17967.500 9500.000 30250.000 17400.000 11174.000 13248.833 ## [145] 9068.750 25633.333 7248.500 66900.000 62812.500 67125.000 ## [151] 88250.000 28300.000 78900.000 71093.750 9750.000 14085.714 ## [157] 12350.000 15216.667 25670.000 26750.000 57414.286 21920.000 ## [163] 43500.000 16912.500 19062.500 15666.667 65044.444 20583.333 ## [169] 11358.333 34816.667 34750.000 39360.000 29092.857 24422.222 ## [175] 23360.000 42685.714 29080.000 16983.333 21666.667 31960.000 ## [181] 24400.000 7242.857 16180.000 28216.667 9633.333 32742.857 ## [187] 28833.333 21360.000 29391.667 24414.286 34720.000 14083.333 ## [193] 14860.000 17916.667 23133.333 27850.000 18370.000 16975.000 ## [199] 45980.000 36700.000 33583.333 29750.000 35416.667 28614.286 ## [205] 22000.000 We need standardized values for both the lag variable and the sale price variable to build the Moran’I scatterplot. Standardized values are just z scores for each observation(\\(z_i =\\frac{ x_i -\\mu}{\\sqrt{Var(x)}}\\)). To get the standardized values, we will use standardize from the robustHD package. We could very easily calculate these with Base R vectorized operations, but it is faster to just use a package function. clev.points$standardized_sale_price &lt;- standardize(clev.points$sale_price) clev.points$standardized_lag_sale_price &lt;- standardize(clev.points$lagged_sale_price) To construct the moran’s I scatterplot, we will use ggplot2 for more aesthically pleasing plots. We will not go into too much depth on the options available for for these plots, but for more information, check out ggplot2 documentation. In addition, The first Exploratory Data Analysis notebook is also a good resource to look at. To make the Moran’s I scatterplot, we make a scatterplot in ggplot2 with the standardized sale price values as the x axis and the standardized lag sale price variable as the y axis. We use geom_point to add the points to the plot. geom_smooth adds a regression line. The default option is a loess smooth line, we specify the method = lm to get a standard linear regression line . We add dotted lines at the x and y axis to separate the 4 types of spatial autocorrelation. We do this with geom_hline for the x axis and geom_vline for the y axis. To get the speciifcations of the scatterplot to better match up with GeoDa, we set the x and y scale limits to -10 and 10. ggplot(data = clev.points, aes(x=standardized_sale_price, y = standardized_lag_sale_price)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_hline(yintercept = 0, lty = 2) + geom_vline(xintercept = 0, lty = 2) + xlim(-10,10) + ylim(-10,10) + ggtitle(&quot;Moran scatter plot sale price&quot;) 10.2.2.1 Interpretation We can see that the shape of the point cloud is determined by the presence of several outliers on the high end (e.g., larger than three standard deviational units from the mean). One observation, with a sales price of 527,409 (compared to the median sales prices of 20,000), is as large as 8 standard deviational units above the mean. On the lower end of the spectrum (to the left of the dashed line in the middle that represents the mean), there is much less spread in the house prices, and those points end up bunched together. By eliminating some of the outliers, one may be able to see more detail for the remaining observations, but we will not pursue that here. 10.2.2.2 Assessing significance We have an estimate of the Moran’s I statistic, but no information on the significance of that statistic. This can be obtained by constructing a distribution by means of random assignment. In order to do this, we first choose the number of permutations desired, which will directly affect the minimum pseudo p-value we can obtain for the test statistic. In the case of 999 permutation the minimum p-value would be .001, which would mean none of the sample distribution statistics are as extreme or more extreme than the test statistic. 10.2.2.2.1 Replicability - the random seed To faciliate replication, it is best to set a seed for the random number generator. The one used in GeoDa is 123456789, so we will demonstrate how to set the seed here. It is just set.seed and the desired seed number as an input. set.seed(123456789) 10.2.2.3 Reference distribution To make the reference distribution, we will need to draw 999 randomized samples of the housing point data of the same size as the number of observations in the housing point data. This random sample will allow us to assign the values to random locations, which will give us a spatially random distribution. To get to this point, we will build up in steps in order to better understand the process. We start by taking one random sample of our points with the base R sample function. We choose the same size as the number of sale price data observations to make a spatially randomized vector of our sale price data. The point of this is to randomly assign the housing prices to the voronoi polygons, then to compute the Moran’s I statistic for the spatially random assignment based off the original weights structure. draw &lt;- sample(clev.points$sale_price, size = length(clev.points$sale_price)) draw ## [1] 28000 82500 14500 135000 29150 13000 25250 34750 30000 14000 ## [11] 3500 45000 76000 8000 25000 14500 15000 7500 5000 26400 ## [21] 40000 65000 25750 15000 5800 4924 9000 41000 65000 527409 ## [31] 29900 5968 11000 33500 6800 64000 41500 21199 8000 15000 ## [41] 47000 7900 8400 8000 16000 20000 9000 15500 4149 17500 ## [51] 22000 19000 27600 72900 5200 4775 4500 9500 32500 5000 ## [61] 122500 7000 26000 2910 138500 165000 2500 92000 15000 40000 ## [71] 75000 32500 5000 7000 11750 20000 49000 41069 49000 15000 ## [81] 6370 15000 26000 38200 15000 85000 62000 120000 65000 70000 ## [91] 25000 84900 65000 28500 1300 6483 155000 73400 13000 4000 ## [101] 305000 230000 11750 100000 15000 21119 75000 42000 20000 3000 ## [111] 76000 12000 20000 11500 5000 10000 25750 116250 13100 7000 ## [121] 48000 16000 10000 1448 29000 167000 5000 25000 8500 6300 ## [131] 8600 27750 235500 81500 78000 228000 69000 3000 32500 20000 ## [141] 4250 169500 10000 10000 8000 109900 20000 19000 7000 56900 ## [151] 47000 103000 1049 5500 10500 26000 25000 125000 23000 24900 ## [161] 48500 13000 16500 5500 17000 11500 10000 12000 45900 51000 ## [171] 131650 24800 11582 6375 73000 11750 16900 172500 7000 2100 ## [181] 48500 16500 15000 42500 275000 18000 15834 89500 6500 27000 ## [191] 8000 23500 5000 48900 5000 19971 63000 10000 72000 38500 ## [201] 7000 12000 68900 34000 5000 Now we can begin to calculate the Moran’s I statistic by first calculating the spatial lag variable based on our queen weights and the spatially random sample. lag1 &lt;- lag.listw(queen.weights,draw) We can get the Moran’s I statistic by regressing the standardized values of the spatial lag variable on the standardized values of the random draw. We can get the standardized value with the standardize function. The summary function allows us to see a summary of the regression statistics. lmfit &lt;- lm(standardize(lag1) ~ standardize(draw)) summary(lmfit) ## ## Call: ## lm(formula = standardize(lag1) ~ standardize(draw)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.2872 -0.6826 -0.2114 0.3316 4.4708 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.236e-16 6.997e-02 0.000 1.000 ## standardize(draw) 3.735e-02 7.014e-02 0.532 0.595 ## ## Residual standard error: 1.002 on 203 degrees of freedom ## Multiple R-squared: 0.001395, Adjusted R-squared: -0.003525 ## F-statistic: 0.2835 on 1 and 203 DF, p-value: 0.595 The slope here is the estimate for standardize(draw). This value is fairly close to zero as the randomization process makes makes the draw spatially random. To build our distribution, we will need to repeat this process many times over. We can accomplish this by way of a for loop. We will need somewhere to store our Moran’s I result for each iteration. To do this we will make an empty vector of a length corresponding to our desired number of permutations. randomized_moran &lt;- rep(NA, 999) The process here is the same as the one followed for one draw, but here we use the for loop to get 999 iterations and store the resulting Moran’s I values in the vector that we created above. First we do the random sample with the sample function. Then we make a lag variable based upon the random draw and our queen weights. Next we run the regression with the lm function between the stanardized values of the lag variable and random draw variable. Lastly, we extract the slope coefficient which is our Moran’s I statistic and store it in index i. Each iteration of the loop will store the value at the subsequent index ie 1, then 2, then 3, and so on. for(i in 1:999){ draw &lt;- sample(clev.points$sale_price, size = length(clev.points$sale_price)) lag &lt;- lag.listw(queen.weights,draw) lmfit &lt;- lm(standardize(lag) ~ standardize(draw)) randomized_moran[i] &lt;- lmfit$coefficients[2] } We can obtain summary statistics of our distribution with summary summary(randomized_moran) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.26210 -0.08531 -0.02554 -0.01655 0.04257 0.31351 sd(randomized_moran) ## [1] 0.09375454 Now to get the p value, we will check the number of samples that had higher Moran’s I statistic than the observed value. To do this, we use the base R which function to get a vector of the indices at which the conditional is TRUE. We then get the length of the vector with length. length(which(randomized_moran &gt; .281)) ## [1] 4 Since the result is 1, there is only 1 value in all of the permutations that is higher than the test statistic. This means that the p value is .002, \\(\\frac{1 + R}{1 + M}\\), where R = 1 and M = 999. There are a number of ways we can visualize the distribution that we just constructed in R. We will use ggplot2 to do these visualizations because it looks much better than base R visualizations. To start, we convert our vector with the randomized moran’s I values into a data frame, so we can use ggplot2 functions. For this, we just use the data.frame function with the vector of randomized moran’s I values as an argument and then assign a name for the column, which is just moran in this case. The first option is a density plot. This requires the standard ggplot function with aes containing the x axis. Additionally we need geom_density. We use geom_vline to plot the mean of the distribution and our observed statistic. df &lt;- data.frame(moran = randomized_moran) ggplot(data = df,aes(x=moran)) + geom_density() + geom_vline(xintercept = moran[[1]], col = &quot;green&quot;) + geom_vline(xintercept = mean(randomized_moran), col = &quot;blue&quot;) The next option is a histogram. The only difference here is that we use geom_histogram instead of `geom_density. ggplot(data = df, aes(x=moran)) + geom_histogram() + geom_vline(xintercept = moran[[1]], col = &quot;green&quot;) + geom_vline(xintercept = mean(randomized_moran), col = &quot;blue&quot;) #### LOWESS smoother The LOWESS smoother is not implemented directly in ggplot2, but can be found in an add-on package. We use the Hmisc package to add this functionality to the ggplot2 plots. To add the smoother to our Moran’s I scatter plot, we use the stat_plsmo from the Hmisc package. The default span for GeoDa is .2 so we will set the span = parameter to .2. With the LOWESS smoother, we can see potential structural breaks in the pattern of spatial autocorrelation. For example some parts of the data, the curve may be very steep, and positive, indicating strong spatial autocorrelation, whereas in other parts, it could be flat, indicating no spatial autocorrelation. ggplot(data = clev.points, aes(x=standardized_sale_price, y = standardized_lag_sale_price)) + geom_point() + stat_plsmo(span = .2, color = &quot;blue&quot;) + geom_hline(yintercept = 0, lty = 2) + geom_vline(xintercept = 0, lty = 2) + xlim(-10,10) + ylim(-10,10) + ggtitle(&quot;LOWESS smooth of Moran Scatterplot&quot;) 10.2.2.4 Chow test Moran’s I scatterplot The Chow test is a statistical test of whether or not the coeffiecients of two different linear regressions are equal. In the case of the Moran’s I scatterplot, it is just the slope of the regression line and the intercepts, since it is a simple linear regression. The brushing operation in GeoDa is fairly difficult to implement in R, but we can do a less interactive version. First we must consider which criteria we want to select points on. This could be anything from its location to other characteristics in the data. In our case we will do it based on location. As an approximation for the midpoint of the set of points, we take the the mean of the x and y coordinates. From there we assign “Select” to the points in the bottom left quadrant and “Rest” to the rest of the points by way of the if_else function. This function takes a conditional, a result to assign in the case where the conditional is TRUE, and a result to assign when the conditional is FALSE. In our case it is “Select” and “Rest”. mid_x &lt;- mean(clev.points$x) mid_y &lt;- mean(clev.points$y) clev.points&lt;- clev.points %&gt;% mutate(bottom_left = if_else((x &lt; mid_x &amp; y &lt; mid_y),&quot;Select&quot;, &quot;Rest&quot;)) Before we run the chow test, we will visualize the difference in slopes of the selected data, non-selected data and the aggregate data. With ggplot2, we can accomplish this by setting categorical colors based whether or not an observation is “Selected” or “Rest”. To do this, we specify aes(color = bottom_left) in both geom_point and geom_smooth. This will give us colored points and regression lines for “Selected” and “Rest”. Then to get blue and red colors, we use scale_color_manual. For this plot, we do not set x and y limits because the -10 to 10 speciifcation is too dificult to see the differences in the regression lines. ggplot(clev.points, aes(x=standardized_sale_price,y=standardized_lag_sale_price)) + geom_point(aes(color=bottom_left)) + geom_smooth(aes(color=bottom_left), method = lm, se = FALSE) + geom_smooth(method=lm,se = FALSE, color = &quot;black&quot;) + scale_color_manual(values=c(&quot;blue&quot;,&quot;red&quot;)) + labs(color=&quot;Selection&quot;) + geom_hline(yintercept = 0, lty = 2) + geom_vline(xintercept = 0, lty = 2) + ggtitle(&quot;Chow test Moran Scatterplot&quot;) To perform the chow test, we need two separate data frames as inputs for the function. To get the two data frames, we use the tidyverse filter function. This function filter out observations based on a conditional. TRUE values for the conditional remain in the data frame while FALSE values are filtered out. clev.select &lt;- clev.points %&gt;% filter(bottom_left == &quot;Select&quot;) clev.rest &lt;- clev.points %&gt;% filter(bottom_left == &quot;Rest&quot;) Now we use the base R lm function to run separate regressions on the standardized lag variable and standardized sale price variable. reg.select &lt;- lm(standardized_lag_sale_price~standardized_sale_price, data=clev.select) reg.rest &lt;- lm(standardized_lag_sale_price~standardized_sale_price, data=clev.rest) Now we use the summary function on each regression object to get summary statistics of the residuals, the regression coefficients and and their respective standard errors, the R squared values, and the F statistic. summary(reg.select) ## ## Call: ## lm(formula = standardized_lag_sale_price ~ standardized_sale_price, ## data = clev.select) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.32083 -0.26382 -0.08486 0.25641 2.38954 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.15667 0.06062 -2.584 0.0117 * ## standardized_sale_price 0.51228 0.11818 4.335 4.53e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.492 on 74 degrees of freedom ## Multiple R-squared: 0.2025, Adjusted R-squared: 0.1917 ## F-statistic: 18.79 on 1 and 74 DF, p-value: 4.534e-05 summary(reg.rest) ## ## Call: ## lm(formula = standardized_lag_sale_price ~ standardized_sale_price, ## data = clev.rest) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.2276 -0.6161 -0.3336 0.2146 4.5370 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.10241 0.09335 1.097 0.275 ## standardized_sale_price 0.42072 0.07816 5.383 3.42e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.056 on 127 degrees of freedom ## Multiple R-squared: 0.1858, Adjusted R-squared: 0.1793 ## F-statistic: 28.97 on 1 and 127 DF, p-value: 3.418e-07 We see that the slopes vary by about .08 and the intercepts vary by .25 To run the chow test, we need 4 inputs for chow.test. We need the two standardized variables from the the “Select” data frame: clev.select and the two standardized variables from the the “Rest” data frame: clev.rest. chow &lt;- chow.test(clev.select$standardized_lag_sale_price, clev.select$standardized_sale_price, clev.rest$standardized_lag_sale_price, clev.rest$standardized_sale_price) chow ## F value d.f.1 d.f.2 P value ## 2.2974700 2.0000000 201.0000000 0.1031467 With a p-value of .103 we do not have significant evidence to conclude that the slopes of the two regressions are different under a standard alpha level of .05. 10.3 Spatial Correlogram 10.3.1 Concept A non-parametric spatial correlogram is an alternative measure of global spatial autocorrelation that does not rely on the specification of a spatial weights matrix. Instead, a local regression is fit to the covariances or correlations computed for all pairs of observations as a function of the distance between them (for example, as outlined in Bjornstad and Falck 2001). With standardized variables z, this boils down to a local regression: \\[z_iz_j = f(d_{ij}) + u\\] where \\(d_{ij}\\) is the distance between a pair of locations i - j, u is an error term and f is a non-parametric function to be determined from the data. Typically, the latter is a LOWESS or kernel regression. 10.3.2 Creating a spatial correlogram In GeoDa, creating a spatial correlogram is much more straight forward than in R. The process in r requires us to start with the sale price points, then to create a neighbors structure base on the distance breaks desired for the correlogram. To start, we use cbind to put the x and y coordinates together for use in the distance based neighbor functions of spdep. coords &lt;- cbind(clev.points$x, clev.points$y) Now we create distance based neighbors coordinate matrix and lower distance bound and an upper distance bound, which is used to define neighbors. We use dnearneigh to create the distance band neighbors. For more in depth information on distance based neighbors, please see the Disatnce Based Weights notebook. We use a distance of 4823.27 to emulate the first example in the GeoDa workbook. dist.band.nb &lt;- dnearneigh(coords,0,4823.27) Using the spdep function `sp.correlogram, we can get measures of spatial autocorrelation for an input number of lag orders. We can then use the base R plotting function to get a look at the autocorrelation values for each lag order. sp &lt;- sp.correlogram(dist.band.nb, clev.points$sale_price, order = 10, method = &quot;I&quot;,style = &quot;W&quot;, randomisation = TRUE, spChk = NULL, zero.policy = TRUE) plot(sp) To get a better looking plot, we can extract the moran’s I values and put them into a data frame, so we can use ggplot2 plotting functionality. morans &lt;- sp$res[,1] df &lt;- data.frame(Morans_I = morans,lags = 1:10 ) ggplot(data = df, aes(x=lags,y=Morans_I)) + geom_point() + geom_smooth(col = &quot;purple&quot;, se = FALSE) + geom_hline(yintercept = 0) + ylim(-.5,.5) To get closer to the GeoDa correlogram plotting functionality, we can convert lags to euclidean distance. df$euclidean_distance &lt;- df$lags * 4823.3 ggplot(data = df, aes(x=euclidean_distance,y=Morans_I)) + geom_point() + geom_smooth(col = &quot;purple&quot;, se = FALSE) + geom_hline(yintercept = 0) + ylim(-.5,.5) + scale_x_continuous(breaks = df$euclidean_distance) The spatial correlogram can be paired with a bar chart that shows the number of neighbor pairs for each lag order. To get this information, we will need to work outside the spdep package and compute them ourselves. To begin, we set up an empty vector to store the pair numbers. pairs &lt;- rep(NA, 10) Here we run dnearneigh on each interval of euclidean distance that corresponds to a lag in 1 to 10. To get the number of pairs for each lag order, we simply sum up the cardinality of the neighbor structure per each lag order and then divide it by two because this sum gives the total number of neighbors and the total number of pairs will be half this number. for (i in 1:10){ nb &lt;- dnearneigh(coords, (i - 1) * 4823.28, i * 4823.28) pairs[i] &lt;- sum(card(nb)) / 2 } Now we create a data frame from the two vectors we create with the lag order values and associated euclidean distance values. df &lt;- data.frame(lag_order = 1:10, auto_corr = morans, num_pairs = pairs) df$euclidean_distance &lt;- df$lag_order * 4823 Here we create two different plots, one is a histogram with the number of pairs in each bin, the other is the spatial correlogram p1 &lt;- ggplot(data = df, aes(x = euclidean_distance,y = auto_corr)) + geom_point() + geom_smooth(col = &quot;purple&quot;, se = FALSE) + geom_hline(yintercept = 0) + ylim(-1,1) + scale_x_continuous(breaks = df$euclidean_distance) p2 &lt;- ggplot(data = df, aes(x=euclidean_distance,y = num_pairs, fill = as.factor(euclidean_distance))) + geom_bar(stat = &quot;identity&quot;) + scale_fill_brewer(palette = &quot;Paired&quot;) + theme(legend.position = &quot;none&quot;) + geom_text(aes(label=num_pairs), position = position_dodge(width = .9), vjust=-.25) + ylim(0, 1.2 * max(pairs)) + scale_x_continuous(breaks = df$euclidean_distance) p1 p2 Using grid.arrange from the gridExtra package, we can combine the two plots into one image. grid.arrange(p1,p2,ncol = 1) Following the same process outlined above, we can make a function that constructs the correlogram based on the desired lag order, distance band, variable, and coordinates. geoda_correlogram &lt;- function(lag.order, distance, var,coords){ # Funtion that outputs a spatial correlogram with a bar plot of neighbor pairs # Inputs: # lag.order: The desired number of lag orders to be included in the plot # distance: The desired distance band for the lags # var: A variable to analyze the spatial autocorelation # coords: A matrix of coordinates of the same length as var # creating vectors to store autocorrelation values and number of pairs pairs &lt;- rep(NA, lag.order) #loop to calculate number of pairs for each lag order for(i in 1:lag.order) { nb &lt;- dnearneigh(coords, (i-1) * distance, i * distance) pairs[i] &lt;- sum(card(nb)) / 2 } # Computing spatial autocorrelation nb1 &lt;- dnearneigh(coords, 0 , distance) sp &lt;- sp.correlogram(nb1, var, order = lag.order, method = &quot;I&quot;, style = &quot;W&quot;, randomisation = FALSE, spChk = NULL, zero.policy = TRUE) # Putting the lag orders, autocorrelation, pairs and distance into a dataframe df &lt;- data.frame(lag = 1:lag.order, num_pairs = pairs, auto_corr = sp$res[,1]) df$euclidean_distance &lt;- df$lag * round(distance, digits = 0) # Making plots p1 &lt;- ggplot(data = df, aes(x = euclidean_distance,y = auto_corr)) + geom_point() + geom_smooth(col = &quot;purple&quot;, se = FALSE) + geom_hline(yintercept = 0) + ylim(-1,1) + scale_x_continuous(breaks = df$euclidean_distance) p2 &lt;- ggplot(data = df, aes(x=euclidean_distance,y=num_pairs, fill = as.factor(euclidean_distance))) + geom_bar(stat= &quot;identity&quot;) + scale_fill_brewer(palette = &quot;Paired&quot;) + theme(legend.position = &quot;none&quot;) + geom_text(aes(label=num_pairs), position = position_dodge(width = .9), vjust=-.25) + ylim(0, 1.2 * max(pairs)) + scale_x_continuous(breaks = df$euclidean_distance) grid.arrange(p1,p2,ncol =1) } geoda_correlogram(10, 4823.3, clev.points$sale_price, coords) 10.3.3 Interpretation The top of the above graph is the actual correlogram. This depicts how spatial autocorrelation changes with distance. The first dot correpsonds with distances between 0 and 4823 feet. The dashed line indicates a spatial autocorrelation of 0. The autocorrelation starts positive and then fluctates above and below the dashed line. Use setwd(directorypath) to specify the working directory.↩︎ Use install.packages(packagename).↩︎ "],["global-spatial-autocorrelation-2.html", "Chapter 11 Global Spatial Autocorrelation 2 Introduction 11.1 Preliminaries 11.2 Bivariate spatial correlation - a Word of Caution 11.3 Bivariate Moran Scatter Plot 11.4 Differential Moran Scatter Plot 11.5 Moran Scatter Plot for EB Rates", " Chapter 11 Global Spatial Autocorrelation 2 Introduction This notebook cover the functionality of the Global Spatial Autocorrelation 2 section of the GeoDa workbook. We refer to that document for details on the methodology, references, etc. The goal of these notes is to approximate as closely as possible the operations carried out using GeoDa by means of a range of R packages. The notes are written with R beginners in mind, more seasoned R users can probably skip most of the comments on data structures and other R particulars. Also, as always in R, there are typically several ways to achieve a specific objective, so what is shown here is just one way that works, but there often are others (that may even be more elegant, work faster, or scale better). For this notebook, we use Cleveland house price data. Our goal in this lab is show how to assign spatial weights based on different distance functions. 11.0.1 Objectives After completing the notebook, you should know how to carry out the following tasks: Visualize bivariate spatial correlation Assess different aspects of bivariate spatial correlation Visualize bivariate spatial correlation among time-differenced values Correct Moran’s I for the variance instability in rates 11.0.1.1 R Packages used sf: To read in the shapefile and make queen contiguity weights spdep: To create spatial weights structure from neighbors structure. ggplot2: To make customized plots such as a bivariate Moran’s I scatter plot robustHD: To compute standarized scores for variables and lag variables. in construction of a Moran’s I scatterplot 11.0.1.2 R Commands used Below follows a list of the commands used in this notebook. For further details and a comprehensive list of options, please consult the R documentation. Base R: install.packages, library, setwd, summary, attributes, lapply, class, length, lm sf: st_read, st_relate, st_crs, st_transform spdep: nb2listw, lag.listw ggplot2: ggplot, geom_smooth, geom_point, xlim, ylim, geom_hline, geom_vline, ggtitle robustHD: standardized 11.1 Preliminaries Before starting, make sure to have the latest version of R and of packages that are compiled for the matching version of R (this document was created using R 3.5.1 of 2018-07-02). Also, optionally, set a working directory, even though we will not actually be saving any files.29 11.1.1 Load packages First, we load all the required packages using the library command. If you don’t have some of these in your system, make sure to install them first as well as their dependencies.30 You will get an error message if something is missing. If needed, just install the missing piece and everything will work after that. library(sf) library(spdep) library(ggplot2) library(robustHD) Obtaining the Data from the GeoDa website To get the data for this notebook, you will and to go to US County Homocide Data The download format is a zipfile, so you will need to unzip it by double clicking on the file in your file finder. From there move the resulting folder titled: nyc into your working directory to continue. Once that is done, you can use the sf function: st_read() to read the shapefile into your R environment. counties &lt;- st_read(&quot;natregimes/natregimes.shp&quot;) ## Reading layer `natregimes&#39; from data source ## `/Users/runner/work/handsonspatialdata/handsonspatialdata/natregimes/natregimes.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 3085 features and 73 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -124.7314 ymin: 24.95597 xmax: -66.96985 ymax: 49.37173 ## Geodetic CRS: WGS 84 11.1.2 Creating the weights The sf neighbor functions assume that coordinates for the data are planar. Since The current projection is that for latitude and longitude, it is best to tranform the coordinates to a planar projection for accuracy. To check the current projection, we use st_crs. To transform the projection, we use st_transform and the epsg number of our desired projection. For more indepth information on this, please check out the Spatial Data Handling notebook. st_crs(counties) ## Coordinate Reference System: ## User input: WGS 84 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;latitude&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;longitude&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]] counties &lt;- st_transform(counties, crs = &quot;ESRI:102003&quot;) To start we create a function for queen contiguity, which is just st_relate with the specified pattern for queen contiguity which is F***T**** st_queen &lt;- function(a, b = a) st_relate(a, b, pattern = &quot;F***T****&quot;) We apply the queen contiguity function to the voronoi polygons and see that the class of the output is sgbp. This structure is close to the nb structure, but has a few difference that we will need to correct to use the rest of spdep functionality. queen.sgbp &lt;- st_queen(counties) class(queen.sgbp) ## [1] &quot;sgbp&quot; &quot;list&quot; This function converts type sgbp to nb. It is covered in more depth in the Contiguity Based Weight notebook. In short, it explicitly changes the name of the class and deals with the observations that have no neighbors. as.nb.sgbp &lt;- function(x, ...) { attrs &lt;- attributes(x) x &lt;- lapply(x, function(i) { if(length(i) == 0L) 0L else i } ) attributes(x) &lt;- attrs class(x) &lt;- &quot;nb&quot; x } queen.nb &lt;- as.nb.sgbp(queen.sgbp) To go from neighbors object to weights object, we use nb2listw, with default parameters, we will get row standardized weights. queen.weights &lt;- nb2listw(queen.nb) 11.2 Bivariate spatial correlation - a Word of Caution The concept of bivariate spatial correlation is complex and often misinterpreted. It is typically considered to the correlation between one variable and the spatial lag of another variable, as originally implemented in the precursor of GeoDa. However this does not take into account the inherent correlation between the two variables. More precisely, the bivariate spatial correlation is between \\(x_i\\) and \\(\\Sigma_jw_{ij}y_j\\), but does not take into account the correlation between \\(x_i\\) and \\(y_i\\), i.e. between two variables at the same location. As a result, this statistic is often interpreted incorrectly, as it may overestimate the spatial aspect of the correlation that instead may be due mostly to in-place correlation. Below, we provide a more in-depth assessment of the different aspects of bivariate spatial and non-spatial association, but first we turn to the original concept of a bivariate Moran scatter plot. 11.3 Bivariate Moran Scatter Plot 11.3.1 Concept In its initial conceptualization, as mentioned above, a bivariate Moran scatter plot extends the idea of a Moran scatter plot with a variable on the x-axis and its spatial lag on a y-axis to a bivariate context. The fundamental difference is that in the bivariate case the spatial lag pertains to a different variable. In essence, this notion of bivariate spatial correlation measures the degree to which the value for a given variable at a location is correlated with its neighbors for a different variable. As in the univariate Moran scatter plot, the interest is in the slope of the linear fit. This yields a Moran’s I-like statistic as: \\[I_B=\\frac{\\Sigma_i(\\Sigma_jw_{ij}*x_i)}{\\Sigma_ix_i^2}\\] or, the slope of a regression of Wy on x. As before, all variables are expressed in standardized form, such that their means are zero and their variance one. In addition, the spatial weights are row-standardized. Note that, unlike in the univariate autocorrelation case, the regression of x on Wy also yields an unbiased estimate of the slope, providing an alternative perspective on bivariate spatial correlation. A special case of bivariate spatial autocorrelation is when the variable is measured at two points in time, say \\(z_{i,t}\\) and \\(z_{i,t-1}\\). The statistic then pertains to the extent to which the value obsereved at a location at a given time, is correlated with its value at neighboring locations at a different point in time. The natural interpretation of this concept is to relate \\(z_{i,t}\\) to \\(\\Sigma_jw_{ij}z_{j,t-1}\\), i.e. the correlation between a value at t and its neighbors in a previous time period: \\[I_T=\\frac{\\Sigma_i(\\Sigma_jw_{ij}z_{j,t-1}*z_{i,t})}{\\Sigma_iz_{i,t}^2}\\] i.e., the effect neighbors in t-1 on the present value. Alternatively, and maybe less intuitively, one can relate the value at a previous time period \\(z_{t-1}\\) to its neighbors in the future, \\(\\Sigma_jw_{ij}z_t\\) as: \\[I_T=\\frac{\\Sigma_i(\\Sigma_jw_{ij}z_{j,t}*z_{i,t-1)}}{\\Sigma_iz_{i,t-1}^2}\\] i.e., the effect of a location at t-1 on its neighbors in the future. While formally correct, this may not be a proper interpretation of the dynamics involved. In fact, the notion of spatial correlation pertains to the effect of neighbors on a central location, not the other way around. While the Moran scatter plot seems to reverse this logic, this is purely a formalism, without any consequences in the univariate case. However, when relating the slope in the scatter plot to the dynamics of a process, this should be interpreted with caution. A possible source of confusion is that the proper regression specification for a dynamic process would be as: \\[z_{i,t}=\\beta_1\\Sigma_jw_{ij}z_{j,t-1} +u_i\\] with \\(u_i\\) as the usual error term, and not as: \\[z_{i,t-1}= \\beta_2\\Sigma_jw_{ij}z_{j,t} + u_i\\] which would have the future predicting the past. This contrasts with the linear regression specification used (purely formally) to estimate the bivariate Moran’s I, for example: \\[\\Sigma_jw_{ij}z_{j,t-1} = \\beta_3z_{i,t} + u_i\\] In terms of the interpretation of a dynamic process, only \\(\\beta_1\\) has intuitive appeal. However, in terms of measuring the degree of spatial correlation between past neighbors and a current value, as measured by a Moran’s I coefficient, \\(\\beta_3\\) is the correct interpretation. As mentioned earlier, a fundamental difference with the univariate case is that the spatially lagged variable is no longer endogenous, so both specifications can be estimated by means of ordinary least squares. In the univariate case, only the specification with the spatially lagged variable on the left hand side yields a valid estimate. As a result, for the univariate Moran’s I, there is no ambiguity about which variables should be on the x-axis and y-axis. Inference is again based on a permutation approach, but with an important difference. Since the interest focuses on the bivariate spatial association, the values for x are fixed at their locations, and only the values for y are randomly permuted. In the usual manner, this yields a reference distribution for the statistic under the null hypothesis that the spatial arrangement of the y values is random. As mentioned, it is important to keep in mind that since the focus is on the correlation between the x value at i and the y values at the neighboring locations, the correlation between x and y at location i is ignored. 11.3.2 Creating a bivariate Moran scatter plot To create the bivariate Moran scatterplot, we will need to standardize the variables beofre proceeding. This is simply, putting the variables in terms of z scores. To do this, we use standardize from the robustHD package. counties$hr80st &lt;- standardize(counties$HR80) counties$hr90st &lt;- standardize(counties$HR90) Now that we have standardized the variables, we can create the lag variable for homocide rates in 1980. This is done with lag.listw from spdep. For more a more indepth coverage of this function , please see the Application of Spatial Weights notebook. counties$whr80st &lt;- lag.listw(queen.weights,counties$hr80st) In constructing the majority of our plots, we use ggplot2 because it makes aesthically pleasing and functional plots. The purpose of this tutorial is not to teach ggplot2, but to use it in creqating specializzed plots such as the bivariate Moran plot. For indepth coverage of basic ggplot2 functionality please see the Exploratory Data Analysis notebooks or ggplot2 documentation. To make the bivariate Moran, we make a scattplot of the lag variable for 1980 homocide rates vs the homocide rate for 1990. In GeoDa the lag variable and standardization are done automatically and all you have to do is select the desired variables. The custom specifications use for this plot are the dashed lines at the x and y axis and the simple regression line. geom_vline and geom_hline. ggplot(data = counties, aes(x=hr90st,y=whr80st)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_hline(yintercept = 0, lty = 2) + geom_vline(xintercept = 0, lty =2) + xlim(-12,12) + ylim(-12,12) + ggtitle(&quot;Bivariate Moran&#39;s I, hr(90) and lagged hr(80) &quot;) To get the summary statistics and bivariate Moran’s I statistic, we will run a rgeression separate from the plot. For this regression, we use the same variables has the bivariate Moran plot in the base R lm function. To view the results, we use the summary command. The Moran’s I statistic is the slope of the regression line, which is the coefficient of hr90st in the summary results lmfit &lt;- lm(whr80st ~ hr90st, data = counties) summary(lmfit) ## ## Call: ## lm(formula = whr80st ~ hr90st, data = counties) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9290 -0.4171 -0.1024 0.3595 2.6279 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.004734 0.010694 0.443 0.658 ## hr90st 0.359668 0.010696 33.628 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.594 on 3083 degrees of freedom ## Multiple R-squared: 0.2684, Adjusted R-squared: 0.2681 ## F-statistic: 1131 on 1 and 3083 DF, p-value: &lt; 2.2e-16 11.3.3 A closer look at bivariate spatial correlation In contrast to the univariate Moran scatter plot, where the interpretation of the linear fit is unequivocably Moran’s I, there is no such clarity in the bivariate case. In addition to the interpretation offered above, which is a traditional Moran’s I-like coefficient, there are at least four additional perspectives that are relevant. We consider each in turn. 11.3.3.1 Serial (temporal) correlation To examine the temporal correlation of homocide rates, we plot the standardized variable for each time period. This gives us a sense of the temporal autocorrelation of homocide rates for a lag order of 1. ggplot(data = counties, aes(x = hr80st, y = hr90st)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_hline(yintercept = 0, lty = 2) + geom_vline(xintercept = 0, lty =2) Again we us the lm function and summary function to get a sense of the relationship between the variables. With a highly significant slope of .552, strong temporal correlation is suggested. lmfit &lt;- lm(hr90st ~ hr80st, data = counties) summary(lmfit) ## ## Call: ## lm(formula = hr90st ~ hr80st, data = counties) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.8479 -0.3701 -0.1609 0.2806 10.3772 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.018e-16 1.501e-02 0.00 1 ## hr80st 5.526e-01 1.501e-02 36.82 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8336 on 3083 degrees of freedom ## Multiple R-squared: 0.3054, Adjusted R-squared: 0.3052 ## F-statistic: 1356 on 1 and 3083 DF, p-value: &lt; 2.2e-16 11.3.3.2 Serial (temporal) correlation between spatial lags Next we run a regression of the spatial lags of homocide rates for both time periods. This will be a regression of \\(\\Sigma_jw_{ij}y_{j,t}\\) on \\(\\Sigma_jw_{ij}y_{j,t-1}\\). To begin, wewill need to create the lag variable for 1990. counties$whr90st &lt;- lag.listw(queen.weights, counties$hr90st) To make this plot, we just plot the lag variable for 1990 against the one for 1980. ggplot(data = counties, aes(x= whr80st, y = whr90st)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_hline(yintercept = 0, lty = 2) + geom_vline(xintercept = 0, lty =2) Again the slope is highly signiificcant with a positive slope of .801, even strong than the actual variable’s temporal correlation. lmfit &lt;- lm(whr90st ~ whr80st, data = counties) summary(lmfit) ## ## Call: ## lm(formula = whr90st ~ whr80st, data = counties) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.73241 -0.20479 -0.03393 0.16637 2.13011 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.0005499 0.0074544 -0.074 0.941 ## whr80st 0.8014254 0.0107383 74.633 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.414 on 3083 degrees of freedom ## Multiple R-squared: 0.6437, Adjusted R-squared: 0.6436 ## F-statistic: 5570 on 1 and 3083 DF, p-value: &lt; 2.2e-16 11.3.3.3 Space-time regression In this case we have already computed the variables need to construct the plot, so we can start with ggplot2. The regression here is \\(y_{i,t}\\) on \\(\\Sigma_jw_{ij}y_{j,t-1}\\) ggplot(data = counties, aes(x= whr80st, y = hr90st)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_hline(yintercept = 0, lty = 2) + geom_vline(xintercept = 0, lty =2) Now we run the lm function to get the regression statistics. lmfit &lt;- lm(hr90st ~ whr80st, data = counties) summary(lmfit) ## ## Call: ## lm(formula = hr90st ~ whr80st, data = counties) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3519 -0.4569 -0.1702 0.2711 10.1136 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.003532 0.015403 -0.229 0.819 ## whr80st 0.746141 0.022188 33.628 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8555 on 3083 degrees of freedom ## Multiple R-squared: 0.2684, Adjusted R-squared: 0.2681 ## F-statistic: 1131 on 1 and 3083 DF, p-value: &lt; 2.2e-16 This yields a highly signifcant slope of .746 11.3.3.4 Serial and space-time regression Finally, we can include both in-situ correlation and space-time correlation in a regression specification of \\(y_{i,t}\\) i.s hr90st on both \\(x_{i,t-1}\\) (ie, hr80st) and \\(\\Sigma_jw_{ij}x_{j,t-1}\\)(ie whr80st) lmfit &lt;- lm(hr90st ~ whr80st + hr80st, data = counties) summary(lmfit) ## ## Call: ## lm(formula = hr90st ~ whr80st + hr80st, data = counties) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2802 -0.3807 -0.1202 0.2574 10.3821 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.002093 0.014282 -0.147 0.883 ## whr80st 0.442224 0.024628 17.956 &lt;2e-16 *** ## hr80st 0.383861 0.017099 22.450 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7932 on 3082 degrees of freedom ## Multiple R-squared: 0.3712, Adjusted R-squared: 0.3708 ## F-statistic: 909.7 on 2 and 3082 DF, p-value: &lt; 2.2e-16 In the regression, both effects are highly significant, with a coefficient of 0.384 for the in-situ effect, and 0.442 for the space-time effect. Needless to say, focusing the analysis solely on the bivariate Moran scatter plot provides only a limited perspective on the complexity of space-time (or, in general, bivariate) spatial and space-time associations. 11.4 Differential Moran Scatter Plot 11.4.1 Concept An alternative to the bivariate spatial correlation between a variable at one point in time and its neighbors at a previous (or, in general, any different) point in time is to control for the locational fixed effects by diffencing the variable. More precisely, we apply the computation of the spatial autocorrelation statistic to the variable \\(y_{i,t} - y_{i,t-1}\\). This takes care of any temporal correlation that may be the result of a fixed effect(i.e. a set of variables determining the value of \\(y_i\\) that remain unchanging over time). Specifically if \\(\\mu_i\\) were a fixed effect associated with location i, we could express the value at each location for time t as the sum of some intinsic value and the fixed effect, $y_{i,t} = y_{i,t}^* + _i $, where \\(\\mu_i\\) is unchanged over time (hence, fixed). The presence of \\(\\mu\\) induces a temporal correlation between \\(y_{i,t}\\) and \\(y_{i,t-1}\\), above and beyond the correlation between \\(y_{i,t}^*\\) and \\(y_{i,t-1}^*\\). Taking the difference eliminates the fixed effect and ensures that any remainin correlation is solely due to \\(y^*\\). A differential Moran’s I is then the slope in a regression of the spatial lag of the difference i.e. \\(\\Sigma_jw_{ij}(y_{j,t} - y_{j,t-1})\\) on the difference \\(y_{i,t} - y_{i,t-1}\\). All the caveats and different interpretations that apply to the generic bivariate case are also relevant here. 11.4.2 Implementation Same as the other sections, we must first calculate and standardize the variables before proceeding to construct the plot. In GeoDa this process is done automatically, but here we will need a few extra steps. First we calcualte the standardized score of the difference between the variable, NOT the difference between the standardized score of each variable. To do this, we just use standardize on the difference between HR80 and HR90. Then we calcualte the lag variable with lag.listw on the standardized difference variable. counties$hr_diff_st &lt;- standardize(counties$HR90 - counties$HR80) counties$hr_diff_lag &lt;- lag.listw(queen.weights, counties$hr_diff_st) To construct the plot, we use the same functions as earlier, but just replace the variables. In this case, we use the ones calcualted above for the standardized difference then the lag of the standardized difference. ggplot(data = counties, aes(x = hr_diff_st, y = hr_diff_lag)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_hline(yintercept = 0, lty = 2) + geom_vline(xintercept = 0, lty =2) lmfit &lt;- lm(hr_diff_lag~hr_diff_st, data = counties) summary(lmfit) ## ## Call: ## lm(formula = hr_diff_lag ~ hr_diff_st, data = counties) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.23658 -0.21238 0.03382 0.22281 2.45484 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.001690 0.008258 -0.205 0.838 ## hr_diff_st 0.052007 0.008260 6.297 3.47e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4587 on 3083 degrees of freedom ## Multiple R-squared: 0.0127, Adjusted R-squared: 0.01238 ## F-statistic: 39.65 on 1 and 3083 DF, p-value: 3.474e-10 The slope is about .052, which is much smaller than in magnitude than the other measures used thus far. 11.4.3 Options 11.5 Moran Scatter Plot for EB Rates 11.5.1 Concept An Empirical Bayes (EB) standardization was suggested by Assunção and Reis (1999) as a means to correct Moran’s I spatial autocorrelation test statistic for varying population densities across observational units, when the variable of interest is a rate or proportion. This standardization borrows ideas from the Bayesian shrinkage estimator outlined in discussion of Empirical Bayes smoothing. This approach is different from EB smoothing in that the spatial autocorrelation is not computed for a smoothed version of the original rate, but for a transformed standardized random variable. In other words, the crude rate is turned into a new variable that has a mean of zero and unit variance, thus avoiding problems with variance instability. The mean and variance used in the transformation are computed for each individual observation, thereby properly accounting for the instability in variance. The technical aspects are given in detail in Assunção and Reis (1999) and in the review by Anselin, Lozano-Gracia, and Koschinky (2006), but we briefly touch upon some salient issues here. The point of departure is the crude rate, say \\(r_i = O_i/P_i\\), where \\(O_i\\) is the count of events at location i and \\(P_i\\) is the corresponding population at risk. The rationale behind the Assuncao-Reis approach is to standardize each \\(r_i\\) as \\[z_i = \\frac{r_i - \\beta}{\\sqrt{\\alpha + (\\beta/P_i)}}\\] using an estimated mean \\(\\beta\\) and standard error \\(\\sqrt{\\alpha + (\\beta/P_i)}\\). The parameter \\(\\alpha\\) and \\(\\beta\\) are related to the prior distribution for the risk, which is detailed in the notebook on rate mapping. In practice, the prior parameters are estimated by means of the so-called method of moments (e.g., Marshall 1991), yielding the following expressions: \\[\\beta=O/P\\] With O as the total event count \\((\\Sigma_iO_i)\\), and P as the total population count \\((\\Sigma_iP_i)\\), and \\[\\alpha = [\\Sigma_iP_i(r_i - \\beta)^2]/P - \\beta/(P/n)\\] with n as the total number of observations (in other words, P/n is the average population) One problem with the method of moments estimator is that the expression for \\(\\alpha\\) could yield a negative result. In that case, its value is typically set to zero, i.e. \\(\\alpha = 0\\). However, Assuncao and Reis(1999), the value for \\(\\alpha\\) is set to zero when the resulting estimate for variance is negative, that is, \\(\\alpha + \\beta/P_i &lt;0\\). Slight differences in the standardized variates may result depending on the convention used. In GeoDa, when the variance estimate is negative, the original raw rate is used. 11.5.2 Implementation To calculate the empirical bayes rates, we will use basic R operations. We start by calculating the initial variables. beta &lt;- sum(counties$HC60) / sum(counties$PO60) r_i &lt;- counties$HC60 / counties$PO60 P_i &lt;- counties$PO60 n &lt;- length(r_i) P &lt;- sum(counties$PO60) With the parameters from above, we can calculate \\(\\alpha\\), by the formula specified in the concept section. alpha &lt;- sum(P_i * (r_i - beta)^2) / P - beta/(P/n) With \\(\\alpha\\), we can get the standard error used to later calculate the standardized values. se &lt;- sqrt(alpha + beta/P_i) Now the standardized values are just the crude rate minus \\(\\beta\\) and divided by the standard error. z_i &lt;- (r_i - beta) / se With our standardized values, we can make the lag variable to constuct our empirical bayes Moran plot. We use lag.listw as before to make the spatial lag varible. counties$hc60_po60 &lt;- z_i counties$lag_hc60_po60 &lt;- lag.listw(queen.weights, z_i) Now with the same ggplot2 functions as earlier we plot the smoothed rate variable against the lag smoothed rate variable. ggplot(data = counties, aes(x = hc60_po60, y = lag_hc60_po60)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_hline(yintercept = 0, lty = 2) + geom_vline(xintercept = 0, lty =2) To get the regression statistics, we run lm and then examine the summary with summary. lmfit &lt;- lm(lag_hc60_po60 ~ hc60_po60, data = counties) summary(lmfit) ## ## Call: ## lm(formula = lag_hc60_po60 ~ hc60_po60, data = counties) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.05433 -0.34093 -0.09708 0.27749 2.89081 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.001913 0.009724 -0.197 0.844 ## hc60_po60 0.518303 0.010598 48.906 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.54 on 3083 degrees of freedom ## Multiple R-squared: 0.4369, Adjusted R-squared: 0.4367 ## F-statistic: 2392 on 1 and 3083 DF, p-value: &lt; 2.2e-16 Use setwd(directorypath) to specify the working directory.↩︎ Use install.packages(packagename).↩︎ "],["local-spatial-autocorrelation-1.html", "Chapter 12 Local Spatial Autocorrelation 1 Introduction 12.1 Preliminaries 12.2 Local Moran 12.3 Local Geary 12.4 Getis-Ord Statistics 12.5 Local Join Count Statistic", " Chapter 12 Local Spatial Autocorrelation 1 Introduction This notebook cover the functionality of the Local Spatial Autocorrelation section of the GeoDa workbook. We refer to that document for details on the methodology, references, etc. The goal of these notes is to approximate as closely as possible the operations carried out using GeoDa by means of a range of R packages. The notes are written with R beginners in mind, more seasoned R users can probably skip most of the comments on data structures and other R particulars. Also, as always in R, there are typically several ways to achieve a specific objective, so what is shown here is just one way that works, but there often are others (that may even be more elegant, work faster, or scale better). For this notebook, we use Cleveland house price data. Our goal in this lab is show how to assign spatial weights based on different distance functions. 12.0.1 Objectives After completing the notebook, you should know how to carry out the following tasks: Identify clusters with the Local Moran cluster map and significance map Identify clusters with the Local Geary cluster map and significance map Identify clusters with the Getis-Ord Gi and Gi* statistics Identify clusters with the Local Join Count statistic Interpret the spatial footprint of spatial clusters Assess potential interaction effects by means of conditional cluster maps Assess the significance by means of a randomization approach Assess the sensitivity of different significance cut-off values Interpret significance by means of Bonferroni bounds 12.0.1.1 R Packages used spatmap: To construct significance and cluster maps for a variety of local statistics geodaData: To load the data for this notebook tmap: To format the maps made rgeoda: To run local spatial autocorrelation analysis 12.0.1.2 R Commands used Below follows a list of the commands used in this notebook. For further details and a comprehensive list of options, please consult the R documentation. Base R: install.packages, library, setwd, set.seed, cut, rep tmap: tm_shape, tm_borders, tm_fill, tm_layout, tm_facets 12.1 Preliminaries Before starting, make sure to have the latest version of R and of packages that are compiled for the matching version of R (this document was created using R 3.5.1 of 2018-07-02). Also, optionally, set a working directory, even though we will not actually be saving any files.31 12.1.1 Load packages First, we load all the required packages using the library command. If you don’t have some of these in your system, make sure to install them first as well as their dependencies.32 You will get an error message if something is missing. If needed, just install the missing piece and everything will work after that. library(sf) library(tmap) library(rgeoda) library(geodaData) library(RColorBrewer) 12.1.2 spatmap The main package used throughout this notebook will be rgeoda. This package provides the statistical computations of local spatial statistics and tmap for the mapping component. All of the visualizations are built with a similar style to GeoDa. The visualizations include cluster maps and their associated significance maps. The mapping functions are built off of tmap and can have additional layers added to them like tm_borders or tm_layout. 12.1.3 geodaData All of the data for the R notebooks is available in the geodaData package. We loaded the library earlier, now to access the individual data sets, we use the double colon notation. This works similar to to accessing a variable with $, in that a drop down menu will appear with a list of the datasets included in the package. For this notebook, we use guerry. guerry &lt;- geodaData::guerry 12.1.4 Univariate analysis Throughout the notebook, we will focus on the variable Donatns, which is charitable donations per capita. Before proceeding with the local spatial statistics and visualizations, we will take preliminary look at the spatial distribution of this variable. This is done with tmap functions. We will not go into too much detail on these because there is a lot to cover local spatial statistics and this functionality was covered in a previous notebook. Please the Basic Mapping notebook for more information on basic tmap functionality For the univariate map, we use the natural breaks or jenks style to get a general sense of the spatial distribution for our variable. tm_shape(guerry) + tm_fill(&quot;Donatns&quot;, style = &quot;jenks&quot;, n = 6) + tm_borders() + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;left&quot;) 12.2 Local Moran 12.2.1 Principle The local Moran statistic was suggested in Anselin(1995) as a way to identify local clusters and local spaital outliers. Most global spatial autocorrelation can be expressed as a double sum over i and j indices, such as \\(\\Sigma_i\\Sigma_jg_{ij}\\). The local form of such a statistic would then be, for each observation(location)i, the sum of the relevant expression over the j index, \\(\\Sigma_jg_{ij}\\). Specifically, the local Moran statistic takes the form \\(cz_i\\Sigma_jw_{ij}z_j\\), with z in deviations from the mean. The scalar c is the same for all locations and therefore does not play a role in the assessment of significance. The latter is obtained by means of a conditional permutation method, where, in turn, each \\(z_i\\) is held fixed, and the remaining z-values are randomly permuted to yield a reference distribution for the statistic. This operates in the same fashion as for the global Moran’s I, except that the permutation is carried out for each observation in turn. The result is a pseudo p-value for each location, which can then be used to assess significance. Note that this notion of significance is not the standard one, and should not be interpreted that way (see the discussion of multiple comparisons below). Assessing significance in and of itself is not that useful for the Local Moran. However, when an indication of significance is combined with the location of each observation in the Moran Scatterplot, a very powerful interpretation becomes possible. The combined information allows for a classification of the significant locations as high-high and low-low spatial clusters, and high-low and low-high spatial outliers. It is important to keep in mind that the reference to high and low is relative to the mean of the variable, and should not be interpreted in an absolute sense. 12.2.2 Implementation With the function local_moran from rgeoda, we can create a local moran cluster map. The parameters needed are an sf dataframe, which is guerry in our case, and the name of a variable from the sf dataframe. Some help functions that create maps based the statistical results of rgeoda: match_palette &lt;- function(patterns, classifications, colors){ classes_present &lt;- base::unique(patterns) mat &lt;- matrix(c(classifications,colors), ncol = 2) logi &lt;- classifications %in% classes_present pre_col &lt;- matrix(mat[logi], ncol = 2) pal &lt;- pre_col[,2] return(pal) } lisa_map &lt;- function(df, lisa, alpha = .05) { clusters &lt;- lisa_clusters(lisa,cutoff = alpha) labels &lt;- lisa_labels(lisa) pvalue &lt;- lisa_pvalues(lisa) colors &lt;- lisa_colors(lisa) lisa_patterns &lt;- labels[clusters+1] pal &lt;- match_palette(lisa_patterns,labels,colors) labels &lt;- labels[labels %in% lisa_patterns] df[&quot;lisa_clusters&quot;] &lt;- clusters tm_shape(df) + tm_fill(&quot;lisa_clusters&quot;,labels = labels, palette = pal,style = &quot;cat&quot;) } significance_map &lt;- function(df, lisa, permutations = 999, alpha = .05) { pvalue &lt;- lisa_pvalues(lisa) target_p &lt;- 1 / (1 + permutations) potential_brks &lt;- c(.00001, .0001, .001, .01) brks &lt;- potential_brks[which(potential_brks &gt; target_p &amp; potential_brks &lt; alpha)] brks2 &lt;- c(target_p, brks, alpha) labels &lt;- c(as.character(brks2), &quot;Not Significant&quot;) brks3 &lt;- c(0, brks2, 1) cuts &lt;- cut(pvalue, breaks = brks3,labels = labels) df[&quot;sig&quot;] &lt;- cuts pal &lt;- rev(brewer.pal(length(labels), &quot;Greens&quot;)) pal[length(pal)] &lt;- &quot;#D3D3D3&quot; tm_shape(df) + tm_fill(&quot;sig&quot;, palette = pal) } It is important to note the default parameters of local_moran. These include permutations = 999, significance_cutoff = .05, and weights = NULL. Permutations is the number of permutations used in computing the reference distributions of the local statistic for each location. Significance_cutoff or alpha is the cutoff significance level. The weights parameter is where we specify the weights used for the computation of the local statistics. In the NULL case, 1st order queen contiguity are computed. w &lt;- queen_weights(guerry) lisa &lt;- local_moran(w, guerry[&#39;Donatns&#39;]) lisa_map(guerry, lisa) To get a significance map for the local moran, we use significance_map. Default number of permutations is 999, the alpha level is .05. significance_map(guerry, lisa) 12.2.2.1 tmap additions With the mapping functions of lisa_map, additional tmap layers can be added with the + operator. This gives the maps strong formatting options. With tm_borders, we can make the borders of the local moran map more distinct. Withtm_layout we can add a title and move the legend to the outside of the map. There many more formatting options, including tmap_arrange, which we used earlier. lisa_map(guerry, lisa) + tm_borders() + tm_layout(title = &quot;Local Moran Cluster Map of Donatns&quot;, legend.outside = TRUE) We can set the tmap mode to “view”” to get an interactive base map with tmap_mode. tmap_mode(&quot;view&quot;) lisa_map(guerry, lisa) + tm_borders() + tm_layout(title = &quot;Local Moran Cluster Map of Donatns&quot;,legend.outside = TRUE) We set tmap_mode(\"plot\") to get normal maps for the rest of the notebook. While basemaps are a nice option, they are not necessary for the remainder of the notebook. tmap_mode(&quot;plot&quot;) 12.2.3 Randomization Options To obtain higher significance levels, we need to use more permutations in the computation of the the local moran for each location. For instance, a pseudo pvalue of .00001 would require 999999 permutations. To get more permutations, we set permutations = 99999 in local_moran. It is important to note that the maximum number of permutations for this function is 99999. lisa &lt;- local_moran(w, guerry[&#39;Donatns&#39;], permutations = 99999) lisa_map(guerry, lisa) + tm_borders() + tm_layout(title = &quot;Local Moran Cluster Map of Donatns&quot;, legend.outside = TRUE) For the significance map, the process is the same, we set permutations = 99999. significance_map(guerry, lisa, permutations = 99999) + tm_borders() + tm_layout(title = &quot;Local Moran Significance Map of Donatns&quot;, legend.outside = TRUE) 12.2.4 Significance An important methodological issue associated with the local spatial autocorrelation statistics is the selection of the p-value cut-off to properly reflect the desired Type I error. Not only are the pseudo p-values not analytical, since they are the result of a computational permutation process, but they also suffer from the problem of multiple comparisons. The bottom line is that a traditional choice of 0.05 is likely to lead to many false positives, i.e., rejections of the null when in fact it holds. To change the cut-off level of significance in the local moran cluster mapping function we use the parameter alpha =. The default option is .05, but if we want another level, say .01, we set alpha = .01. lisa_map(guerry, lisa, alpha = .01) + tm_borders() + tm_layout(title = &quot;Local Moran Cluster Map of Donatns&quot;, legend.outside = TRUE) The process is the same in significance_map, we set alpha = .01. significance_map(guerry, lisa, permutations = 99999, alpha = .01) + tm_borders() + tm_layout(title = &quot;Local Moran Significance Map of Donatns&quot;, legend.outside = TRUE) 12.2.4.1 Bonferroni bound The Bonferroni bound constructs a bound on the overall p-value by taking \\(\\alpha\\) and dividing it by the number of comparisons. In our context, the latter corresponds to the number of observation, n. As a result, the Bonferroni bound would be \\(\\alpha/n = .00012\\), the cutoff p-value to be used to determine significance. We assign bonferroni to be .01 / 85. Then we use lisa_map with permutations = 99999 and alpha = bonferroni. This will give us a local moran cluster map with a bonferroni significance cut-off. bonferroni &lt;- .01 / 85 lisa_map(guerry, lisa, alpha = bonferroni) + tm_borders() + tm_layout(title = &quot;Local Moran Cluster Map of Donatns&quot;, legend.outside = TRUE) To make the significance map with the bonferroni bound, we set alpha = bonferroni. significance_map(guerry, lisa,permutations = 99999, alpha = bonferroni) + tm_borders() + tm_layout(title = &quot;Local Moran Significance Map of Donatns&quot;, legend.outside = TRUE) 12.2.4.2 Interpretation of significance As mentioned, there is no fully satisfactory solution to deal with the multiple comparison problem. Therefore, it is recommended to carry out a sensitivity analysis and to identify the stage where the results become interesting. A mechanical use of 0.05 as a cut off value is definitely not the proper way to proceed. Also, for the Bonferroni procedure to work properly, it is necessary to have a large number of permutations, to ensure that the minimum p-value can be less than \\(\\alpha/n\\). The maximum number of permutations supported is 99999. The bonferroni approach will be limited for datasets with many locations. With \\(\\alpha = .01\\), datasets with n &gt; 1000, cannot yield significant locations. 12.2.5 Interpretation of clusters Strictly speaking, the locations shown as significant on the significance and cluster maps are not the actual clusters, but the cores of a cluster. In contrast, in the case of spatial outliers, they are the actual locations of interest. 12.2.6 Conditional local cluster maps To make the conditional map, we first need to make two categorical variables, with two categories. cut breaks the data up into two equal pieces. With the two categorical variables, we can create facets with tmap. guerry$cut.literacy &lt;- cut(guerry$Litercy, breaks = 2) guerry$cut.clergy &lt;- cut(guerry$Clergy, breaks = 2) To make conditional maps, the only addition needed is tm_facets, which will use the two categorical variables created above. We set by = c(\"cut.literacy\",\"cut.clergy\"). This will give use four maps faceted by the two categorical variables that we made above. lisa_map(guerry, lisa) + tm_borders() + tm_facets(by = c(&quot;cut.literacy&quot;,&quot;cut.clergy&quot;),free.coords = FALSE,drop.units=FALSE) 12.3 Local Geary 12.3.1 Principle The Local Geary statistic, first outlined in Anselin (1995), and further elaborated upon in Anselin (2018), is a Local Indicator of Spatial Association (LISA) that uses a different measure of attribute similarity. As in its global counterpart, the focus is on squared differences, or, rather, dissimilarity. In other words, small values of the statistics suggest positive spatial autocorrelation, whereas large values suggest negative spatial autocorrelation. Formally, the Local Geary statistic is \\[LG_i = \\Sigma_jw_{ij}(x_i-x_j)^2\\] in the usual notation. Inference is again based on a conditional permutation procedure and is interpreted in the same way as for the Local Moran statistic. However, the interpretation of significant locations in terms of the type of association is not as straightforward. In essence, this is because the attribute similarity is not a cross-product and thus has no direct correspondence with the slope in a scatter plot. Nevertheless, we can use the linking capability within GeoDa to make an incomplete classification. Those locations identified as significant and with the Local Geary statistic smaller than its mean, suggest positive spatial autocorrelation (small differences imply similarity). For those observations that can be classified in the upper-right or lower-left quadrants of a matching Moran scatter plot, we can identify the association as high-high or low-low. However, given that the squared difference can cross the mean, there may be observations for which such a classification is not possible. We will refer to those as other positive spatial autocorrelation. For negative spatial autocorrelation (large values imply dissimilarity), it is not possible to assess whether the association is between high-low or low-high outliers, since the squaring of the differences removes the sign. 12.3.2 Implementation For the local geary map, we use loca_geary(). It has the same default parameters with 999 permutations, an alpha level of .05, and 1st order queen contiguity weights. For mapping function lisa_map(), the inputs are the same as lisa_map with an sf dataframe: guerry, and the results of local_geary(): lisa. We can add tmap layers to this mapping function too. Here we use tm_borders and tm_layout lisa &lt;- local_geary(w, guerry[&#39;Donatns&#39;]) lisa_map(guerry, lisa) + tm_borders() + tm_layout(&quot;Local Geary Cluster Map&quot;, legend.outside = TRUE) To get the significance map that directly corresponds with the Local Geary map, the random seed needs to be the same and set before each function, as with the moran. significance_map(guerry,lisa) + tm_borders() + tm_layout(&quot;Local Geary Significance Map&quot;, legend.outside = TRUE) 12.3.2.1 Interpretation and significance To get more permutations, we set permutations = 99999 lisa &lt;- local_geary(w, guerry[&#39;Donatns&#39;], permutations = 99999) lisa_map(guerry, lisa) + tm_borders() + tm_layout(&quot;Local Geary Cluster Map&quot;, legend.outside = TRUE) We do the same thing to get more permutations for the significance map. significance_map(guerry, lisa,permutations = 99999) + tm_borders() + tm_layout(&quot;Local Geary Significance Map&quot;, legend.outside = TRUE) 12.3.2.2 Changing the significance threshold We can change the significance cut-off with alpha =, as with lisa_map and significance_map lisa_map(guerry, lisa, alpha = .01) + tm_borders() + tm_layout(&quot;Local Geary Cluster Map&quot;, legend.outside = TRUE) 12.4 Getis-Ord Statistics 12.4.1 Principle A third class of statistics for local spatial autocorrelation was suggested by Getis and Ord (1992), and further elaborated upon in Ord and Getis (1995). It is derived from a point pattern analysis logic. In its earliest formulation the statistic consisted of a ratio of the number of observations within a given range of a point to the total count of points. In a more general form, the statistic is applied to the values at neighboring locations (as defined by the spatial weights). There are two versions of the statistic. They differ in that one takes the value at the given location into account, and the other does not. The \\(G_i\\) statistic consists of a ratio of the weighted average of the values in the neighboring locations, to the sum of all values, not including the value at the location \\(x_i\\) \\[G_i = \\frac{\\Sigma_{j\\neq i}w_{ij}x_j}{\\Sigma_{j\\neq i}x_j}\\] In contrast, the \\(G_i^*\\) statistic includes the value \\(x_i\\) in numerator and denominator: \\[G_i^*=\\frac{\\Sigma_jw_{ij}x_j}{\\Sigma_jx_j}\\] Note that in this case, the denominator is constant across all observations and simply consists of the total sum of all values in the data set. The interpretation of the Getis-Ord statistics is very straightforward: a value larger than the mean (or, a positive value for a standardized z-value) suggests a high-high cluster or hot spot, a value smaller than the mean (or, negative for a z-value) indicates a low-low cluster or cold spot. In contrast to the Local Moran and Local Geary statistics, the Getis-Ord approach does not consider spatial outliers. Inference is based on conditional permutation, using an identical procedure as for the other statistics. 12.4.2 Implementation We can make a cluster map for the local G statistic with loca_g(). The formatting, parameters, and default options are the same with this function as the other mapping functions in rgeoda. lisa &lt;- local_g(w, guerry[&#39;Donatns&#39;]) lisa_map(guerry, lisa) + tm_borders() + tm_layout(title = &quot;Local G Cluster Map&quot;,legend.outside = TRUE) To make the \\(G*\\) cluster map, we run local_gstar(). lisa &lt;- local_gstar(w, guerry[&#39;Donatns&#39;]) lisa_map(guerry, lisa) + tm_borders() + tm_layout(title = &quot;Local G* Cluster Map&quot;,legend.outside = TRUE) For the significance map, we use significance_map. significance_map(guerry, lisa) + tm_borders() + tm_layout(title = &quot;Local G Significance Map&quot;,legend.outside = TRUE) 12.4.3 Interpretation and significance To change the permutations and the cut-off significance level, we use permutation =, and alpha =. The default options for these parameters are 999 for permutations and .05 for alpha, as with the other spatmap mapping functions. Here we change permutations = 99999 and alpha = .01. lisa_map(guerry, lisa, alpha = .01) + tm_borders() + tm_layout(title = &quot;Local G Cluster Map&quot;,legend.outside = TRUE) The process is the same for the corresponding significance map. Increasing the permutations gives us more detailed information about the significance at each location. significance_map(guerry, lisa, permutations = 99999,alpha = .01) + tm_borders() + tm_layout(title = &quot;Local G Significance Map&quot;,legend.outside = TRUE) 12.5 Local Join Count Statistic 12.5.1 Principle Recently, Anselin and Li (2019) showed how a constrained version of the \\(G_i^*\\) statistic yields a local version of the well-known join count statistic for spatial autocorrelation of binary variables, popularized by Cliff and Ord (1973). Expressed as a LISA statistic, a local version of the so-called BB join count statistic is \\[BB_i = x_i\\Sigma_jw_{ij}x_j\\] where \\(x_{i,j}\\) can only take on the values of 1 and 0, and \\(w_{ij}\\) are the elements of a binary spatial weights matrix (i.e., not row-standardized). For the most meaningful results, the value of 1 should be chosen for the case with the fewest observations (of course, the definition of what is 1 and 0 can easily be switched). The statistic is only meaningful for those observations where \\(x_i =1\\), since for \\(x_i =0\\) the result will always equal zero. A pseudo p-value is obtained by means of a conditional permutation approach, in the same way as for the other local spatial autocorrelation statistics, but only for those observations with \\(x_i=1\\). The same caveats as before should be kept in mind when interpreting the results, which are subject to multiple comparisons and the sensitivity of the pseudo p-value to the actual simulation experiment (random seed, number of permutations). Technical details are provided in Anselin and Li (2019). 12.5.2 Implementation Since the local join count only uses binary variables(numeric variables of 1 or 0), we must make one for guerry. To get the number of observations in guerry we use nrow. We create and empty vector of 0’s of length n with rep. We assign 1 for the locations that have Donatns greater than 10996. Lastly we add the binary variable doncat to the sf dataframe. n &lt;- nrow(guerry) doncat &lt;- rep(0, n) doncat[guerry$Donatns &gt; 10996] &lt;- 1 guerry$doncat &lt;- doncat We map these locations using tmap functions. We set style = \"cat\" because the variable is and only has two possible values. We use color white for 0 and color blue for 1. tm_shape(guerry) + tm_fill(&quot;doncat&quot;, style = &quot;cat&quot;, palette = c(&quot;white&quot;, &quot;blue&quot;)) + tm_borders() + tm_layout(legend.outside = TRUE) To make the local join count cluster map, we use local_joincount() with doncat as the input variables. We change permutations to be 99999. This function has the same default options and paramters as the other mapping functions. lisa &lt;- local_joincount(w, guerry[&#39;doncat&#39;], permutations = 99999) lisa_map(guerry, lisa) + tm_borders() + tm_layout(title = &quot;Local G Cluster Map&quot;,legend.outside = TRUE) Use setwd(directorypath) to specify the working directory.↩︎ Use install.packages(packagename).↩︎ "],["references.html", "Chapter 13 References", " Chapter 13 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
